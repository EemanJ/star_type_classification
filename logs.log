2023-04-29 03:41:34,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 03:41:34,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 03:41:34,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 03:41:34,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 03:41:36,272:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-04-29 03:41:36,712:INFO:PyCaret ClassificationExperiment
2023-04-29 03:41:36,712:INFO:Logging name: clf-default-name
2023-04-29 03:41:36,712:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-04-29 03:41:36,712:INFO:version 3.0.0.rc8
2023-04-29 03:41:36,712:INFO:Initializing setup()
2023-04-29 03:41:36,714:INFO:self.USI: d32e
2023-04-29 03:41:36,714:INFO:self._variable_keys: {'gpu_param', '_available_plots', '_ml_usecase', 'fold_shuffle_param', 'X_train', 'data', 'target_param', 'logging_param', 'X', 'memory', 'html_param', 'fix_imbalance', 'pipeline', 'log_plots_param', 'n_jobs_param', 'exp_id', 'fold_groups_param', 'is_multiclass', 'X_test', 'y_test', 'fold_generator', 'exp_name_log', 'USI', 'seed', 'gpu_n_jobs_param', 'idx', 'y_train', 'y'}
2023-04-29 03:41:36,714:INFO:Checking environment
2023-04-29 03:41:36,714:INFO:python_version: 3.9.13
2023-04-29 03:41:36,714:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-04-29 03:41:36,714:INFO:machine: AMD64
2023-04-29 03:41:36,714:INFO:platform: Windows-10-10.0.22621-SP0
2023-04-29 03:41:36,714:INFO:Memory: svmem(total=16782143488, available=1711984640, percent=89.8, used=15070158848, free=1711984640)
2023-04-29 03:41:36,714:INFO:Physical Core: 10
2023-04-29 03:41:36,714:INFO:Logical Core: 12
2023-04-29 03:41:36,714:INFO:Checking libraries
2023-04-29 03:41:36,714:INFO:System:
2023-04-29 03:41:36,714:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-04-29 03:41:36,714:INFO:executable: C:\Users\eeman\anaconda3\python.exe
2023-04-29 03:41:36,714:INFO:   machine: Windows-10-10.0.22621-SP0
2023-04-29 03:41:36,714:INFO:PyCaret required dependencies:
2023-04-29 03:41:36,714:INFO:                 pip: 22.2.2
2023-04-29 03:41:36,716:INFO:          setuptools: 63.4.1
2023-04-29 03:41:36,716:INFO:             pycaret: 3.0.0rc8
2023-04-29 03:41:36,716:INFO:             IPython: 7.31.1
2023-04-29 03:41:36,716:INFO:          ipywidgets: 7.6.5
2023-04-29 03:41:36,716:INFO:                tqdm: 4.64.1
2023-04-29 03:41:36,716:INFO:               numpy: 1.21.5
2023-04-29 03:41:36,716:INFO:              pandas: 1.4.4
2023-04-29 03:41:36,716:INFO:              jinja2: 2.11.3
2023-04-29 03:41:36,716:INFO:               scipy: 1.9.1
2023-04-29 03:41:36,716:INFO:              joblib: 1.2.0
2023-04-29 03:41:36,716:INFO:             sklearn: 1.0.2
2023-04-29 03:41:36,716:INFO:                pyod: 1.0.7
2023-04-29 03:41:36,716:INFO:            imblearn: 0.10.1
2023-04-29 03:41:36,716:INFO:   category_encoders: 2.6.0
2023-04-29 03:41:36,716:INFO:            lightgbm: 3.3.4
2023-04-29 03:41:36,716:INFO:               numba: 0.55.1
2023-04-29 03:41:36,716:INFO:            requests: 2.28.1
2023-04-29 03:41:36,716:INFO:          matplotlib: 3.5.2
2023-04-29 03:41:36,716:INFO:          scikitplot: 0.3.7
2023-04-29 03:41:36,716:INFO:         yellowbrick: 1.5
2023-04-29 03:41:36,716:INFO:              plotly: 5.9.0
2023-04-29 03:41:36,716:INFO:             kaleido: 0.2.1
2023-04-29 03:41:36,716:INFO:         statsmodels: 0.13.5
2023-04-29 03:41:36,718:INFO:              sktime: 0.15.1
2023-04-29 03:41:36,718:INFO:               tbats: 1.1.2
2023-04-29 03:41:36,718:INFO:            pmdarima: 2.0.2
2023-04-29 03:41:36,718:INFO:              psutil: 5.9.0
2023-04-29 03:41:36,718:INFO:PyCaret optional dependencies:
2023-04-29 03:41:36,763:INFO:                shap: 0.41.0
2023-04-29 03:41:36,763:INFO:           interpret: Not installed
2023-04-29 03:41:36,763:INFO:                umap: Not installed
2023-04-29 03:41:36,763:INFO:    pandas_profiling: Not installed
2023-04-29 03:41:36,763:INFO:  explainerdashboard: Not installed
2023-04-29 03:41:36,763:INFO:             autoviz: Not installed
2023-04-29 03:41:36,763:INFO:           fairlearn: Not installed
2023-04-29 03:41:36,763:INFO:             xgboost: Not installed
2023-04-29 03:41:36,763:INFO:            catboost: Not installed
2023-04-29 03:41:36,763:INFO:              kmodes: Not installed
2023-04-29 03:41:36,763:INFO:             mlxtend: Not installed
2023-04-29 03:41:36,763:INFO:       statsforecast: Not installed
2023-04-29 03:41:36,763:INFO:        tune_sklearn: Not installed
2023-04-29 03:41:36,763:INFO:                 ray: Not installed
2023-04-29 03:41:36,763:INFO:            hyperopt: Not installed
2023-04-29 03:41:36,765:INFO:              optuna: Not installed
2023-04-29 03:41:36,765:INFO:               skopt: Not installed
2023-04-29 03:41:36,765:INFO:              mlflow: Not installed
2023-04-29 03:41:36,765:INFO:              gradio: Not installed
2023-04-29 03:41:36,765:INFO:             fastapi: Not installed
2023-04-29 03:41:36,765:INFO:             uvicorn: Not installed
2023-04-29 03:41:36,765:INFO:              m2cgen: Not installed
2023-04-29 03:41:36,765:INFO:           evidently: Not installed
2023-04-29 03:41:36,765:INFO:                nltk: 3.7
2023-04-29 03:41:36,765:INFO:            pyLDAvis: Not installed
2023-04-29 03:41:36,765:INFO:              gensim: 4.1.2
2023-04-29 03:41:36,765:INFO:               spacy: Not installed
2023-04-29 03:41:36,765:INFO:           wordcloud: Not installed
2023-04-29 03:41:36,765:INFO:            textblob: Not installed
2023-04-29 03:41:36,765:INFO:               fugue: Not installed
2023-04-29 03:41:36,765:INFO:           streamlit: Not installed
2023-04-29 03:41:36,765:INFO:             prophet: Not installed
2023-04-29 03:41:36,765:INFO:None
2023-04-29 03:41:36,765:INFO:Set up data.
2023-04-29 03:41:36,777:INFO:Set up train/test split.
2023-04-29 03:41:36,787:INFO:Set up index.
2023-04-29 03:41:36,787:INFO:Set up folding strategy.
2023-04-29 03:41:36,787:INFO:Assigning column types.
2023-04-29 03:41:36,793:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-29 03:41:36,954:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-29 03:41:36,962:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:41:37,082:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,140:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,294:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-29 03:41:37,298:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:41:37,394:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,396:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,396:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-29 03:41:37,560:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:41:37,658:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,660:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,846:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:41:37,948:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,948:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,948:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-04-29 03:41:38,214:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:38,214:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:38,480:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:38,480:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:38,484:INFO:Preparing preprocessing pipeline...
2023-04-29 03:41:38,490:INFO:Set up simple imputation.
2023-04-29 03:41:38,496:INFO:Set up encoding of categorical features.
2023-04-29 03:41:38,585:INFO:Finished creating preprocessing pipeline.
2023-04-29 03:41:38,599:INFO:Pipeline: Pipeline(memory=Memory(location=C:\Users\eeman\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'L', 'R', 'A_M'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose=0))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    inc...
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Color', 'Spectral_Class'],
                                    transformer=OneHotEncoder(cols=['Color',
                                                                    'Spectral_Class'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2023-04-29 03:41:38,599:INFO:Creating final display dataframe.
2023-04-29 03:41:38,994:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target              Type
2                   Target type        Multiclass
3           Original data shape          (216, 7)
4        Transformed data shape         (216, 24)
5   Transformed train set shape         (151, 24)
6    Transformed test set shape          (65, 24)
7              Numeric features                 4
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              d32e
2023-04-29 03:41:39,306:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:39,308:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:39,564:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:39,566:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:39,566:INFO:setup() successfully completed in 2.86s...............
2023-04-29 03:41:39,590:INFO:Initializing compare_models()
2023-04-29 03:41:39,592:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-04-29 03:41:39,592:INFO:Checking exceptions
2023-04-29 03:41:39,596:INFO:Preparing display monitor
2023-04-29 03:41:39,707:INFO:Initializing Logistic Regression
2023-04-29 03:41:39,707:INFO:Total runtime is 0.0 minutes
2023-04-29 03:41:39,711:INFO:SubProcess create_model() called ==================================
2023-04-29 03:41:39,713:INFO:Initializing create_model()
2023-04-29 03:41:39,713:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:41:39,713:INFO:Checking exceptions
2023-04-29 03:41:39,713:INFO:Importing libraries
2023-04-29 03:41:39,713:INFO:Copying training dataset
2023-04-29 03:41:39,719:INFO:Defining folds
2023-04-29 03:41:39,719:INFO:Declaring metric variables
2023-04-29 03:41:39,726:INFO:Importing untrained model
2023-04-29 03:41:39,732:INFO:Logistic Regression Imported successfully
2023-04-29 03:41:39,745:INFO:Starting cross validation
2023-04-29 03:41:39,747:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:41:53,574:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:53,589:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:53,733:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:53,761:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:53,785:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,045:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,076:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,235:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,362:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,555:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,753:INFO:Calculating mean and std
2023-04-29 03:41:54,757:INFO:Creating metrics dataframe
2023-04-29 03:41:54,770:INFO:Uploading results into container
2023-04-29 03:41:54,773:INFO:Uploading model into container now
2023-04-29 03:41:54,773:INFO:_master_model_container: 1
2023-04-29 03:41:54,773:INFO:_display_container: 2
2023-04-29 03:41:54,773:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:41:54,773:INFO:create_model() successfully completed......................................
2023-04-29 03:41:55,060:INFO:SubProcess create_model() end ==================================
2023-04-29 03:41:55,065:INFO:Creating metrics dataframe
2023-04-29 03:41:55,090:INFO:Initializing K Neighbors Classifier
2023-04-29 03:41:55,090:INFO:Total runtime is 0.2563843488693237 minutes
2023-04-29 03:41:55,100:INFO:SubProcess create_model() called ==================================
2023-04-29 03:41:55,101:INFO:Initializing create_model()
2023-04-29 03:41:55,102:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:41:55,102:INFO:Checking exceptions
2023-04-29 03:41:55,102:INFO:Importing libraries
2023-04-29 03:41:55,102:INFO:Copying training dataset
2023-04-29 03:41:55,113:INFO:Defining folds
2023-04-29 03:41:55,113:INFO:Declaring metric variables
2023-04-29 03:41:55,121:INFO:Importing untrained model
2023-04-29 03:41:55,130:INFO:K Neighbors Classifier Imported successfully
2023-04-29 03:41:55,149:INFO:Starting cross validation
2023-04-29 03:41:55,150:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:41:55,618:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,628:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,631:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,647:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,689:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,701:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,755:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,875:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:41:55,883:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:04,643:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:42:04,865:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:42:05,020:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:05,032:INFO:Calculating mean and std
2023-04-29 03:42:05,036:INFO:Creating metrics dataframe
2023-04-29 03:42:05,052:INFO:Uploading results into container
2023-04-29 03:42:05,056:INFO:Uploading model into container now
2023-04-29 03:42:05,056:INFO:_master_model_container: 2
2023-04-29 03:42:05,056:INFO:_display_container: 2
2023-04-29 03:42:05,056:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-04-29 03:42:05,060:INFO:create_model() successfully completed......................................
2023-04-29 03:42:05,453:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:05,453:INFO:Creating metrics dataframe
2023-04-29 03:42:05,490:INFO:Initializing Naive Bayes
2023-04-29 03:42:05,490:INFO:Total runtime is 0.4297042449315389 minutes
2023-04-29 03:42:05,502:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:05,502:INFO:Initializing create_model()
2023-04-29 03:42:05,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:05,506:INFO:Checking exceptions
2023-04-29 03:42:05,507:INFO:Importing libraries
2023-04-29 03:42:05,507:INFO:Copying training dataset
2023-04-29 03:42:05,519:INFO:Defining folds
2023-04-29 03:42:05,519:INFO:Declaring metric variables
2023-04-29 03:42:05,531:INFO:Importing untrained model
2023-04-29 03:42:05,547:INFO:Naive Bayes Imported successfully
2023-04-29 03:42:05,571:INFO:Starting cross validation
2023-04-29 03:42:05,575:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:06,121:INFO:Calculating mean and std
2023-04-29 03:42:06,125:INFO:Creating metrics dataframe
2023-04-29 03:42:06,133:INFO:Uploading results into container
2023-04-29 03:42:06,137:INFO:Uploading model into container now
2023-04-29 03:42:06,141:INFO:_master_model_container: 3
2023-04-29 03:42:06,141:INFO:_display_container: 2
2023-04-29 03:42:06,145:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:42:06,145:INFO:create_model() successfully completed......................................
2023-04-29 03:42:06,399:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:06,399:INFO:Creating metrics dataframe
2023-04-29 03:42:06,431:INFO:Initializing Decision Tree Classifier
2023-04-29 03:42:06,431:INFO:Total runtime is 0.44539469480514526 minutes
2023-04-29 03:42:06,439:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:06,439:INFO:Initializing create_model()
2023-04-29 03:42:06,439:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:06,439:INFO:Checking exceptions
2023-04-29 03:42:06,439:INFO:Importing libraries
2023-04-29 03:42:06,439:INFO:Copying training dataset
2023-04-29 03:42:06,451:INFO:Defining folds
2023-04-29 03:42:06,455:INFO:Declaring metric variables
2023-04-29 03:42:06,467:INFO:Importing untrained model
2023-04-29 03:42:06,483:INFO:Decision Tree Classifier Imported successfully
2023-04-29 03:42:06,504:INFO:Starting cross validation
2023-04-29 03:42:06,508:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:07,038:INFO:Calculating mean and std
2023-04-29 03:42:07,042:INFO:Creating metrics dataframe
2023-04-29 03:42:07,050:INFO:Uploading results into container
2023-04-29 03:42:07,050:INFO:Uploading model into container now
2023-04-29 03:42:07,050:INFO:_master_model_container: 4
2023-04-29 03:42:07,054:INFO:_display_container: 2
2023-04-29 03:42:07,054:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=123, splitter='best')
2023-04-29 03:42:07,054:INFO:create_model() successfully completed......................................
2023-04-29 03:42:07,284:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:07,284:INFO:Creating metrics dataframe
2023-04-29 03:42:07,336:INFO:Initializing SVM - Linear Kernel
2023-04-29 03:42:07,340:INFO:Total runtime is 0.4605435570081075 minutes
2023-04-29 03:42:07,352:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:07,352:INFO:Initializing create_model()
2023-04-29 03:42:07,356:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:07,356:INFO:Checking exceptions
2023-04-29 03:42:07,356:INFO:Importing libraries
2023-04-29 03:42:07,356:INFO:Copying training dataset
2023-04-29 03:42:07,368:INFO:Defining folds
2023-04-29 03:42:07,368:INFO:Declaring metric variables
2023-04-29 03:42:07,380:INFO:Importing untrained model
2023-04-29 03:42:07,392:INFO:SVM - Linear Kernel Imported successfully
2023-04-29 03:42:07,424:INFO:Starting cross validation
2023-04-29 03:42:07,432:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:07,724:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,736:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,796:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,805:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,846:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,858:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,926:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,951:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,963:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:08,032:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:08,052:INFO:Calculating mean and std
2023-04-29 03:42:08,060:INFO:Creating metrics dataframe
2023-04-29 03:42:08,076:INFO:Uploading results into container
2023-04-29 03:42:08,076:INFO:Uploading model into container now
2023-04-29 03:42:08,080:INFO:_master_model_container: 5
2023-04-29 03:42:08,080:INFO:_display_container: 2
2023-04-29 03:42:08,083:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-04-29 03:42:08,083:INFO:create_model() successfully completed......................................
2023-04-29 03:42:08,421:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:08,421:INFO:Creating metrics dataframe
2023-04-29 03:42:08,462:INFO:Initializing Ridge Classifier
2023-04-29 03:42:08,462:INFO:Total runtime is 0.4792390465736389 minutes
2023-04-29 03:42:08,474:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:08,474:INFO:Initializing create_model()
2023-04-29 03:42:08,474:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:08,478:INFO:Checking exceptions
2023-04-29 03:42:08,478:INFO:Importing libraries
2023-04-29 03:42:08,478:INFO:Copying training dataset
2023-04-29 03:42:08,490:INFO:Defining folds
2023-04-29 03:42:08,494:INFO:Declaring metric variables
2023-04-29 03:42:08,506:INFO:Importing untrained model
2023-04-29 03:42:08,518:INFO:Ridge Classifier Imported successfully
2023-04-29 03:42:08,542:INFO:Starting cross validation
2023-04-29 03:42:08,546:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:08,882:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:08,906:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:08,950:INFO:Calculating mean and std
2023-04-29 03:42:08,958:INFO:Creating metrics dataframe
2023-04-29 03:42:08,973:INFO:Uploading results into container
2023-04-29 03:42:08,975:INFO:Uploading model into container now
2023-04-29 03:42:08,975:INFO:_master_model_container: 6
2023-04-29 03:42:08,979:INFO:_display_container: 2
2023-04-29 03:42:08,979:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:42:08,979:INFO:create_model() successfully completed......................................
2023-04-29 03:42:09,306:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:09,306:INFO:Creating metrics dataframe
2023-04-29 03:42:09,338:INFO:Initializing Random Forest Classifier
2023-04-29 03:42:09,342:INFO:Total runtime is 0.49391197760899863 minutes
2023-04-29 03:42:09,350:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:09,350:INFO:Initializing create_model()
2023-04-29 03:42:09,350:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:09,350:INFO:Checking exceptions
2023-04-29 03:42:09,354:INFO:Importing libraries
2023-04-29 03:42:09,354:INFO:Copying training dataset
2023-04-29 03:42:09,366:INFO:Defining folds
2023-04-29 03:42:09,366:INFO:Declaring metric variables
2023-04-29 03:42:09,379:INFO:Importing untrained model
2023-04-29 03:42:09,391:INFO:Random Forest Classifier Imported successfully
2023-04-29 03:42:09,411:INFO:Starting cross validation
2023-04-29 03:42:09,415:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:12,856:INFO:Calculating mean and std
2023-04-29 03:42:12,882:INFO:Creating metrics dataframe
2023-04-29 03:42:12,902:INFO:Uploading results into container
2023-04-29 03:42:12,906:INFO:Uploading model into container now
2023-04-29 03:42:12,906:INFO:_master_model_container: 7
2023-04-29 03:42:12,911:INFO:_display_container: 2
2023-04-29 03:42:12,911:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 03:42:12,911:INFO:create_model() successfully completed......................................
2023-04-29 03:42:13,336:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:13,336:INFO:Creating metrics dataframe
2023-04-29 03:42:13,426:INFO:Initializing Quadratic Discriminant Analysis
2023-04-29 03:42:13,426:INFO:Total runtime is 0.5619824965794882 minutes
2023-04-29 03:42:13,446:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:13,446:INFO:Initializing create_model()
2023-04-29 03:42:13,446:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:13,446:INFO:Checking exceptions
2023-04-29 03:42:13,446:INFO:Importing libraries
2023-04-29 03:42:13,446:INFO:Copying training dataset
2023-04-29 03:42:13,467:INFO:Defining folds
2023-04-29 03:42:13,467:INFO:Declaring metric variables
2023-04-29 03:42:13,487:INFO:Importing untrained model
2023-04-29 03:42:13,504:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 03:42:13,540:INFO:Starting cross validation
2023-04-29 03:42:13,548:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:13,756:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,792:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,845:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,854:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,871:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,876:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,876:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:13,885:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,918:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,947:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,947:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,951:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:13,964:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,972:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,980:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,980:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,984:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:13,988:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,988:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,988:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,004:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,004:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,008:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,008:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,008:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,008:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,017:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:14,021:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,021:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:14,033:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,033:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,037:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,069:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,073:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,073:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,086:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,102:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,102:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,102:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,102:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,114:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,114:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,114:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,114:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,122:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,122:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,122:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,134:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,134:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,134:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,139:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,139:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,143:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,143:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,143:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,143:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,147:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,151:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,159:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,192:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,192:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,192:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,200:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,204:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,204:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,204:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,212:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,216:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,216:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,216:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,224:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,236:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,236:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,236:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,244:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,256:INFO:Calculating mean and std
2023-04-29 03:42:14,269:INFO:Creating metrics dataframe
2023-04-29 03:42:14,370:INFO:Uploading results into container
2023-04-29 03:42:14,376:INFO:Uploading model into container now
2023-04-29 03:42:14,380:INFO:_master_model_container: 8
2023-04-29 03:42:14,380:INFO:_display_container: 2
2023-04-29 03:42:14,383:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 03:42:14,383:INFO:create_model() successfully completed......................................
2023-04-29 03:42:14,642:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:14,646:INFO:Creating metrics dataframe
2023-04-29 03:42:14,686:INFO:Initializing Ada Boost Classifier
2023-04-29 03:42:14,686:INFO:Total runtime is 0.582974672317505 minutes
2023-04-29 03:42:14,698:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:14,698:INFO:Initializing create_model()
2023-04-29 03:42:14,702:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:14,702:INFO:Checking exceptions
2023-04-29 03:42:14,702:INFO:Importing libraries
2023-04-29 03:42:14,702:INFO:Copying training dataset
2023-04-29 03:42:14,714:INFO:Defining folds
2023-04-29 03:42:14,714:INFO:Declaring metric variables
2023-04-29 03:42:14,730:INFO:Importing untrained model
2023-04-29 03:42:14,742:INFO:Ada Boost Classifier Imported successfully
2023-04-29 03:42:14,771:INFO:Starting cross validation
2023-04-29 03:42:14,775:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:15,745:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,773:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,779:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,800:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,836:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,856:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,872:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,884:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,892:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,928:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,936:INFO:Calculating mean and std
2023-04-29 03:42:15,936:INFO:Creating metrics dataframe
2023-04-29 03:42:15,956:INFO:Uploading results into container
2023-04-29 03:42:15,960:INFO:Uploading model into container now
2023-04-29 03:42:15,960:INFO:_master_model_container: 9
2023-04-29 03:42:15,964:INFO:_display_container: 2
2023-04-29 03:42:15,964:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2023-04-29 03:42:15,964:INFO:create_model() successfully completed......................................
2023-04-29 03:42:16,269:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:16,269:INFO:Creating metrics dataframe
2023-04-29 03:42:16,305:INFO:Initializing Gradient Boosting Classifier
2023-04-29 03:42:16,305:INFO:Total runtime is 0.6099696914354962 minutes
2023-04-29 03:42:16,317:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:16,317:INFO:Initializing create_model()
2023-04-29 03:42:16,317:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:16,317:INFO:Checking exceptions
2023-04-29 03:42:16,317:INFO:Importing libraries
2023-04-29 03:42:16,317:INFO:Copying training dataset
2023-04-29 03:42:16,337:INFO:Defining folds
2023-04-29 03:42:16,337:INFO:Declaring metric variables
2023-04-29 03:42:16,353:INFO:Importing untrained model
2023-04-29 03:42:16,361:INFO:Gradient Boosting Classifier Imported successfully
2023-04-29 03:42:16,386:INFO:Starting cross validation
2023-04-29 03:42:16,390:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:18,227:INFO:Calculating mean and std
2023-04-29 03:42:18,230:INFO:Creating metrics dataframe
2023-04-29 03:42:18,242:INFO:Uploading results into container
2023-04-29 03:42:18,242:INFO:Uploading model into container now
2023-04-29 03:42:18,242:INFO:_master_model_container: 10
2023-04-29 03:42:18,242:INFO:_display_container: 2
2023-04-29 03:42:18,246:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-04-29 03:42:18,246:INFO:create_model() successfully completed......................................
2023-04-29 03:42:18,560:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:18,560:INFO:Creating metrics dataframe
2023-04-29 03:42:18,588:INFO:Initializing Linear Discriminant Analysis
2023-04-29 03:42:18,592:INFO:Total runtime is 0.6480814695358278 minutes
2023-04-29 03:42:18,600:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:18,604:INFO:Initializing create_model()
2023-04-29 03:42:18,604:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:18,604:INFO:Checking exceptions
2023-04-29 03:42:18,604:INFO:Importing libraries
2023-04-29 03:42:18,604:INFO:Copying training dataset
2023-04-29 03:42:18,616:INFO:Defining folds
2023-04-29 03:42:18,616:INFO:Declaring metric variables
2023-04-29 03:42:18,628:INFO:Importing untrained model
2023-04-29 03:42:18,641:INFO:Linear Discriminant Analysis Imported successfully
2023-04-29 03:42:18,663:INFO:Starting cross validation
2023-04-29 03:42:18,665:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:19,185:INFO:Calculating mean and std
2023-04-29 03:42:19,188:INFO:Creating metrics dataframe
2023-04-29 03:42:19,196:INFO:Uploading results into container
2023-04-29 03:42:19,196:INFO:Uploading model into container now
2023-04-29 03:42:19,196:INFO:_master_model_container: 11
2023-04-29 03:42:19,196:INFO:_display_container: 2
2023-04-29 03:42:19,200:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-04-29 03:42:19,200:INFO:create_model() successfully completed......................................
2023-04-29 03:42:19,409:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:19,409:INFO:Creating metrics dataframe
2023-04-29 03:42:19,461:INFO:Initializing Extra Trees Classifier
2023-04-29 03:42:19,461:INFO:Total runtime is 0.6625618298848471 minutes
2023-04-29 03:42:19,469:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:19,473:INFO:Initializing create_model()
2023-04-29 03:42:19,473:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:19,473:INFO:Checking exceptions
2023-04-29 03:42:19,473:INFO:Importing libraries
2023-04-29 03:42:19,473:INFO:Copying training dataset
2023-04-29 03:42:19,482:INFO:Defining folds
2023-04-29 03:42:19,482:INFO:Declaring metric variables
2023-04-29 03:42:19,494:INFO:Importing untrained model
2023-04-29 03:42:19,506:INFO:Extra Trees Classifier Imported successfully
2023-04-29 03:42:19,526:INFO:Starting cross validation
2023-04-29 03:42:19,526:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:20,453:INFO:Calculating mean and std
2023-04-29 03:42:20,458:INFO:Creating metrics dataframe
2023-04-29 03:42:20,466:INFO:Uploading results into container
2023-04-29 03:42:20,466:INFO:Uploading model into container now
2023-04-29 03:42:20,470:INFO:_master_model_container: 12
2023-04-29 03:42:20,470:INFO:_display_container: 2
2023-04-29 03:42:20,470:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 03:42:20,470:INFO:create_model() successfully completed......................................
2023-04-29 03:42:20,711:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:20,711:INFO:Creating metrics dataframe
2023-04-29 03:42:20,747:INFO:Initializing Light Gradient Boosting Machine
2023-04-29 03:42:20,747:INFO:Total runtime is 0.6839951356252035 minutes
2023-04-29 03:42:20,755:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:20,755:INFO:Initializing create_model()
2023-04-29 03:42:20,755:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:20,755:INFO:Checking exceptions
2023-04-29 03:42:20,755:INFO:Importing libraries
2023-04-29 03:42:20,755:INFO:Copying training dataset
2023-04-29 03:42:20,767:INFO:Defining folds
2023-04-29 03:42:20,767:INFO:Declaring metric variables
2023-04-29 03:42:20,771:INFO:Importing untrained model
2023-04-29 03:42:20,779:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 03:42:20,795:INFO:Starting cross validation
2023-04-29 03:42:20,799:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:24,176:INFO:Calculating mean and std
2023-04-29 03:42:24,180:INFO:Creating metrics dataframe
2023-04-29 03:42:24,188:INFO:Uploading results into container
2023-04-29 03:42:24,188:INFO:Uploading model into container now
2023-04-29 03:42:24,188:INFO:_master_model_container: 13
2023-04-29 03:42:24,188:INFO:_display_container: 2
2023-04-29 03:42:24,192:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 03:42:24,192:INFO:create_model() successfully completed......................................
2023-04-29 03:42:24,490:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:24,490:INFO:Creating metrics dataframe
2023-04-29 03:42:24,527:INFO:Initializing Dummy Classifier
2023-04-29 03:42:24,527:INFO:Total runtime is 0.7469920794169109 minutes
2023-04-29 03:42:24,535:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:24,539:INFO:Initializing create_model()
2023-04-29 03:42:24,539:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:24,539:INFO:Checking exceptions
2023-04-29 03:42:24,539:INFO:Importing libraries
2023-04-29 03:42:24,539:INFO:Copying training dataset
2023-04-29 03:42:24,551:INFO:Defining folds
2023-04-29 03:42:24,551:INFO:Declaring metric variables
2023-04-29 03:42:24,563:INFO:Importing untrained model
2023-04-29 03:42:24,571:INFO:Dummy Classifier Imported successfully
2023-04-29 03:42:24,591:INFO:Starting cross validation
2023-04-29 03:42:24,595:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:24,957:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:24,977:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:24,977:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:24,997:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,001:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,025:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,029:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,033:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,037:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,065:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,073:INFO:Calculating mean and std
2023-04-29 03:42:25,077:INFO:Creating metrics dataframe
2023-04-29 03:42:25,085:INFO:Uploading results into container
2023-04-29 03:42:25,085:INFO:Uploading model into container now
2023-04-29 03:42:25,089:INFO:_master_model_container: 14
2023-04-29 03:42:25,089:INFO:_display_container: 2
2023-04-29 03:42:25,089:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2023-04-29 03:42:25,089:INFO:create_model() successfully completed......................................
2023-04-29 03:42:25,421:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:25,421:INFO:Creating metrics dataframe
2023-04-29 03:42:25,501:INFO:Initializing create_model()
2023-04-29 03:42:25,501:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:25,501:INFO:Checking exceptions
2023-04-29 03:42:25,505:INFO:Importing libraries
2023-04-29 03:42:25,505:INFO:Copying training dataset
2023-04-29 03:42:25,513:INFO:Defining folds
2023-04-29 03:42:25,513:INFO:Declaring metric variables
2023-04-29 03:42:25,513:INFO:Importing untrained model
2023-04-29 03:42:25,513:INFO:Declaring custom model
2023-04-29 03:42:25,513:INFO:Random Forest Classifier Imported successfully
2023-04-29 03:42:25,518:INFO:Cross validation set to False
2023-04-29 03:42:25,518:INFO:Fitting Model
2023-04-29 03:42:25,902:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 03:42:25,902:INFO:create_model() successfully completed......................................
2023-04-29 03:42:26,387:INFO:_master_model_container: 14
2023-04-29 03:42:26,387:INFO:_display_container: 2
2023-04-29 03:42:26,392:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 03:42:26,392:INFO:compare_models() successfully completed......................................
2023-04-29 03:42:26,470:INFO:Initializing create_model()
2023-04-29 03:42:26,470:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=ridge, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:26,470:INFO:Checking exceptions
2023-04-29 03:42:26,548:INFO:Importing libraries
2023-04-29 03:42:26,548:INFO:Copying training dataset
2023-04-29 03:42:26,557:INFO:Defining folds
2023-04-29 03:42:26,557:INFO:Declaring metric variables
2023-04-29 03:42:26,565:INFO:Importing untrained model
2023-04-29 03:42:26,573:INFO:Ridge Classifier Imported successfully
2023-04-29 03:42:26,593:INFO:Starting cross validation
2023-04-29 03:42:26,597:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:26,907:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:26,979:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:26,991:INFO:Calculating mean and std
2023-04-29 03:42:26,991:INFO:Creating metrics dataframe
2023-04-29 03:42:27,007:INFO:Finalizing model
2023-04-29 03:42:27,091:INFO:Uploading results into container
2023-04-29 03:42:27,095:INFO:Uploading model into container now
2023-04-29 03:42:27,123:INFO:_master_model_container: 15
2023-04-29 03:42:27,127:INFO:_display_container: 3
2023-04-29 03:42:27,127:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:42:27,127:INFO:create_model() successfully completed......................................
2023-04-29 03:42:27,520:INFO:Initializing tune_model()
2023-04-29 03:42:27,524:INFO:tune_model(estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>)
2023-04-29 03:42:27,524:INFO:Checking exceptions
2023-04-29 03:42:27,612:INFO:Copying training dataset
2023-04-29 03:42:27,624:INFO:Checking base model
2023-04-29 03:42:27,624:INFO:Base model : Ridge Classifier
2023-04-29 03:42:27,632:INFO:Declaring metric variables
2023-04-29 03:42:27,644:INFO:Defining Hyperparameters
2023-04-29 03:42:27,945:INFO:Tuning with n_jobs=-1
2023-04-29 03:42:27,945:INFO:Initializing RandomizedSearchCV
2023-04-29 03:42:28,127:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,220:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,236:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,252:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,268:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,292:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,309:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,333:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,349:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,361:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,377:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,405:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,417:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,421:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,437:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,465:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,481:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,481:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,498:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,510:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,526:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,554:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,558:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,582:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,606:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,626:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,642:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,654:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,670:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,682:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,699:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,710:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,738:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,762:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,782:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,798:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,818:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,830:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,850:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,870:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,878:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,898:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,910:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,926:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,942:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,958:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,978:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,007:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,019:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,031:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,051:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,063:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,083:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,095:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,110:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,126:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,151:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,179:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,195:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,215:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,231:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,247:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,259:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,275:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,287:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,307:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,319:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,335:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,359:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,387:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,407:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,423:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,436:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,452:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,467:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,483:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,495:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,515:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,527:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,543:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,559:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,599:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,599:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,616:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,629:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,645:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,657:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,677:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,689:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,707:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,721:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,737:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,749:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,761:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,781:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,789:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,814:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,822:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,838:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,846:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,894:INFO:best_params: {'actual_estimator__normalize': False, 'actual_estimator__fit_intercept': True, 'actual_estimator__alpha': 8.6}
2023-04-29 03:42:29,894:INFO:Hyperparameter search completed
2023-04-29 03:42:29,898:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:29,898:INFO:Initializing create_model()
2023-04-29 03:42:29,898:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE4726F40>, model_only=True, return_train_score=False, kwargs={'normalize': False, 'fit_intercept': True, 'alpha': 8.6})
2023-04-29 03:42:29,898:INFO:Checking exceptions
2023-04-29 03:42:29,898:INFO:Importing libraries
2023-04-29 03:42:29,898:INFO:Copying training dataset
2023-04-29 03:42:29,902:INFO:Defining folds
2023-04-29 03:42:29,906:INFO:Declaring metric variables
2023-04-29 03:42:29,910:INFO:Importing untrained model
2023-04-29 03:42:29,914:INFO:Declaring custom model
2023-04-29 03:42:29,922:INFO:Ridge Classifier Imported successfully
2023-04-29 03:42:29,938:INFO:Starting cross validation
2023-04-29 03:42:29,938:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:30,050:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,070:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,090:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,115:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,127:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,140:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:30,144:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,160:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,164:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,180:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,196:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,202:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:30,253:INFO:Calculating mean and std
2023-04-29 03:42:30,257:INFO:Creating metrics dataframe
2023-04-29 03:42:30,273:INFO:Finalizing model
2023-04-29 03:42:30,341:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,361:INFO:Uploading results into container
2023-04-29 03:42:30,361:INFO:Uploading model into container now
2023-04-29 03:42:30,365:INFO:_master_model_container: 16
2023-04-29 03:42:30,365:INFO:_display_container: 4
2023-04-29 03:42:30,365:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:42:30,365:INFO:create_model() successfully completed......................................
2023-04-29 03:42:30,685:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:30,685:INFO:choose_better activated
2023-04-29 03:42:30,697:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:30,697:INFO:Initializing create_model()
2023-04-29 03:42:30,697:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:30,697:INFO:Checking exceptions
2023-04-29 03:42:30,702:INFO:Importing libraries
2023-04-29 03:42:30,705:INFO:Copying training dataset
2023-04-29 03:42:30,710:INFO:Defining folds
2023-04-29 03:42:30,710:INFO:Declaring metric variables
2023-04-29 03:42:30,714:INFO:Importing untrained model
2023-04-29 03:42:30,714:INFO:Declaring custom model
2023-04-29 03:42:30,714:INFO:Ridge Classifier Imported successfully
2023-04-29 03:42:30,714:INFO:Starting cross validation
2023-04-29 03:42:30,718:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:30,974:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:31,091:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:31,120:INFO:Calculating mean and std
2023-04-29 03:42:31,120:INFO:Creating metrics dataframe
2023-04-29 03:42:31,124:INFO:Finalizing model
2023-04-29 03:42:31,180:INFO:Uploading results into container
2023-04-29 03:42:31,184:INFO:Uploading model into container now
2023-04-29 03:42:31,184:INFO:_master_model_container: 17
2023-04-29 03:42:31,184:INFO:_display_container: 5
2023-04-29 03:42:31,184:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:42:31,184:INFO:create_model() successfully completed......................................
2023-04-29 03:42:31,473:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:31,477:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001) result for Accuracy is 0.8946
2023-04-29 03:42:31,477:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001) result for Accuracy is 0.9013
2023-04-29 03:42:31,477:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001) is best model
2023-04-29 03:42:31,481:INFO:choose_better completed
2023-04-29 03:42:31,512:INFO:_master_model_container: 17
2023-04-29 03:42:31,514:INFO:_display_container: 4
2023-04-29 03:42:31,514:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:42:31,514:INFO:tune_model() successfully completed......................................
2023-04-29 03:42:31,879:INFO:Initializing evaluate_model()
2023-04-29 03:42:31,879:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:42:31,943:INFO:Initializing plot_model()
2023-04-29 03:42:31,943:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 03:42:31,943:INFO:Checking exceptions
2023-04-29 03:42:31,951:INFO:Preloading libraries
2023-04-29 03:42:31,951:INFO:Copying training dataset
2023-04-29 03:42:31,951:INFO:Plot type: pipeline
2023-04-29 03:42:32,472:INFO:Visual Rendered Successfully
2023-04-29 03:42:32,805:INFO:plot_model() successfully completed......................................
2023-04-29 03:42:32,821:INFO:Initializing create_model()
2023-04-29 03:42:32,821:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=nb, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:32,821:INFO:Checking exceptions
2023-04-29 03:42:32,871:INFO:Importing libraries
2023-04-29 03:42:32,875:INFO:Copying training dataset
2023-04-29 03:42:32,879:INFO:Defining folds
2023-04-29 03:42:32,883:INFO:Declaring metric variables
2023-04-29 03:42:32,887:INFO:Importing untrained model
2023-04-29 03:42:32,895:INFO:Naive Bayes Imported successfully
2023-04-29 03:42:32,911:INFO:Starting cross validation
2023-04-29 03:42:32,911:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:33,284:INFO:Calculating mean and std
2023-04-29 03:42:33,284:INFO:Creating metrics dataframe
2023-04-29 03:42:33,296:INFO:Finalizing model
2023-04-29 03:42:33,357:INFO:Uploading results into container
2023-04-29 03:42:33,357:INFO:Uploading model into container now
2023-04-29 03:42:33,381:INFO:_master_model_container: 18
2023-04-29 03:42:33,381:INFO:_display_container: 5
2023-04-29 03:42:33,381:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:42:33,381:INFO:create_model() successfully completed......................................
2023-04-29 03:42:33,607:INFO:Initializing tune_model()
2023-04-29 03:42:33,607:INFO:tune_model(estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>)
2023-04-29 03:42:33,607:INFO:Checking exceptions
2023-04-29 03:42:33,711:INFO:Copying training dataset
2023-04-29 03:42:33,716:INFO:Checking base model
2023-04-29 03:42:33,716:INFO:Base model : Naive Bayes
2023-04-29 03:42:33,724:INFO:Declaring metric variables
2023-04-29 03:42:33,732:INFO:Defining Hyperparameters
2023-04-29 03:42:33,928:INFO:Tuning with n_jobs=-1
2023-04-29 03:42:33,928:INFO:Initializing RandomizedSearchCV
2023-04-29 03:42:35,937:INFO:best_params: {'actual_estimator__var_smoothing': 2e-09}
2023-04-29 03:42:35,942:INFO:Hyperparameter search completed
2023-04-29 03:42:35,942:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:35,944:INFO:Initializing create_model()
2023-04-29 03:42:35,944:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE1762BE0>, model_only=True, return_train_score=False, kwargs={'var_smoothing': 2e-09})
2023-04-29 03:42:35,944:INFO:Checking exceptions
2023-04-29 03:42:35,946:INFO:Importing libraries
2023-04-29 03:42:35,946:INFO:Copying training dataset
2023-04-29 03:42:35,954:INFO:Defining folds
2023-04-29 03:42:35,954:INFO:Declaring metric variables
2023-04-29 03:42:35,964:INFO:Importing untrained model
2023-04-29 03:42:35,964:INFO:Declaring custom model
2023-04-29 03:42:35,974:INFO:Naive Bayes Imported successfully
2023-04-29 03:42:35,992:INFO:Starting cross validation
2023-04-29 03:42:35,996:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:36,523:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:36,583:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:36,742:INFO:Calculating mean and std
2023-04-29 03:42:36,748:INFO:Creating metrics dataframe
2023-04-29 03:42:36,774:INFO:Finalizing model
2023-04-29 03:42:36,898:INFO:Uploading results into container
2023-04-29 03:42:36,900:INFO:Uploading model into container now
2023-04-29 03:42:36,903:INFO:_master_model_container: 19
2023-04-29 03:42:36,903:INFO:_display_container: 6
2023-04-29 03:42:36,905:INFO:GaussianNB(priors=None, var_smoothing=2e-09)
2023-04-29 03:42:36,905:INFO:create_model() successfully completed......................................
2023-04-29 03:42:37,199:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:37,199:INFO:choose_better activated
2023-04-29 03:42:37,213:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:37,215:INFO:Initializing create_model()
2023-04-29 03:42:37,215:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:37,215:INFO:Checking exceptions
2023-04-29 03:42:37,221:INFO:Importing libraries
2023-04-29 03:42:37,221:INFO:Copying training dataset
2023-04-29 03:42:37,231:INFO:Defining folds
2023-04-29 03:42:37,231:INFO:Declaring metric variables
2023-04-29 03:42:37,231:INFO:Importing untrained model
2023-04-29 03:42:37,231:INFO:Declaring custom model
2023-04-29 03:42:37,233:INFO:Naive Bayes Imported successfully
2023-04-29 03:42:37,233:INFO:Starting cross validation
2023-04-29 03:42:37,237:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:38,022:INFO:Calculating mean and std
2023-04-29 03:42:38,024:INFO:Creating metrics dataframe
2023-04-29 03:42:38,034:INFO:Finalizing model
2023-04-29 03:42:38,136:INFO:Uploading results into container
2023-04-29 03:42:38,138:INFO:Uploading model into container now
2023-04-29 03:42:38,140:INFO:_master_model_container: 20
2023-04-29 03:42:38,140:INFO:_display_container: 7
2023-04-29 03:42:38,140:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:42:38,142:INFO:create_model() successfully completed......................................
2023-04-29 03:42:38,436:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:38,438:INFO:GaussianNB(priors=None, var_smoothing=1e-09) result for Accuracy is 0.8408
2023-04-29 03:42:38,440:INFO:GaussianNB(priors=None, var_smoothing=2e-09) result for Accuracy is 0.8142
2023-04-29 03:42:38,440:INFO:GaussianNB(priors=None, var_smoothing=1e-09) is best model
2023-04-29 03:42:38,442:INFO:choose_better completed
2023-04-29 03:42:38,442:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 03:42:38,480:INFO:_master_model_container: 20
2023-04-29 03:42:38,480:INFO:_display_container: 6
2023-04-29 03:42:38,482:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:42:38,482:INFO:tune_model() successfully completed......................................
2023-04-29 03:42:38,835:INFO:Initializing evaluate_model()
2023-04-29 03:42:38,837:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:42:38,921:INFO:Initializing plot_model()
2023-04-29 03:42:38,923:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GaussianNB(priors=None, var_smoothing=1e-09), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 03:42:38,923:INFO:Checking exceptions
2023-04-29 03:42:38,929:INFO:Preloading libraries
2023-04-29 03:42:38,931:INFO:Copying training dataset
2023-04-29 03:42:38,931:INFO:Plot type: pipeline
2023-04-29 03:42:39,494:INFO:Visual Rendered Successfully
2023-04-29 03:42:39,753:INFO:plot_model() successfully completed......................................
2023-04-29 03:42:39,783:INFO:Initializing create_model()
2023-04-29 03:42:39,785:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:39,785:INFO:Checking exceptions
2023-04-29 03:42:39,882:INFO:Importing libraries
2023-04-29 03:42:39,882:INFO:Copying training dataset
2023-04-29 03:42:39,892:INFO:Defining folds
2023-04-29 03:42:39,894:INFO:Declaring metric variables
2023-04-29 03:42:39,906:INFO:Importing untrained model
2023-04-29 03:42:39,922:INFO:Logistic Regression Imported successfully
2023-04-29 03:42:39,954:INFO:Starting cross validation
2023-04-29 03:42:39,959:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:43,307:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:43,799:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:43,826:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:43,922:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,027:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,056:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,271:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,281:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,505:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,640:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,798:INFO:Calculating mean and std
2023-04-29 03:42:44,804:INFO:Creating metrics dataframe
2023-04-29 03:42:44,822:INFO:Finalizing model
2023-04-29 03:42:46,228:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:46,258:INFO:Uploading results into container
2023-04-29 03:42:46,262:INFO:Uploading model into container now
2023-04-29 03:42:46,307:INFO:_master_model_container: 21
2023-04-29 03:42:46,307:INFO:_display_container: 7
2023-04-29 03:42:46,310:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:42:46,311:INFO:create_model() successfully completed......................................
2023-04-29 03:42:46,702:INFO:Initializing tune_model()
2023-04-29 03:42:46,702:INFO:tune_model(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>)
2023-04-29 03:42:46,702:INFO:Checking exceptions
2023-04-29 03:42:46,896:INFO:Copying training dataset
2023-04-29 03:42:46,908:INFO:Checking base model
2023-04-29 03:42:46,908:INFO:Base model : Logistic Regression
2023-04-29 03:42:46,922:INFO:Declaring metric variables
2023-04-29 03:42:46,930:INFO:Defining Hyperparameters
2023-04-29 03:42:47,250:INFO:Tuning with n_jobs=-1
2023-04-29 03:42:47,252:INFO:Initializing RandomizedSearchCV
2023-04-29 03:42:50,463:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:50,467:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:50,810:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,035:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,164:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,214:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,255:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,358:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,724:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:52,085:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:54,574:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:54,590:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,555:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,566:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,728:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,735:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,751:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,798:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,940:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:56,091:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:56,775:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:56,819:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:56,904:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:56,904:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:57,072:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:57,190:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:57,196:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:57,263:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:57,586:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,055:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,109:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,241:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,250:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,391:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,460:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,542:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,700:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,868:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,262:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,446:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,647:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,672:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,768:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,864:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,991:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,044:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,093:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,267:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,428:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,748:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,830:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,984:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,175:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,246:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,279:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,410:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,526:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,860:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,217:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,255:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,278:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,290:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,431:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,659:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,738:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,844:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,860:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,096:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,411:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,498:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,501:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,680:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,762:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,939:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,245:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,404:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,782:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,794:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,880:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,901:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,974:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,050:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,156:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,561:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,685:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,828:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,836:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,178:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,255:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,368:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,378:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,440:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,476:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,521:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 7.689}
2023-04-29 03:43:06,525:INFO:Hyperparameter search completed
2023-04-29 03:43:06,525:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:06,525:INFO:Initializing create_model()
2023-04-29 03:43:06,525:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE4D6AC70>, model_only=True, return_train_score=False, kwargs={'class_weight': {}, 'C': 7.689})
2023-04-29 03:43:06,527:INFO:Checking exceptions
2023-04-29 03:43:06,527:INFO:Importing libraries
2023-04-29 03:43:06,527:INFO:Copying training dataset
2023-04-29 03:43:06,533:INFO:Defining folds
2023-04-29 03:43:06,533:INFO:Declaring metric variables
2023-04-29 03:43:06,541:INFO:Importing untrained model
2023-04-29 03:43:06,541:INFO:Declaring custom model
2023-04-29 03:43:06,552:INFO:Logistic Regression Imported successfully
2023-04-29 03:43:06,570:INFO:Starting cross validation
2023-04-29 03:43:06,574:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:07,629:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:07,721:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:07,952:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:07,952:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:07,954:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,003:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,084:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,093:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,142:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,299:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,393:INFO:Calculating mean and std
2023-04-29 03:43:08,396:INFO:Creating metrics dataframe
2023-04-29 03:43:08,411:INFO:Finalizing model
2023-04-29 03:43:09,405:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:09,416:INFO:Uploading results into container
2023-04-29 03:43:09,416:INFO:Uploading model into container now
2023-04-29 03:43:09,419:INFO:_master_model_container: 22
2023-04-29 03:43:09,419:INFO:_display_container: 8
2023-04-29 03:43:09,419:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:43:09,419:INFO:create_model() successfully completed......................................
2023-04-29 03:43:09,626:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:09,626:INFO:choose_better activated
2023-04-29 03:43:09,635:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:09,635:INFO:Initializing create_model()
2023-04-29 03:43:09,635:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:09,635:INFO:Checking exceptions
2023-04-29 03:43:09,639:INFO:Importing libraries
2023-04-29 03:43:09,639:INFO:Copying training dataset
2023-04-29 03:43:09,646:INFO:Defining folds
2023-04-29 03:43:09,646:INFO:Declaring metric variables
2023-04-29 03:43:09,646:INFO:Importing untrained model
2023-04-29 03:43:09,647:INFO:Declaring custom model
2023-04-29 03:43:09,647:INFO:Logistic Regression Imported successfully
2023-04-29 03:43:09,651:INFO:Starting cross validation
2023-04-29 03:43:09,651:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:10,845:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:10,915:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:10,983:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,038:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,040:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,126:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,144:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,162:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,246:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,297:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,396:INFO:Calculating mean and std
2023-04-29 03:43:11,397:INFO:Creating metrics dataframe
2023-04-29 03:43:11,404:INFO:Finalizing model
2023-04-29 03:43:12,234:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:12,238:INFO:Uploading results into container
2023-04-29 03:43:12,238:INFO:Uploading model into container now
2023-04-29 03:43:12,238:INFO:_master_model_container: 23
2023-04-29 03:43:12,238:INFO:_display_container: 9
2023-04-29 03:43:12,242:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:43:12,242:INFO:create_model() successfully completed......................................
2023-04-29 03:43:12,463:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:12,463:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.9404
2023-04-29 03:43:12,467:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.9604
2023-04-29 03:43:12,467:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2023-04-29 03:43:12,470:INFO:choose_better completed
2023-04-29 03:43:12,493:INFO:_master_model_container: 23
2023-04-29 03:43:12,496:INFO:_display_container: 8
2023-04-29 03:43:12,496:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:43:12,496:INFO:tune_model() successfully completed......................................
2023-04-29 03:43:12,833:INFO:Initializing evaluate_model()
2023-04-29 03:43:12,833:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:43:12,871:INFO:Initializing plot_model()
2023-04-29 03:43:12,871:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 03:43:12,871:INFO:Checking exceptions
2023-04-29 03:43:12,875:INFO:Preloading libraries
2023-04-29 03:43:12,875:INFO:Copying training dataset
2023-04-29 03:43:12,875:INFO:Plot type: pipeline
2023-04-29 03:43:13,200:INFO:Visual Rendered Successfully
2023-04-29 03:43:13,404:INFO:plot_model() successfully completed......................................
