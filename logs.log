2023-04-29 03:41:34,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 03:41:34,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 03:41:34,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 03:41:34,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 03:41:36,272:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-04-29 03:41:36,712:INFO:PyCaret ClassificationExperiment
2023-04-29 03:41:36,712:INFO:Logging name: clf-default-name
2023-04-29 03:41:36,712:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-04-29 03:41:36,712:INFO:version 3.0.0.rc8
2023-04-29 03:41:36,712:INFO:Initializing setup()
2023-04-29 03:41:36,714:INFO:self.USI: d32e
2023-04-29 03:41:36,714:INFO:self._variable_keys: {'gpu_param', '_available_plots', '_ml_usecase', 'fold_shuffle_param', 'X_train', 'data', 'target_param', 'logging_param', 'X', 'memory', 'html_param', 'fix_imbalance', 'pipeline', 'log_plots_param', 'n_jobs_param', 'exp_id', 'fold_groups_param', 'is_multiclass', 'X_test', 'y_test', 'fold_generator', 'exp_name_log', 'USI', 'seed', 'gpu_n_jobs_param', 'idx', 'y_train', 'y'}
2023-04-29 03:41:36,714:INFO:Checking environment
2023-04-29 03:41:36,714:INFO:python_version: 3.9.13
2023-04-29 03:41:36,714:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-04-29 03:41:36,714:INFO:machine: AMD64
2023-04-29 03:41:36,714:INFO:platform: Windows-10-10.0.22621-SP0
2023-04-29 03:41:36,714:INFO:Memory: svmem(total=16782143488, available=1711984640, percent=89.8, used=15070158848, free=1711984640)
2023-04-29 03:41:36,714:INFO:Physical Core: 10
2023-04-29 03:41:36,714:INFO:Logical Core: 12
2023-04-29 03:41:36,714:INFO:Checking libraries
2023-04-29 03:41:36,714:INFO:System:
2023-04-29 03:41:36,714:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-04-29 03:41:36,714:INFO:executable: C:\Users\eeman\anaconda3\python.exe
2023-04-29 03:41:36,714:INFO:   machine: Windows-10-10.0.22621-SP0
2023-04-29 03:41:36,714:INFO:PyCaret required dependencies:
2023-04-29 03:41:36,714:INFO:                 pip: 22.2.2
2023-04-29 03:41:36,716:INFO:          setuptools: 63.4.1
2023-04-29 03:41:36,716:INFO:             pycaret: 3.0.0rc8
2023-04-29 03:41:36,716:INFO:             IPython: 7.31.1
2023-04-29 03:41:36,716:INFO:          ipywidgets: 7.6.5
2023-04-29 03:41:36,716:INFO:                tqdm: 4.64.1
2023-04-29 03:41:36,716:INFO:               numpy: 1.21.5
2023-04-29 03:41:36,716:INFO:              pandas: 1.4.4
2023-04-29 03:41:36,716:INFO:              jinja2: 2.11.3
2023-04-29 03:41:36,716:INFO:               scipy: 1.9.1
2023-04-29 03:41:36,716:INFO:              joblib: 1.2.0
2023-04-29 03:41:36,716:INFO:             sklearn: 1.0.2
2023-04-29 03:41:36,716:INFO:                pyod: 1.0.7
2023-04-29 03:41:36,716:INFO:            imblearn: 0.10.1
2023-04-29 03:41:36,716:INFO:   category_encoders: 2.6.0
2023-04-29 03:41:36,716:INFO:            lightgbm: 3.3.4
2023-04-29 03:41:36,716:INFO:               numba: 0.55.1
2023-04-29 03:41:36,716:INFO:            requests: 2.28.1
2023-04-29 03:41:36,716:INFO:          matplotlib: 3.5.2
2023-04-29 03:41:36,716:INFO:          scikitplot: 0.3.7
2023-04-29 03:41:36,716:INFO:         yellowbrick: 1.5
2023-04-29 03:41:36,716:INFO:              plotly: 5.9.0
2023-04-29 03:41:36,716:INFO:             kaleido: 0.2.1
2023-04-29 03:41:36,716:INFO:         statsmodels: 0.13.5
2023-04-29 03:41:36,718:INFO:              sktime: 0.15.1
2023-04-29 03:41:36,718:INFO:               tbats: 1.1.2
2023-04-29 03:41:36,718:INFO:            pmdarima: 2.0.2
2023-04-29 03:41:36,718:INFO:              psutil: 5.9.0
2023-04-29 03:41:36,718:INFO:PyCaret optional dependencies:
2023-04-29 03:41:36,763:INFO:                shap: 0.41.0
2023-04-29 03:41:36,763:INFO:           interpret: Not installed
2023-04-29 03:41:36,763:INFO:                umap: Not installed
2023-04-29 03:41:36,763:INFO:    pandas_profiling: Not installed
2023-04-29 03:41:36,763:INFO:  explainerdashboard: Not installed
2023-04-29 03:41:36,763:INFO:             autoviz: Not installed
2023-04-29 03:41:36,763:INFO:           fairlearn: Not installed
2023-04-29 03:41:36,763:INFO:             xgboost: Not installed
2023-04-29 03:41:36,763:INFO:            catboost: Not installed
2023-04-29 03:41:36,763:INFO:              kmodes: Not installed
2023-04-29 03:41:36,763:INFO:             mlxtend: Not installed
2023-04-29 03:41:36,763:INFO:       statsforecast: Not installed
2023-04-29 03:41:36,763:INFO:        tune_sklearn: Not installed
2023-04-29 03:41:36,763:INFO:                 ray: Not installed
2023-04-29 03:41:36,763:INFO:            hyperopt: Not installed
2023-04-29 03:41:36,765:INFO:              optuna: Not installed
2023-04-29 03:41:36,765:INFO:               skopt: Not installed
2023-04-29 03:41:36,765:INFO:              mlflow: Not installed
2023-04-29 03:41:36,765:INFO:              gradio: Not installed
2023-04-29 03:41:36,765:INFO:             fastapi: Not installed
2023-04-29 03:41:36,765:INFO:             uvicorn: Not installed
2023-04-29 03:41:36,765:INFO:              m2cgen: Not installed
2023-04-29 03:41:36,765:INFO:           evidently: Not installed
2023-04-29 03:41:36,765:INFO:                nltk: 3.7
2023-04-29 03:41:36,765:INFO:            pyLDAvis: Not installed
2023-04-29 03:41:36,765:INFO:              gensim: 4.1.2
2023-04-29 03:41:36,765:INFO:               spacy: Not installed
2023-04-29 03:41:36,765:INFO:           wordcloud: Not installed
2023-04-29 03:41:36,765:INFO:            textblob: Not installed
2023-04-29 03:41:36,765:INFO:               fugue: Not installed
2023-04-29 03:41:36,765:INFO:           streamlit: Not installed
2023-04-29 03:41:36,765:INFO:             prophet: Not installed
2023-04-29 03:41:36,765:INFO:None
2023-04-29 03:41:36,765:INFO:Set up data.
2023-04-29 03:41:36,777:INFO:Set up train/test split.
2023-04-29 03:41:36,787:INFO:Set up index.
2023-04-29 03:41:36,787:INFO:Set up folding strategy.
2023-04-29 03:41:36,787:INFO:Assigning column types.
2023-04-29 03:41:36,793:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-29 03:41:36,954:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-29 03:41:36,962:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:41:37,082:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,140:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,294:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-29 03:41:37,298:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:41:37,394:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,396:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,396:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-29 03:41:37,560:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:41:37,658:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,660:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,846:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:41:37,948:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,948:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:37,948:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-04-29 03:41:38,214:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:38,214:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:38,480:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:38,480:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:38,484:INFO:Preparing preprocessing pipeline...
2023-04-29 03:41:38,490:INFO:Set up simple imputation.
2023-04-29 03:41:38,496:INFO:Set up encoding of categorical features.
2023-04-29 03:41:38,585:INFO:Finished creating preprocessing pipeline.
2023-04-29 03:41:38,599:INFO:Pipeline: Pipeline(memory=Memory(location=C:\Users\eeman\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'L', 'R', 'A_M'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose=0))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    inc...
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Color', 'Spectral_Class'],
                                    transformer=OneHotEncoder(cols=['Color',
                                                                    'Spectral_Class'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2023-04-29 03:41:38,599:INFO:Creating final display dataframe.
2023-04-29 03:41:38,994:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target              Type
2                   Target type        Multiclass
3           Original data shape          (216, 7)
4        Transformed data shape         (216, 24)
5   Transformed train set shape         (151, 24)
6    Transformed test set shape          (65, 24)
7              Numeric features                 4
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              d32e
2023-04-29 03:41:39,306:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:39,308:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:39,564:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:39,566:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:41:39,566:INFO:setup() successfully completed in 2.86s...............
2023-04-29 03:41:39,590:INFO:Initializing compare_models()
2023-04-29 03:41:39,592:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-04-29 03:41:39,592:INFO:Checking exceptions
2023-04-29 03:41:39,596:INFO:Preparing display monitor
2023-04-29 03:41:39,707:INFO:Initializing Logistic Regression
2023-04-29 03:41:39,707:INFO:Total runtime is 0.0 minutes
2023-04-29 03:41:39,711:INFO:SubProcess create_model() called ==================================
2023-04-29 03:41:39,713:INFO:Initializing create_model()
2023-04-29 03:41:39,713:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:41:39,713:INFO:Checking exceptions
2023-04-29 03:41:39,713:INFO:Importing libraries
2023-04-29 03:41:39,713:INFO:Copying training dataset
2023-04-29 03:41:39,719:INFO:Defining folds
2023-04-29 03:41:39,719:INFO:Declaring metric variables
2023-04-29 03:41:39,726:INFO:Importing untrained model
2023-04-29 03:41:39,732:INFO:Logistic Regression Imported successfully
2023-04-29 03:41:39,745:INFO:Starting cross validation
2023-04-29 03:41:39,747:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:41:53,574:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:53,589:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:53,733:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:53,761:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:53,785:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,045:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,076:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,235:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,362:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,555:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:41:54,753:INFO:Calculating mean and std
2023-04-29 03:41:54,757:INFO:Creating metrics dataframe
2023-04-29 03:41:54,770:INFO:Uploading results into container
2023-04-29 03:41:54,773:INFO:Uploading model into container now
2023-04-29 03:41:54,773:INFO:_master_model_container: 1
2023-04-29 03:41:54,773:INFO:_display_container: 2
2023-04-29 03:41:54,773:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:41:54,773:INFO:create_model() successfully completed......................................
2023-04-29 03:41:55,060:INFO:SubProcess create_model() end ==================================
2023-04-29 03:41:55,065:INFO:Creating metrics dataframe
2023-04-29 03:41:55,090:INFO:Initializing K Neighbors Classifier
2023-04-29 03:41:55,090:INFO:Total runtime is 0.2563843488693237 minutes
2023-04-29 03:41:55,100:INFO:SubProcess create_model() called ==================================
2023-04-29 03:41:55,101:INFO:Initializing create_model()
2023-04-29 03:41:55,102:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:41:55,102:INFO:Checking exceptions
2023-04-29 03:41:55,102:INFO:Importing libraries
2023-04-29 03:41:55,102:INFO:Copying training dataset
2023-04-29 03:41:55,113:INFO:Defining folds
2023-04-29 03:41:55,113:INFO:Declaring metric variables
2023-04-29 03:41:55,121:INFO:Importing untrained model
2023-04-29 03:41:55,130:INFO:K Neighbors Classifier Imported successfully
2023-04-29 03:41:55,149:INFO:Starting cross validation
2023-04-29 03:41:55,150:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:41:55,618:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,628:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,631:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,647:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,689:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,701:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,755:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:41:55,875:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:41:55,883:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:04,643:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:42:04,865:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:42:05,020:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:05,032:INFO:Calculating mean and std
2023-04-29 03:42:05,036:INFO:Creating metrics dataframe
2023-04-29 03:42:05,052:INFO:Uploading results into container
2023-04-29 03:42:05,056:INFO:Uploading model into container now
2023-04-29 03:42:05,056:INFO:_master_model_container: 2
2023-04-29 03:42:05,056:INFO:_display_container: 2
2023-04-29 03:42:05,056:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-04-29 03:42:05,060:INFO:create_model() successfully completed......................................
2023-04-29 03:42:05,453:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:05,453:INFO:Creating metrics dataframe
2023-04-29 03:42:05,490:INFO:Initializing Naive Bayes
2023-04-29 03:42:05,490:INFO:Total runtime is 0.4297042449315389 minutes
2023-04-29 03:42:05,502:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:05,502:INFO:Initializing create_model()
2023-04-29 03:42:05,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:05,506:INFO:Checking exceptions
2023-04-29 03:42:05,507:INFO:Importing libraries
2023-04-29 03:42:05,507:INFO:Copying training dataset
2023-04-29 03:42:05,519:INFO:Defining folds
2023-04-29 03:42:05,519:INFO:Declaring metric variables
2023-04-29 03:42:05,531:INFO:Importing untrained model
2023-04-29 03:42:05,547:INFO:Naive Bayes Imported successfully
2023-04-29 03:42:05,571:INFO:Starting cross validation
2023-04-29 03:42:05,575:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:06,121:INFO:Calculating mean and std
2023-04-29 03:42:06,125:INFO:Creating metrics dataframe
2023-04-29 03:42:06,133:INFO:Uploading results into container
2023-04-29 03:42:06,137:INFO:Uploading model into container now
2023-04-29 03:42:06,141:INFO:_master_model_container: 3
2023-04-29 03:42:06,141:INFO:_display_container: 2
2023-04-29 03:42:06,145:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:42:06,145:INFO:create_model() successfully completed......................................
2023-04-29 03:42:06,399:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:06,399:INFO:Creating metrics dataframe
2023-04-29 03:42:06,431:INFO:Initializing Decision Tree Classifier
2023-04-29 03:42:06,431:INFO:Total runtime is 0.44539469480514526 minutes
2023-04-29 03:42:06,439:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:06,439:INFO:Initializing create_model()
2023-04-29 03:42:06,439:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:06,439:INFO:Checking exceptions
2023-04-29 03:42:06,439:INFO:Importing libraries
2023-04-29 03:42:06,439:INFO:Copying training dataset
2023-04-29 03:42:06,451:INFO:Defining folds
2023-04-29 03:42:06,455:INFO:Declaring metric variables
2023-04-29 03:42:06,467:INFO:Importing untrained model
2023-04-29 03:42:06,483:INFO:Decision Tree Classifier Imported successfully
2023-04-29 03:42:06,504:INFO:Starting cross validation
2023-04-29 03:42:06,508:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:07,038:INFO:Calculating mean and std
2023-04-29 03:42:07,042:INFO:Creating metrics dataframe
2023-04-29 03:42:07,050:INFO:Uploading results into container
2023-04-29 03:42:07,050:INFO:Uploading model into container now
2023-04-29 03:42:07,050:INFO:_master_model_container: 4
2023-04-29 03:42:07,054:INFO:_display_container: 2
2023-04-29 03:42:07,054:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=123, splitter='best')
2023-04-29 03:42:07,054:INFO:create_model() successfully completed......................................
2023-04-29 03:42:07,284:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:07,284:INFO:Creating metrics dataframe
2023-04-29 03:42:07,336:INFO:Initializing SVM - Linear Kernel
2023-04-29 03:42:07,340:INFO:Total runtime is 0.4605435570081075 minutes
2023-04-29 03:42:07,352:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:07,352:INFO:Initializing create_model()
2023-04-29 03:42:07,356:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:07,356:INFO:Checking exceptions
2023-04-29 03:42:07,356:INFO:Importing libraries
2023-04-29 03:42:07,356:INFO:Copying training dataset
2023-04-29 03:42:07,368:INFO:Defining folds
2023-04-29 03:42:07,368:INFO:Declaring metric variables
2023-04-29 03:42:07,380:INFO:Importing untrained model
2023-04-29 03:42:07,392:INFO:SVM - Linear Kernel Imported successfully
2023-04-29 03:42:07,424:INFO:Starting cross validation
2023-04-29 03:42:07,432:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:07,724:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,736:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,796:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,805:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,846:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,858:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,926:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,951:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:07,963:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:08,032:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:08,052:INFO:Calculating mean and std
2023-04-29 03:42:08,060:INFO:Creating metrics dataframe
2023-04-29 03:42:08,076:INFO:Uploading results into container
2023-04-29 03:42:08,076:INFO:Uploading model into container now
2023-04-29 03:42:08,080:INFO:_master_model_container: 5
2023-04-29 03:42:08,080:INFO:_display_container: 2
2023-04-29 03:42:08,083:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-04-29 03:42:08,083:INFO:create_model() successfully completed......................................
2023-04-29 03:42:08,421:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:08,421:INFO:Creating metrics dataframe
2023-04-29 03:42:08,462:INFO:Initializing Ridge Classifier
2023-04-29 03:42:08,462:INFO:Total runtime is 0.4792390465736389 minutes
2023-04-29 03:42:08,474:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:08,474:INFO:Initializing create_model()
2023-04-29 03:42:08,474:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:08,478:INFO:Checking exceptions
2023-04-29 03:42:08,478:INFO:Importing libraries
2023-04-29 03:42:08,478:INFO:Copying training dataset
2023-04-29 03:42:08,490:INFO:Defining folds
2023-04-29 03:42:08,494:INFO:Declaring metric variables
2023-04-29 03:42:08,506:INFO:Importing untrained model
2023-04-29 03:42:08,518:INFO:Ridge Classifier Imported successfully
2023-04-29 03:42:08,542:INFO:Starting cross validation
2023-04-29 03:42:08,546:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:08,882:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:08,906:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:08,950:INFO:Calculating mean and std
2023-04-29 03:42:08,958:INFO:Creating metrics dataframe
2023-04-29 03:42:08,973:INFO:Uploading results into container
2023-04-29 03:42:08,975:INFO:Uploading model into container now
2023-04-29 03:42:08,975:INFO:_master_model_container: 6
2023-04-29 03:42:08,979:INFO:_display_container: 2
2023-04-29 03:42:08,979:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:42:08,979:INFO:create_model() successfully completed......................................
2023-04-29 03:42:09,306:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:09,306:INFO:Creating metrics dataframe
2023-04-29 03:42:09,338:INFO:Initializing Random Forest Classifier
2023-04-29 03:42:09,342:INFO:Total runtime is 0.49391197760899863 minutes
2023-04-29 03:42:09,350:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:09,350:INFO:Initializing create_model()
2023-04-29 03:42:09,350:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:09,350:INFO:Checking exceptions
2023-04-29 03:42:09,354:INFO:Importing libraries
2023-04-29 03:42:09,354:INFO:Copying training dataset
2023-04-29 03:42:09,366:INFO:Defining folds
2023-04-29 03:42:09,366:INFO:Declaring metric variables
2023-04-29 03:42:09,379:INFO:Importing untrained model
2023-04-29 03:42:09,391:INFO:Random Forest Classifier Imported successfully
2023-04-29 03:42:09,411:INFO:Starting cross validation
2023-04-29 03:42:09,415:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:12,856:INFO:Calculating mean and std
2023-04-29 03:42:12,882:INFO:Creating metrics dataframe
2023-04-29 03:42:12,902:INFO:Uploading results into container
2023-04-29 03:42:12,906:INFO:Uploading model into container now
2023-04-29 03:42:12,906:INFO:_master_model_container: 7
2023-04-29 03:42:12,911:INFO:_display_container: 2
2023-04-29 03:42:12,911:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 03:42:12,911:INFO:create_model() successfully completed......................................
2023-04-29 03:42:13,336:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:13,336:INFO:Creating metrics dataframe
2023-04-29 03:42:13,426:INFO:Initializing Quadratic Discriminant Analysis
2023-04-29 03:42:13,426:INFO:Total runtime is 0.5619824965794882 minutes
2023-04-29 03:42:13,446:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:13,446:INFO:Initializing create_model()
2023-04-29 03:42:13,446:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:13,446:INFO:Checking exceptions
2023-04-29 03:42:13,446:INFO:Importing libraries
2023-04-29 03:42:13,446:INFO:Copying training dataset
2023-04-29 03:42:13,467:INFO:Defining folds
2023-04-29 03:42:13,467:INFO:Declaring metric variables
2023-04-29 03:42:13,487:INFO:Importing untrained model
2023-04-29 03:42:13,504:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 03:42:13,540:INFO:Starting cross validation
2023-04-29 03:42:13,548:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:13,756:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,792:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,845:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,854:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,871:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,876:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,876:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:13,885:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,918:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,947:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,947:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,951:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:13,964:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,972:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:13,980:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,980:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,984:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:13,988:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,988:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:13,988:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,004:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,004:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,008:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,008:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,008:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,008:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,017:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:14,021:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,021:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:42:14,033:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,033:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,037:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,069:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,073:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,073:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,086:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,102:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,102:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,102:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,102:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,114:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,114:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,114:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,114:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,122:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,122:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,122:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,134:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,134:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,134:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,139:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,139:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,143:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,143:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,143:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,143:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,147:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,151:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,159:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,192:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,192:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,192:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,200:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,204:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,204:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,204:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,212:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,216:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,216:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,216:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,224:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,236:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,236:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 03:42:14,236:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 03:42:14,244:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:14,256:INFO:Calculating mean and std
2023-04-29 03:42:14,269:INFO:Creating metrics dataframe
2023-04-29 03:42:14,370:INFO:Uploading results into container
2023-04-29 03:42:14,376:INFO:Uploading model into container now
2023-04-29 03:42:14,380:INFO:_master_model_container: 8
2023-04-29 03:42:14,380:INFO:_display_container: 2
2023-04-29 03:42:14,383:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 03:42:14,383:INFO:create_model() successfully completed......................................
2023-04-29 03:42:14,642:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:14,646:INFO:Creating metrics dataframe
2023-04-29 03:42:14,686:INFO:Initializing Ada Boost Classifier
2023-04-29 03:42:14,686:INFO:Total runtime is 0.582974672317505 minutes
2023-04-29 03:42:14,698:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:14,698:INFO:Initializing create_model()
2023-04-29 03:42:14,702:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:14,702:INFO:Checking exceptions
2023-04-29 03:42:14,702:INFO:Importing libraries
2023-04-29 03:42:14,702:INFO:Copying training dataset
2023-04-29 03:42:14,714:INFO:Defining folds
2023-04-29 03:42:14,714:INFO:Declaring metric variables
2023-04-29 03:42:14,730:INFO:Importing untrained model
2023-04-29 03:42:14,742:INFO:Ada Boost Classifier Imported successfully
2023-04-29 03:42:14,771:INFO:Starting cross validation
2023-04-29 03:42:14,775:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:15,745:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,773:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,779:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,800:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,836:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,856:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,872:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,884:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,892:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,928:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:15,936:INFO:Calculating mean and std
2023-04-29 03:42:15,936:INFO:Creating metrics dataframe
2023-04-29 03:42:15,956:INFO:Uploading results into container
2023-04-29 03:42:15,960:INFO:Uploading model into container now
2023-04-29 03:42:15,960:INFO:_master_model_container: 9
2023-04-29 03:42:15,964:INFO:_display_container: 2
2023-04-29 03:42:15,964:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2023-04-29 03:42:15,964:INFO:create_model() successfully completed......................................
2023-04-29 03:42:16,269:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:16,269:INFO:Creating metrics dataframe
2023-04-29 03:42:16,305:INFO:Initializing Gradient Boosting Classifier
2023-04-29 03:42:16,305:INFO:Total runtime is 0.6099696914354962 minutes
2023-04-29 03:42:16,317:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:16,317:INFO:Initializing create_model()
2023-04-29 03:42:16,317:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:16,317:INFO:Checking exceptions
2023-04-29 03:42:16,317:INFO:Importing libraries
2023-04-29 03:42:16,317:INFO:Copying training dataset
2023-04-29 03:42:16,337:INFO:Defining folds
2023-04-29 03:42:16,337:INFO:Declaring metric variables
2023-04-29 03:42:16,353:INFO:Importing untrained model
2023-04-29 03:42:16,361:INFO:Gradient Boosting Classifier Imported successfully
2023-04-29 03:42:16,386:INFO:Starting cross validation
2023-04-29 03:42:16,390:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:18,227:INFO:Calculating mean and std
2023-04-29 03:42:18,230:INFO:Creating metrics dataframe
2023-04-29 03:42:18,242:INFO:Uploading results into container
2023-04-29 03:42:18,242:INFO:Uploading model into container now
2023-04-29 03:42:18,242:INFO:_master_model_container: 10
2023-04-29 03:42:18,242:INFO:_display_container: 2
2023-04-29 03:42:18,246:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-04-29 03:42:18,246:INFO:create_model() successfully completed......................................
2023-04-29 03:42:18,560:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:18,560:INFO:Creating metrics dataframe
2023-04-29 03:42:18,588:INFO:Initializing Linear Discriminant Analysis
2023-04-29 03:42:18,592:INFO:Total runtime is 0.6480814695358278 minutes
2023-04-29 03:42:18,600:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:18,604:INFO:Initializing create_model()
2023-04-29 03:42:18,604:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:18,604:INFO:Checking exceptions
2023-04-29 03:42:18,604:INFO:Importing libraries
2023-04-29 03:42:18,604:INFO:Copying training dataset
2023-04-29 03:42:18,616:INFO:Defining folds
2023-04-29 03:42:18,616:INFO:Declaring metric variables
2023-04-29 03:42:18,628:INFO:Importing untrained model
2023-04-29 03:42:18,641:INFO:Linear Discriminant Analysis Imported successfully
2023-04-29 03:42:18,663:INFO:Starting cross validation
2023-04-29 03:42:18,665:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:19,185:INFO:Calculating mean and std
2023-04-29 03:42:19,188:INFO:Creating metrics dataframe
2023-04-29 03:42:19,196:INFO:Uploading results into container
2023-04-29 03:42:19,196:INFO:Uploading model into container now
2023-04-29 03:42:19,196:INFO:_master_model_container: 11
2023-04-29 03:42:19,196:INFO:_display_container: 2
2023-04-29 03:42:19,200:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-04-29 03:42:19,200:INFO:create_model() successfully completed......................................
2023-04-29 03:42:19,409:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:19,409:INFO:Creating metrics dataframe
2023-04-29 03:42:19,461:INFO:Initializing Extra Trees Classifier
2023-04-29 03:42:19,461:INFO:Total runtime is 0.6625618298848471 minutes
2023-04-29 03:42:19,469:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:19,473:INFO:Initializing create_model()
2023-04-29 03:42:19,473:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:19,473:INFO:Checking exceptions
2023-04-29 03:42:19,473:INFO:Importing libraries
2023-04-29 03:42:19,473:INFO:Copying training dataset
2023-04-29 03:42:19,482:INFO:Defining folds
2023-04-29 03:42:19,482:INFO:Declaring metric variables
2023-04-29 03:42:19,494:INFO:Importing untrained model
2023-04-29 03:42:19,506:INFO:Extra Trees Classifier Imported successfully
2023-04-29 03:42:19,526:INFO:Starting cross validation
2023-04-29 03:42:19,526:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:20,453:INFO:Calculating mean and std
2023-04-29 03:42:20,458:INFO:Creating metrics dataframe
2023-04-29 03:42:20,466:INFO:Uploading results into container
2023-04-29 03:42:20,466:INFO:Uploading model into container now
2023-04-29 03:42:20,470:INFO:_master_model_container: 12
2023-04-29 03:42:20,470:INFO:_display_container: 2
2023-04-29 03:42:20,470:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 03:42:20,470:INFO:create_model() successfully completed......................................
2023-04-29 03:42:20,711:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:20,711:INFO:Creating metrics dataframe
2023-04-29 03:42:20,747:INFO:Initializing Light Gradient Boosting Machine
2023-04-29 03:42:20,747:INFO:Total runtime is 0.6839951356252035 minutes
2023-04-29 03:42:20,755:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:20,755:INFO:Initializing create_model()
2023-04-29 03:42:20,755:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:20,755:INFO:Checking exceptions
2023-04-29 03:42:20,755:INFO:Importing libraries
2023-04-29 03:42:20,755:INFO:Copying training dataset
2023-04-29 03:42:20,767:INFO:Defining folds
2023-04-29 03:42:20,767:INFO:Declaring metric variables
2023-04-29 03:42:20,771:INFO:Importing untrained model
2023-04-29 03:42:20,779:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 03:42:20,795:INFO:Starting cross validation
2023-04-29 03:42:20,799:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:24,176:INFO:Calculating mean and std
2023-04-29 03:42:24,180:INFO:Creating metrics dataframe
2023-04-29 03:42:24,188:INFO:Uploading results into container
2023-04-29 03:42:24,188:INFO:Uploading model into container now
2023-04-29 03:42:24,188:INFO:_master_model_container: 13
2023-04-29 03:42:24,188:INFO:_display_container: 2
2023-04-29 03:42:24,192:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 03:42:24,192:INFO:create_model() successfully completed......................................
2023-04-29 03:42:24,490:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:24,490:INFO:Creating metrics dataframe
2023-04-29 03:42:24,527:INFO:Initializing Dummy Classifier
2023-04-29 03:42:24,527:INFO:Total runtime is 0.7469920794169109 minutes
2023-04-29 03:42:24,535:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:24,539:INFO:Initializing create_model()
2023-04-29 03:42:24,539:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE163CA60>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:24,539:INFO:Checking exceptions
2023-04-29 03:42:24,539:INFO:Importing libraries
2023-04-29 03:42:24,539:INFO:Copying training dataset
2023-04-29 03:42:24,551:INFO:Defining folds
2023-04-29 03:42:24,551:INFO:Declaring metric variables
2023-04-29 03:42:24,563:INFO:Importing untrained model
2023-04-29 03:42:24,571:INFO:Dummy Classifier Imported successfully
2023-04-29 03:42:24,591:INFO:Starting cross validation
2023-04-29 03:42:24,595:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:24,957:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:24,977:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:24,977:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:24,997:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,001:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,025:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,029:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,033:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,037:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,065:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:25,073:INFO:Calculating mean and std
2023-04-29 03:42:25,077:INFO:Creating metrics dataframe
2023-04-29 03:42:25,085:INFO:Uploading results into container
2023-04-29 03:42:25,085:INFO:Uploading model into container now
2023-04-29 03:42:25,089:INFO:_master_model_container: 14
2023-04-29 03:42:25,089:INFO:_display_container: 2
2023-04-29 03:42:25,089:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2023-04-29 03:42:25,089:INFO:create_model() successfully completed......................................
2023-04-29 03:42:25,421:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:25,421:INFO:Creating metrics dataframe
2023-04-29 03:42:25,501:INFO:Initializing create_model()
2023-04-29 03:42:25,501:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:25,501:INFO:Checking exceptions
2023-04-29 03:42:25,505:INFO:Importing libraries
2023-04-29 03:42:25,505:INFO:Copying training dataset
2023-04-29 03:42:25,513:INFO:Defining folds
2023-04-29 03:42:25,513:INFO:Declaring metric variables
2023-04-29 03:42:25,513:INFO:Importing untrained model
2023-04-29 03:42:25,513:INFO:Declaring custom model
2023-04-29 03:42:25,513:INFO:Random Forest Classifier Imported successfully
2023-04-29 03:42:25,518:INFO:Cross validation set to False
2023-04-29 03:42:25,518:INFO:Fitting Model
2023-04-29 03:42:25,902:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 03:42:25,902:INFO:create_model() successfully completed......................................
2023-04-29 03:42:26,387:INFO:_master_model_container: 14
2023-04-29 03:42:26,387:INFO:_display_container: 2
2023-04-29 03:42:26,392:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 03:42:26,392:INFO:compare_models() successfully completed......................................
2023-04-29 03:42:26,470:INFO:Initializing create_model()
2023-04-29 03:42:26,470:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=ridge, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:26,470:INFO:Checking exceptions
2023-04-29 03:42:26,548:INFO:Importing libraries
2023-04-29 03:42:26,548:INFO:Copying training dataset
2023-04-29 03:42:26,557:INFO:Defining folds
2023-04-29 03:42:26,557:INFO:Declaring metric variables
2023-04-29 03:42:26,565:INFO:Importing untrained model
2023-04-29 03:42:26,573:INFO:Ridge Classifier Imported successfully
2023-04-29 03:42:26,593:INFO:Starting cross validation
2023-04-29 03:42:26,597:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:26,907:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:26,979:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:26,991:INFO:Calculating mean and std
2023-04-29 03:42:26,991:INFO:Creating metrics dataframe
2023-04-29 03:42:27,007:INFO:Finalizing model
2023-04-29 03:42:27,091:INFO:Uploading results into container
2023-04-29 03:42:27,095:INFO:Uploading model into container now
2023-04-29 03:42:27,123:INFO:_master_model_container: 15
2023-04-29 03:42:27,127:INFO:_display_container: 3
2023-04-29 03:42:27,127:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:42:27,127:INFO:create_model() successfully completed......................................
2023-04-29 03:42:27,520:INFO:Initializing tune_model()
2023-04-29 03:42:27,524:INFO:tune_model(estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>)
2023-04-29 03:42:27,524:INFO:Checking exceptions
2023-04-29 03:42:27,612:INFO:Copying training dataset
2023-04-29 03:42:27,624:INFO:Checking base model
2023-04-29 03:42:27,624:INFO:Base model : Ridge Classifier
2023-04-29 03:42:27,632:INFO:Declaring metric variables
2023-04-29 03:42:27,644:INFO:Defining Hyperparameters
2023-04-29 03:42:27,945:INFO:Tuning with n_jobs=-1
2023-04-29 03:42:27,945:INFO:Initializing RandomizedSearchCV
2023-04-29 03:42:28,127:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,220:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,236:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,252:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,268:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,292:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,309:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,333:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,349:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,361:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,377:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,405:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,417:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,421:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,437:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,465:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,481:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,481:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,498:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,510:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,526:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,554:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,558:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,582:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,606:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,626:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,642:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,654:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,670:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,682:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,699:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,710:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,738:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,762:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,782:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,798:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,818:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,830:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,850:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,870:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,878:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,898:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,910:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,926:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,942:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,958:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:28,978:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,007:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,019:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,031:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,051:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,063:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,083:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,095:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,110:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,126:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,151:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,179:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,195:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,215:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,231:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,247:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,259:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,275:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,287:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,307:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,319:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,335:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,359:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,387:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,407:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,423:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,436:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,452:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,467:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,483:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,495:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,515:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,527:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,543:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,559:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,599:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,599:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,616:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,629:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,645:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,657:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,677:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,689:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,707:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:29,721:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,737:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,749:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,761:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,781:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,789:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,814:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,822:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,838:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,846:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:42:29,894:INFO:best_params: {'actual_estimator__normalize': False, 'actual_estimator__fit_intercept': True, 'actual_estimator__alpha': 8.6}
2023-04-29 03:42:29,894:INFO:Hyperparameter search completed
2023-04-29 03:42:29,898:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:29,898:INFO:Initializing create_model()
2023-04-29 03:42:29,898:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE4726F40>, model_only=True, return_train_score=False, kwargs={'normalize': False, 'fit_intercept': True, 'alpha': 8.6})
2023-04-29 03:42:29,898:INFO:Checking exceptions
2023-04-29 03:42:29,898:INFO:Importing libraries
2023-04-29 03:42:29,898:INFO:Copying training dataset
2023-04-29 03:42:29,902:INFO:Defining folds
2023-04-29 03:42:29,906:INFO:Declaring metric variables
2023-04-29 03:42:29,910:INFO:Importing untrained model
2023-04-29 03:42:29,914:INFO:Declaring custom model
2023-04-29 03:42:29,922:INFO:Ridge Classifier Imported successfully
2023-04-29 03:42:29,938:INFO:Starting cross validation
2023-04-29 03:42:29,938:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:30,050:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,070:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,090:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,115:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,127:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,140:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:30,144:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,160:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,164:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,180:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,196:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,202:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:30,253:INFO:Calculating mean and std
2023-04-29 03:42:30,257:INFO:Creating metrics dataframe
2023-04-29 03:42:30,273:INFO:Finalizing model
2023-04-29 03:42:30,341:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:42:30,361:INFO:Uploading results into container
2023-04-29 03:42:30,361:INFO:Uploading model into container now
2023-04-29 03:42:30,365:INFO:_master_model_container: 16
2023-04-29 03:42:30,365:INFO:_display_container: 4
2023-04-29 03:42:30,365:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:42:30,365:INFO:create_model() successfully completed......................................
2023-04-29 03:42:30,685:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:30,685:INFO:choose_better activated
2023-04-29 03:42:30,697:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:30,697:INFO:Initializing create_model()
2023-04-29 03:42:30,697:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:30,697:INFO:Checking exceptions
2023-04-29 03:42:30,702:INFO:Importing libraries
2023-04-29 03:42:30,705:INFO:Copying training dataset
2023-04-29 03:42:30,710:INFO:Defining folds
2023-04-29 03:42:30,710:INFO:Declaring metric variables
2023-04-29 03:42:30,714:INFO:Importing untrained model
2023-04-29 03:42:30,714:INFO:Declaring custom model
2023-04-29 03:42:30,714:INFO:Ridge Classifier Imported successfully
2023-04-29 03:42:30,714:INFO:Starting cross validation
2023-04-29 03:42:30,718:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:30,974:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:31,091:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:31,120:INFO:Calculating mean and std
2023-04-29 03:42:31,120:INFO:Creating metrics dataframe
2023-04-29 03:42:31,124:INFO:Finalizing model
2023-04-29 03:42:31,180:INFO:Uploading results into container
2023-04-29 03:42:31,184:INFO:Uploading model into container now
2023-04-29 03:42:31,184:INFO:_master_model_container: 17
2023-04-29 03:42:31,184:INFO:_display_container: 5
2023-04-29 03:42:31,184:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:42:31,184:INFO:create_model() successfully completed......................................
2023-04-29 03:42:31,473:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:31,477:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001) result for Accuracy is 0.8946
2023-04-29 03:42:31,477:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001) result for Accuracy is 0.9013
2023-04-29 03:42:31,477:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001) is best model
2023-04-29 03:42:31,481:INFO:choose_better completed
2023-04-29 03:42:31,512:INFO:_master_model_container: 17
2023-04-29 03:42:31,514:INFO:_display_container: 4
2023-04-29 03:42:31,514:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:42:31,514:INFO:tune_model() successfully completed......................................
2023-04-29 03:42:31,879:INFO:Initializing evaluate_model()
2023-04-29 03:42:31,879:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:42:31,943:INFO:Initializing plot_model()
2023-04-29 03:42:31,943:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 03:42:31,943:INFO:Checking exceptions
2023-04-29 03:42:31,951:INFO:Preloading libraries
2023-04-29 03:42:31,951:INFO:Copying training dataset
2023-04-29 03:42:31,951:INFO:Plot type: pipeline
2023-04-29 03:42:32,472:INFO:Visual Rendered Successfully
2023-04-29 03:42:32,805:INFO:plot_model() successfully completed......................................
2023-04-29 03:42:32,821:INFO:Initializing create_model()
2023-04-29 03:42:32,821:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=nb, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:32,821:INFO:Checking exceptions
2023-04-29 03:42:32,871:INFO:Importing libraries
2023-04-29 03:42:32,875:INFO:Copying training dataset
2023-04-29 03:42:32,879:INFO:Defining folds
2023-04-29 03:42:32,883:INFO:Declaring metric variables
2023-04-29 03:42:32,887:INFO:Importing untrained model
2023-04-29 03:42:32,895:INFO:Naive Bayes Imported successfully
2023-04-29 03:42:32,911:INFO:Starting cross validation
2023-04-29 03:42:32,911:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:33,284:INFO:Calculating mean and std
2023-04-29 03:42:33,284:INFO:Creating metrics dataframe
2023-04-29 03:42:33,296:INFO:Finalizing model
2023-04-29 03:42:33,357:INFO:Uploading results into container
2023-04-29 03:42:33,357:INFO:Uploading model into container now
2023-04-29 03:42:33,381:INFO:_master_model_container: 18
2023-04-29 03:42:33,381:INFO:_display_container: 5
2023-04-29 03:42:33,381:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:42:33,381:INFO:create_model() successfully completed......................................
2023-04-29 03:42:33,607:INFO:Initializing tune_model()
2023-04-29 03:42:33,607:INFO:tune_model(estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>)
2023-04-29 03:42:33,607:INFO:Checking exceptions
2023-04-29 03:42:33,711:INFO:Copying training dataset
2023-04-29 03:42:33,716:INFO:Checking base model
2023-04-29 03:42:33,716:INFO:Base model : Naive Bayes
2023-04-29 03:42:33,724:INFO:Declaring metric variables
2023-04-29 03:42:33,732:INFO:Defining Hyperparameters
2023-04-29 03:42:33,928:INFO:Tuning with n_jobs=-1
2023-04-29 03:42:33,928:INFO:Initializing RandomizedSearchCV
2023-04-29 03:42:35,937:INFO:best_params: {'actual_estimator__var_smoothing': 2e-09}
2023-04-29 03:42:35,942:INFO:Hyperparameter search completed
2023-04-29 03:42:35,942:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:35,944:INFO:Initializing create_model()
2023-04-29 03:42:35,944:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE1762BE0>, model_only=True, return_train_score=False, kwargs={'var_smoothing': 2e-09})
2023-04-29 03:42:35,944:INFO:Checking exceptions
2023-04-29 03:42:35,946:INFO:Importing libraries
2023-04-29 03:42:35,946:INFO:Copying training dataset
2023-04-29 03:42:35,954:INFO:Defining folds
2023-04-29 03:42:35,954:INFO:Declaring metric variables
2023-04-29 03:42:35,964:INFO:Importing untrained model
2023-04-29 03:42:35,964:INFO:Declaring custom model
2023-04-29 03:42:35,974:INFO:Naive Bayes Imported successfully
2023-04-29 03:42:35,992:INFO:Starting cross validation
2023-04-29 03:42:35,996:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:36,523:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:36,583:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:42:36,742:INFO:Calculating mean and std
2023-04-29 03:42:36,748:INFO:Creating metrics dataframe
2023-04-29 03:42:36,774:INFO:Finalizing model
2023-04-29 03:42:36,898:INFO:Uploading results into container
2023-04-29 03:42:36,900:INFO:Uploading model into container now
2023-04-29 03:42:36,903:INFO:_master_model_container: 19
2023-04-29 03:42:36,903:INFO:_display_container: 6
2023-04-29 03:42:36,905:INFO:GaussianNB(priors=None, var_smoothing=2e-09)
2023-04-29 03:42:36,905:INFO:create_model() successfully completed......................................
2023-04-29 03:42:37,199:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:37,199:INFO:choose_better activated
2023-04-29 03:42:37,213:INFO:SubProcess create_model() called ==================================
2023-04-29 03:42:37,215:INFO:Initializing create_model()
2023-04-29 03:42:37,215:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:37,215:INFO:Checking exceptions
2023-04-29 03:42:37,221:INFO:Importing libraries
2023-04-29 03:42:37,221:INFO:Copying training dataset
2023-04-29 03:42:37,231:INFO:Defining folds
2023-04-29 03:42:37,231:INFO:Declaring metric variables
2023-04-29 03:42:37,231:INFO:Importing untrained model
2023-04-29 03:42:37,231:INFO:Declaring custom model
2023-04-29 03:42:37,233:INFO:Naive Bayes Imported successfully
2023-04-29 03:42:37,233:INFO:Starting cross validation
2023-04-29 03:42:37,237:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:38,022:INFO:Calculating mean and std
2023-04-29 03:42:38,024:INFO:Creating metrics dataframe
2023-04-29 03:42:38,034:INFO:Finalizing model
2023-04-29 03:42:38,136:INFO:Uploading results into container
2023-04-29 03:42:38,138:INFO:Uploading model into container now
2023-04-29 03:42:38,140:INFO:_master_model_container: 20
2023-04-29 03:42:38,140:INFO:_display_container: 7
2023-04-29 03:42:38,140:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:42:38,142:INFO:create_model() successfully completed......................................
2023-04-29 03:42:38,436:INFO:SubProcess create_model() end ==================================
2023-04-29 03:42:38,438:INFO:GaussianNB(priors=None, var_smoothing=1e-09) result for Accuracy is 0.8408
2023-04-29 03:42:38,440:INFO:GaussianNB(priors=None, var_smoothing=2e-09) result for Accuracy is 0.8142
2023-04-29 03:42:38,440:INFO:GaussianNB(priors=None, var_smoothing=1e-09) is best model
2023-04-29 03:42:38,442:INFO:choose_better completed
2023-04-29 03:42:38,442:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 03:42:38,480:INFO:_master_model_container: 20
2023-04-29 03:42:38,480:INFO:_display_container: 6
2023-04-29 03:42:38,482:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:42:38,482:INFO:tune_model() successfully completed......................................
2023-04-29 03:42:38,835:INFO:Initializing evaluate_model()
2023-04-29 03:42:38,837:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:42:38,921:INFO:Initializing plot_model()
2023-04-29 03:42:38,923:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GaussianNB(priors=None, var_smoothing=1e-09), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 03:42:38,923:INFO:Checking exceptions
2023-04-29 03:42:38,929:INFO:Preloading libraries
2023-04-29 03:42:38,931:INFO:Copying training dataset
2023-04-29 03:42:38,931:INFO:Plot type: pipeline
2023-04-29 03:42:39,494:INFO:Visual Rendered Successfully
2023-04-29 03:42:39,753:INFO:plot_model() successfully completed......................................
2023-04-29 03:42:39,783:INFO:Initializing create_model()
2023-04-29 03:42:39,785:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:42:39,785:INFO:Checking exceptions
2023-04-29 03:42:39,882:INFO:Importing libraries
2023-04-29 03:42:39,882:INFO:Copying training dataset
2023-04-29 03:42:39,892:INFO:Defining folds
2023-04-29 03:42:39,894:INFO:Declaring metric variables
2023-04-29 03:42:39,906:INFO:Importing untrained model
2023-04-29 03:42:39,922:INFO:Logistic Regression Imported successfully
2023-04-29 03:42:39,954:INFO:Starting cross validation
2023-04-29 03:42:39,959:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:42:43,307:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:43,799:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:43,826:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:43,922:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,027:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,056:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,271:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,281:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,505:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,640:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:44,798:INFO:Calculating mean and std
2023-04-29 03:42:44,804:INFO:Creating metrics dataframe
2023-04-29 03:42:44,822:INFO:Finalizing model
2023-04-29 03:42:46,228:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:46,258:INFO:Uploading results into container
2023-04-29 03:42:46,262:INFO:Uploading model into container now
2023-04-29 03:42:46,307:INFO:_master_model_container: 21
2023-04-29 03:42:46,307:INFO:_display_container: 7
2023-04-29 03:42:46,310:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:42:46,311:INFO:create_model() successfully completed......................................
2023-04-29 03:42:46,702:INFO:Initializing tune_model()
2023-04-29 03:42:46,702:INFO:tune_model(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>)
2023-04-29 03:42:46,702:INFO:Checking exceptions
2023-04-29 03:42:46,896:INFO:Copying training dataset
2023-04-29 03:42:46,908:INFO:Checking base model
2023-04-29 03:42:46,908:INFO:Base model : Logistic Regression
2023-04-29 03:42:46,922:INFO:Declaring metric variables
2023-04-29 03:42:46,930:INFO:Defining Hyperparameters
2023-04-29 03:42:47,250:INFO:Tuning with n_jobs=-1
2023-04-29 03:42:47,252:INFO:Initializing RandomizedSearchCV
2023-04-29 03:42:50,463:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:50,467:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:50,810:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,035:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,164:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,214:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,255:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,358:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:51,724:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:52,085:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:54,574:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:54,590:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,555:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,566:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,728:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,735:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,751:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,798:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:55,940:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:56,091:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:56,775:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:56,819:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:56,904:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:56,904:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:57,072:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:57,190:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:57,196:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:57,263:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:57,586:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,055:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,109:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,241:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,250:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,391:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,460:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,542:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,700:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:58,868:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,262:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,446:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,647:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,672:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,768:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,864:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:42:59,991:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,044:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,093:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,267:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,428:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,748:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,830:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:00,984:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,175:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,246:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,279:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,410:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,526:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:01,860:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,217:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,255:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,278:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,290:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,431:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,659:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,738:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,844:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:02,860:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,096:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,411:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,498:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,501:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,680:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,762:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:03,939:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,245:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,404:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,782:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,794:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,880:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,901:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:04,974:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,050:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,156:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,561:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,685:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,828:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:05,836:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,178:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,255:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,368:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,378:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,440:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,476:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:06,521:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 7.689}
2023-04-29 03:43:06,525:INFO:Hyperparameter search completed
2023-04-29 03:43:06,525:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:06,525:INFO:Initializing create_model()
2023-04-29 03:43:06,525:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE4D6AC70>, model_only=True, return_train_score=False, kwargs={'class_weight': {}, 'C': 7.689})
2023-04-29 03:43:06,527:INFO:Checking exceptions
2023-04-29 03:43:06,527:INFO:Importing libraries
2023-04-29 03:43:06,527:INFO:Copying training dataset
2023-04-29 03:43:06,533:INFO:Defining folds
2023-04-29 03:43:06,533:INFO:Declaring metric variables
2023-04-29 03:43:06,541:INFO:Importing untrained model
2023-04-29 03:43:06,541:INFO:Declaring custom model
2023-04-29 03:43:06,552:INFO:Logistic Regression Imported successfully
2023-04-29 03:43:06,570:INFO:Starting cross validation
2023-04-29 03:43:06,574:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:07,629:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:07,721:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:07,952:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:07,952:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:07,954:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,003:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,084:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,093:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,142:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,299:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:08,393:INFO:Calculating mean and std
2023-04-29 03:43:08,396:INFO:Creating metrics dataframe
2023-04-29 03:43:08,411:INFO:Finalizing model
2023-04-29 03:43:09,405:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:09,416:INFO:Uploading results into container
2023-04-29 03:43:09,416:INFO:Uploading model into container now
2023-04-29 03:43:09,419:INFO:_master_model_container: 22
2023-04-29 03:43:09,419:INFO:_display_container: 8
2023-04-29 03:43:09,419:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:43:09,419:INFO:create_model() successfully completed......................................
2023-04-29 03:43:09,626:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:09,626:INFO:choose_better activated
2023-04-29 03:43:09,635:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:09,635:INFO:Initializing create_model()
2023-04-29 03:43:09,635:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:09,635:INFO:Checking exceptions
2023-04-29 03:43:09,639:INFO:Importing libraries
2023-04-29 03:43:09,639:INFO:Copying training dataset
2023-04-29 03:43:09,646:INFO:Defining folds
2023-04-29 03:43:09,646:INFO:Declaring metric variables
2023-04-29 03:43:09,646:INFO:Importing untrained model
2023-04-29 03:43:09,647:INFO:Declaring custom model
2023-04-29 03:43:09,647:INFO:Logistic Regression Imported successfully
2023-04-29 03:43:09,651:INFO:Starting cross validation
2023-04-29 03:43:09,651:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:10,845:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:10,915:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:10,983:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,038:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,040:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,126:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,144:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,162:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,246:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,297:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:11,396:INFO:Calculating mean and std
2023-04-29 03:43:11,397:INFO:Creating metrics dataframe
2023-04-29 03:43:11,404:INFO:Finalizing model
2023-04-29 03:43:12,234:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 03:43:12,238:INFO:Uploading results into container
2023-04-29 03:43:12,238:INFO:Uploading model into container now
2023-04-29 03:43:12,238:INFO:_master_model_container: 23
2023-04-29 03:43:12,238:INFO:_display_container: 9
2023-04-29 03:43:12,242:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:43:12,242:INFO:create_model() successfully completed......................................
2023-04-29 03:43:12,463:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:12,463:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.9404
2023-04-29 03:43:12,467:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.9604
2023-04-29 03:43:12,467:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2023-04-29 03:43:12,470:INFO:choose_better completed
2023-04-29 03:43:12,493:INFO:_master_model_container: 23
2023-04-29 03:43:12,496:INFO:_display_container: 8
2023-04-29 03:43:12,496:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:43:12,496:INFO:tune_model() successfully completed......................................
2023-04-29 03:43:12,833:INFO:Initializing evaluate_model()
2023-04-29 03:43:12,833:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, estimator=LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:43:12,871:INFO:Initializing plot_model()
2023-04-29 03:43:12,871:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 03:43:12,871:INFO:Checking exceptions
2023-04-29 03:43:12,875:INFO:Preloading libraries
2023-04-29 03:43:12,875:INFO:Copying training dataset
2023-04-29 03:43:12,875:INFO:Plot type: pipeline
2023-04-29 03:43:13,200:INFO:Visual Rendered Successfully
2023-04-29 03:43:13,404:INFO:plot_model() successfully completed......................................
2023-04-29 03:43:19,103:INFO:PyCaret ClassificationExperiment
2023-04-29 03:43:19,106:INFO:Logging name: clf-default-name
2023-04-29 03:43:19,106:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-04-29 03:43:19,106:INFO:version 3.0.0.rc8
2023-04-29 03:43:19,106:INFO:Initializing setup()
2023-04-29 03:43:19,106:INFO:self.USI: 6d9c
2023-04-29 03:43:19,106:INFO:self._variable_keys: {'gpu_param', '_available_plots', '_ml_usecase', 'fold_shuffle_param', 'X_train', 'data', 'target_param', 'logging_param', 'X', 'memory', 'html_param', 'fix_imbalance', 'pipeline', 'log_plots_param', 'n_jobs_param', 'exp_id', 'fold_groups_param', 'is_multiclass', 'X_test', 'y_test', 'fold_generator', 'exp_name_log', 'USI', 'seed', 'gpu_n_jobs_param', 'idx', 'y_train', 'y'}
2023-04-29 03:43:19,106:INFO:Checking environment
2023-04-29 03:43:19,106:INFO:python_version: 3.9.13
2023-04-29 03:43:19,106:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-04-29 03:43:19,106:INFO:machine: AMD64
2023-04-29 03:43:19,106:INFO:platform: Windows-10-10.0.22621-SP0
2023-04-29 03:43:19,106:INFO:Memory: svmem(total=16782143488, available=323936256, percent=98.1, used=16458207232, free=323936256)
2023-04-29 03:43:19,106:INFO:Physical Core: 10
2023-04-29 03:43:19,106:INFO:Logical Core: 12
2023-04-29 03:43:19,106:INFO:Checking libraries
2023-04-29 03:43:19,106:INFO:System:
2023-04-29 03:43:19,106:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-04-29 03:43:19,106:INFO:executable: C:\Users\eeman\anaconda3\python.exe
2023-04-29 03:43:19,106:INFO:   machine: Windows-10-10.0.22621-SP0
2023-04-29 03:43:19,106:INFO:PyCaret required dependencies:
2023-04-29 03:43:19,106:INFO:                 pip: 22.2.2
2023-04-29 03:43:19,106:INFO:          setuptools: 63.4.1
2023-04-29 03:43:19,106:INFO:             pycaret: 3.0.0rc8
2023-04-29 03:43:19,106:INFO:             IPython: 7.31.1
2023-04-29 03:43:19,106:INFO:          ipywidgets: 7.6.5
2023-04-29 03:43:19,106:INFO:                tqdm: 4.64.1
2023-04-29 03:43:19,106:INFO:               numpy: 1.21.5
2023-04-29 03:43:19,106:INFO:              pandas: 1.4.4
2023-04-29 03:43:19,106:INFO:              jinja2: 2.11.3
2023-04-29 03:43:19,106:INFO:               scipy: 1.9.1
2023-04-29 03:43:19,106:INFO:              joblib: 1.2.0
2023-04-29 03:43:19,106:INFO:             sklearn: 1.0.2
2023-04-29 03:43:19,106:INFO:                pyod: 1.0.7
2023-04-29 03:43:19,106:INFO:            imblearn: 0.10.1
2023-04-29 03:43:19,106:INFO:   category_encoders: 2.6.0
2023-04-29 03:43:19,106:INFO:            lightgbm: 3.3.4
2023-04-29 03:43:19,106:INFO:               numba: 0.55.1
2023-04-29 03:43:19,106:INFO:            requests: 2.28.1
2023-04-29 03:43:19,106:INFO:          matplotlib: 3.5.2
2023-04-29 03:43:19,106:INFO:          scikitplot: 0.3.7
2023-04-29 03:43:19,106:INFO:         yellowbrick: 1.5
2023-04-29 03:43:19,106:INFO:              plotly: 5.9.0
2023-04-29 03:43:19,106:INFO:             kaleido: 0.2.1
2023-04-29 03:43:19,109:INFO:         statsmodels: 0.13.5
2023-04-29 03:43:19,109:INFO:              sktime: 0.15.1
2023-04-29 03:43:19,109:INFO:               tbats: 1.1.2
2023-04-29 03:43:19,109:INFO:            pmdarima: 2.0.2
2023-04-29 03:43:19,109:INFO:              psutil: 5.9.0
2023-04-29 03:43:19,109:INFO:PyCaret optional dependencies:
2023-04-29 03:43:19,109:INFO:                shap: 0.41.0
2023-04-29 03:43:19,109:INFO:           interpret: Not installed
2023-04-29 03:43:19,109:INFO:                umap: Not installed
2023-04-29 03:43:19,109:INFO:    pandas_profiling: Not installed
2023-04-29 03:43:19,109:INFO:  explainerdashboard: Not installed
2023-04-29 03:43:19,109:INFO:             autoviz: Not installed
2023-04-29 03:43:19,109:INFO:           fairlearn: Not installed
2023-04-29 03:43:19,109:INFO:             xgboost: Not installed
2023-04-29 03:43:19,109:INFO:            catboost: Not installed
2023-04-29 03:43:19,109:INFO:              kmodes: Not installed
2023-04-29 03:43:19,109:INFO:             mlxtend: Not installed
2023-04-29 03:43:19,109:INFO:       statsforecast: Not installed
2023-04-29 03:43:19,109:INFO:        tune_sklearn: Not installed
2023-04-29 03:43:19,109:INFO:                 ray: Not installed
2023-04-29 03:43:19,109:INFO:            hyperopt: Not installed
2023-04-29 03:43:19,109:INFO:              optuna: Not installed
2023-04-29 03:43:19,109:INFO:               skopt: Not installed
2023-04-29 03:43:19,109:INFO:              mlflow: Not installed
2023-04-29 03:43:19,109:INFO:              gradio: Not installed
2023-04-29 03:43:19,109:INFO:             fastapi: Not installed
2023-04-29 03:43:19,109:INFO:             uvicorn: Not installed
2023-04-29 03:43:19,109:INFO:              m2cgen: Not installed
2023-04-29 03:43:19,109:INFO:           evidently: Not installed
2023-04-29 03:43:19,109:INFO:                nltk: 3.7
2023-04-29 03:43:19,109:INFO:            pyLDAvis: Not installed
2023-04-29 03:43:19,109:INFO:              gensim: 4.1.2
2023-04-29 03:43:19,109:INFO:               spacy: Not installed
2023-04-29 03:43:19,109:INFO:           wordcloud: Not installed
2023-04-29 03:43:19,112:INFO:            textblob: Not installed
2023-04-29 03:43:19,112:INFO:               fugue: Not installed
2023-04-29 03:43:19,112:INFO:           streamlit: Not installed
2023-04-29 03:43:19,112:INFO:             prophet: Not installed
2023-04-29 03:43:19,112:INFO:None
2023-04-29 03:43:19,112:INFO:Set up data.
2023-04-29 03:43:19,118:INFO:Set up train/test split.
2023-04-29 03:43:19,130:INFO:Set up index.
2023-04-29 03:43:19,130:INFO:Set up folding strategy.
2023-04-29 03:43:19,130:INFO:Assigning column types.
2023-04-29 03:43:19,136:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-29 03:43:19,260:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-29 03:43:19,262:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:43:19,336:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:19,336:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:19,453:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-29 03:43:19,456:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:43:19,526:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:19,529:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:19,529:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-29 03:43:19,698:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:43:19,832:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:19,832:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:20,048:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 03:43:20,163:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:20,166:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:20,166:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-04-29 03:43:20,520:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:20,520:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:20,982:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:20,982:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:20,985:INFO:Preparing preprocessing pipeline...
2023-04-29 03:43:20,988:INFO:Set up simple imputation.
2023-04-29 03:43:20,994:INFO:Set up encoding of categorical features.
2023-04-29 03:43:20,994:INFO:Set up feature normalization.
2023-04-29 03:43:20,994:INFO:Set up PCA.
2023-04-29 03:43:21,115:INFO:Finished creating preprocessing pipeline.
2023-04-29 03:43:21,134:INFO:Pipeline: Pipeline(memory=Memory(location=C:\Users\eeman\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'L', 'R', 'A_M'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose=0))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    inc...
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('pca',
                 TransformerWrapper(exclude=[], include=None,
                                    transformer=PCA(copy=True,
                                                    iterated_power='auto',
                                                    n_components=3,
                                                    random_state=None,
                                                    svd_solver='auto', tol=0.0,
                                                    whiten=False)))],
         verbose=False)
2023-04-29 03:43:21,137:INFO:Creating final display dataframe.
2023-04-29 03:43:21,974:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target              Type
2                   Target type        Multiclass
3           Original data shape          (216, 7)
4        Transformed data shape          (216, 4)
5   Transformed train set shape          (151, 4)
6    Transformed test set shape           (65, 4)
7              Numeric features                 4
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15                    Normalize              True
16             Normalize method            zscore
17                          PCA              True
18                   PCA method            linear
19               PCA components                 3
20               Fold Generator   StratifiedKFold
21                  Fold Number                10
22                     CPU Jobs                -1
23                      Use GPU             False
24               Log Experiment             False
25              Experiment Name  clf-default-name
26                          USI              6d9c
2023-04-29 03:43:22,278:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:22,278:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:22,544:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:22,544:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 03:43:22,547:INFO:setup() successfully completed in 3.44s...............
2023-04-29 03:43:22,574:INFO:Initializing compare_models()
2023-04-29 03:43:22,574:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-04-29 03:43:22,574:INFO:Checking exceptions
2023-04-29 03:43:22,581:INFO:Preparing display monitor
2023-04-29 03:43:22,727:INFO:Initializing Logistic Regression
2023-04-29 03:43:22,733:INFO:Total runtime is 0.0001069943110148112 minutes
2023-04-29 03:43:22,742:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:22,745:INFO:Initializing create_model()
2023-04-29 03:43:22,745:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:22,745:INFO:Checking exceptions
2023-04-29 03:43:22,745:INFO:Importing libraries
2023-04-29 03:43:22,745:INFO:Copying training dataset
2023-04-29 03:43:22,748:INFO:Defining folds
2023-04-29 03:43:22,748:INFO:Declaring metric variables
2023-04-29 03:43:22,757:INFO:Importing untrained model
2023-04-29 03:43:22,776:INFO:Logistic Regression Imported successfully
2023-04-29 03:43:22,800:INFO:Starting cross validation
2023-04-29 03:43:22,803:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:23,395:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:23,470:INFO:Calculating mean and std
2023-04-29 03:43:23,470:INFO:Creating metrics dataframe
2023-04-29 03:43:23,476:INFO:Uploading results into container
2023-04-29 03:43:23,479:INFO:Uploading model into container now
2023-04-29 03:43:23,479:INFO:_master_model_container: 1
2023-04-29 03:43:23,479:INFO:_display_container: 2
2023-04-29 03:43:23,479:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:43:23,479:INFO:create_model() successfully completed......................................
2023-04-29 03:43:23,800:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:23,803:INFO:Creating metrics dataframe
2023-04-29 03:43:23,821:INFO:Initializing K Neighbors Classifier
2023-04-29 03:43:23,821:INFO:Total runtime is 0.01824693282445272 minutes
2023-04-29 03:43:23,830:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:23,830:INFO:Initializing create_model()
2023-04-29 03:43:23,830:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:23,830:INFO:Checking exceptions
2023-04-29 03:43:23,830:INFO:Importing libraries
2023-04-29 03:43:23,830:INFO:Copying training dataset
2023-04-29 03:43:23,844:INFO:Defining folds
2023-04-29 03:43:23,844:INFO:Declaring metric variables
2023-04-29 03:43:23,849:INFO:Importing untrained model
2023-04-29 03:43:23,858:INFO:K Neighbors Classifier Imported successfully
2023-04-29 03:43:23,870:INFO:Starting cross validation
2023-04-29 03:43:23,873:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:24,279:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:43:24,285:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:43:24,291:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:43:24,301:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:43:24,310:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:43:24,320:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:43:24,355:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:43:24,361:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:43:24,371:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:43:24,399:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 03:43:24,424:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:24,506:INFO:Calculating mean and std
2023-04-29 03:43:24,509:INFO:Creating metrics dataframe
2023-04-29 03:43:24,518:INFO:Uploading results into container
2023-04-29 03:43:24,518:INFO:Uploading model into container now
2023-04-29 03:43:24,521:INFO:_master_model_container: 2
2023-04-29 03:43:24,521:INFO:_display_container: 2
2023-04-29 03:43:24,521:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-04-29 03:43:24,521:INFO:create_model() successfully completed......................................
2023-04-29 03:43:24,733:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:24,733:INFO:Creating metrics dataframe
2023-04-29 03:43:24,756:INFO:Initializing Naive Bayes
2023-04-29 03:43:24,756:INFO:Total runtime is 0.033830992380778 minutes
2023-04-29 03:43:24,765:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:24,765:INFO:Initializing create_model()
2023-04-29 03:43:24,765:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:24,768:INFO:Checking exceptions
2023-04-29 03:43:24,768:INFO:Importing libraries
2023-04-29 03:43:24,768:INFO:Copying training dataset
2023-04-29 03:43:24,777:INFO:Defining folds
2023-04-29 03:43:24,777:INFO:Declaring metric variables
2023-04-29 03:43:24,785:INFO:Importing untrained model
2023-04-29 03:43:24,794:INFO:Naive Bayes Imported successfully
2023-04-29 03:43:24,813:INFO:Starting cross validation
2023-04-29 03:43:24,815:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:25,258:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:25,313:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:25,322:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:25,349:INFO:Calculating mean and std
2023-04-29 03:43:25,352:INFO:Creating metrics dataframe
2023-04-29 03:43:25,361:INFO:Uploading results into container
2023-04-29 03:43:25,364:INFO:Uploading model into container now
2023-04-29 03:43:25,364:INFO:_master_model_container: 3
2023-04-29 03:43:25,364:INFO:_display_container: 2
2023-04-29 03:43:25,364:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:43:25,364:INFO:create_model() successfully completed......................................
2023-04-29 03:43:25,750:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:25,754:INFO:Creating metrics dataframe
2023-04-29 03:43:25,785:INFO:Initializing Decision Tree Classifier
2023-04-29 03:43:25,785:INFO:Total runtime is 0.05097347100575765 minutes
2023-04-29 03:43:25,801:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:25,801:INFO:Initializing create_model()
2023-04-29 03:43:25,801:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:25,801:INFO:Checking exceptions
2023-04-29 03:43:25,804:INFO:Importing libraries
2023-04-29 03:43:25,804:INFO:Copying training dataset
2023-04-29 03:43:25,816:INFO:Defining folds
2023-04-29 03:43:25,816:INFO:Declaring metric variables
2023-04-29 03:43:25,832:INFO:Importing untrained model
2023-04-29 03:43:25,842:INFO:Decision Tree Classifier Imported successfully
2023-04-29 03:43:25,875:INFO:Starting cross validation
2023-04-29 03:43:25,878:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:26,410:INFO:Calculating mean and std
2023-04-29 03:43:26,414:INFO:Creating metrics dataframe
2023-04-29 03:43:26,422:INFO:Uploading results into container
2023-04-29 03:43:26,423:INFO:Uploading model into container now
2023-04-29 03:43:26,424:INFO:_master_model_container: 4
2023-04-29 03:43:26,424:INFO:_display_container: 2
2023-04-29 03:43:26,425:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=123, splitter='best')
2023-04-29 03:43:26,425:INFO:create_model() successfully completed......................................
2023-04-29 03:43:26,730:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:26,731:INFO:Creating metrics dataframe
2023-04-29 03:43:26,774:INFO:Initializing SVM - Linear Kernel
2023-04-29 03:43:26,774:INFO:Total runtime is 0.06746254762013754 minutes
2023-04-29 03:43:26,784:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:26,785:INFO:Initializing create_model()
2023-04-29 03:43:26,785:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:26,785:INFO:Checking exceptions
2023-04-29 03:43:26,785:INFO:Importing libraries
2023-04-29 03:43:26,787:INFO:Copying training dataset
2023-04-29 03:43:26,798:INFO:Defining folds
2023-04-29 03:43:26,799:INFO:Declaring metric variables
2023-04-29 03:43:26,808:INFO:Importing untrained model
2023-04-29 03:43:26,827:INFO:SVM - Linear Kernel Imported successfully
2023-04-29 03:43:26,850:INFO:Starting cross validation
2023-04-29 03:43:26,854:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:27,266:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:27,267:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:27,281:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:27,303:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:27,331:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:27,340:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:27,356:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:27,362:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:27,374:INFO:Calculating mean and std
2023-04-29 03:43:27,378:INFO:Creating metrics dataframe
2023-04-29 03:43:27,386:INFO:Uploading results into container
2023-04-29 03:43:27,389:INFO:Uploading model into container now
2023-04-29 03:43:27,390:INFO:_master_model_container: 5
2023-04-29 03:43:27,390:INFO:_display_container: 2
2023-04-29 03:43:27,391:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-04-29 03:43:27,392:INFO:create_model() successfully completed......................................
2023-04-29 03:43:27,694:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:27,694:INFO:Creating metrics dataframe
2023-04-29 03:43:27,721:INFO:Initializing Ridge Classifier
2023-04-29 03:43:27,722:INFO:Total runtime is 0.08326124350229899 minutes
2023-04-29 03:43:27,729:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:27,729:INFO:Initializing create_model()
2023-04-29 03:43:27,729:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:27,730:INFO:Checking exceptions
2023-04-29 03:43:27,730:INFO:Importing libraries
2023-04-29 03:43:27,730:INFO:Copying training dataset
2023-04-29 03:43:27,749:INFO:Defining folds
2023-04-29 03:43:27,749:INFO:Declaring metric variables
2023-04-29 03:43:27,758:INFO:Importing untrained model
2023-04-29 03:43:27,773:INFO:Ridge Classifier Imported successfully
2023-04-29 03:43:27,801:INFO:Starting cross validation
2023-04-29 03:43:27,804:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:28,197:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:28,225:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:28,255:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:28,259:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:28,264:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:28,277:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:28,280:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:28,282:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:28,292:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:28,313:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:28,320:INFO:Calculating mean and std
2023-04-29 03:43:28,324:INFO:Creating metrics dataframe
2023-04-29 03:43:28,332:INFO:Uploading results into container
2023-04-29 03:43:28,333:INFO:Uploading model into container now
2023-04-29 03:43:28,334:INFO:_master_model_container: 6
2023-04-29 03:43:28,334:INFO:_display_container: 2
2023-04-29 03:43:28,335:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:43:28,335:INFO:create_model() successfully completed......................................
2023-04-29 03:43:28,629:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:28,630:INFO:Creating metrics dataframe
2023-04-29 03:43:28,667:INFO:Initializing Random Forest Classifier
2023-04-29 03:43:28,668:INFO:Total runtime is 0.09902886549631755 minutes
2023-04-29 03:43:28,683:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:28,684:INFO:Initializing create_model()
2023-04-29 03:43:28,684:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:28,684:INFO:Checking exceptions
2023-04-29 03:43:28,685:INFO:Importing libraries
2023-04-29 03:43:28,685:INFO:Copying training dataset
2023-04-29 03:43:28,708:INFO:Defining folds
2023-04-29 03:43:28,708:INFO:Declaring metric variables
2023-04-29 03:43:28,720:INFO:Importing untrained model
2023-04-29 03:43:28,732:INFO:Random Forest Classifier Imported successfully
2023-04-29 03:43:28,749:INFO:Starting cross validation
2023-04-29 03:43:28,752:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:30,364:INFO:Calculating mean and std
2023-04-29 03:43:30,368:INFO:Creating metrics dataframe
2023-04-29 03:43:30,376:INFO:Uploading results into container
2023-04-29 03:43:30,379:INFO:Uploading model into container now
2023-04-29 03:43:30,379:INFO:_master_model_container: 7
2023-04-29 03:43:30,379:INFO:_display_container: 2
2023-04-29 03:43:30,381:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 03:43:30,381:INFO:create_model() successfully completed......................................
2023-04-29 03:43:30,703:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:30,705:INFO:Creating metrics dataframe
2023-04-29 03:43:30,753:INFO:Initializing Quadratic Discriminant Analysis
2023-04-29 03:43:30,753:INFO:Total runtime is 0.13377832174301146 minutes
2023-04-29 03:43:30,761:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:30,762:INFO:Initializing create_model()
2023-04-29 03:43:30,762:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:30,763:INFO:Checking exceptions
2023-04-29 03:43:30,763:INFO:Importing libraries
2023-04-29 03:43:30,764:INFO:Copying training dataset
2023-04-29 03:43:30,773:INFO:Defining folds
2023-04-29 03:43:30,773:INFO:Declaring metric variables
2023-04-29 03:43:30,794:INFO:Importing untrained model
2023-04-29 03:43:30,806:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 03:43:30,828:INFO:Starting cross validation
2023-04-29 03:43:30,832:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:31,001:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:43:31,096:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:43:31,156:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:43:31,178:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:43:31,237:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:43:31,243:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:43:31,270:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:43:31,274:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:43:31,278:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:43:31,292:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 03:43:31,362:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:31,414:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:31,436:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:31,460:INFO:Calculating mean and std
2023-04-29 03:43:31,464:INFO:Creating metrics dataframe
2023-04-29 03:43:31,472:INFO:Uploading results into container
2023-04-29 03:43:31,472:INFO:Uploading model into container now
2023-04-29 03:43:31,474:INFO:_master_model_container: 8
2023-04-29 03:43:31,474:INFO:_display_container: 2
2023-04-29 03:43:31,474:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 03:43:31,476:INFO:create_model() successfully completed......................................
2023-04-29 03:43:31,802:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:31,804:INFO:Creating metrics dataframe
2023-04-29 03:43:31,840:INFO:Initializing Ada Boost Classifier
2023-04-29 03:43:31,840:INFO:Total runtime is 0.15189481576283773 minutes
2023-04-29 03:43:31,851:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:31,851:INFO:Initializing create_model()
2023-04-29 03:43:31,853:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:31,853:INFO:Checking exceptions
2023-04-29 03:43:31,853:INFO:Importing libraries
2023-04-29 03:43:31,853:INFO:Copying training dataset
2023-04-29 03:43:31,869:INFO:Defining folds
2023-04-29 03:43:31,871:INFO:Declaring metric variables
2023-04-29 03:43:31,879:INFO:Importing untrained model
2023-04-29 03:43:31,895:INFO:Ada Boost Classifier Imported successfully
2023-04-29 03:43:31,924:INFO:Starting cross validation
2023-04-29 03:43:31,930:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:32,506:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:32,511:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:32,546:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:32,565:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:32,593:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:32,600:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:32,609:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:32,647:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:32,656:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:32,711:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:32,718:INFO:Calculating mean and std
2023-04-29 03:43:32,721:INFO:Creating metrics dataframe
2023-04-29 03:43:32,739:INFO:Uploading results into container
2023-04-29 03:43:32,741:INFO:Uploading model into container now
2023-04-29 03:43:32,742:INFO:_master_model_container: 9
2023-04-29 03:43:32,742:INFO:_display_container: 2
2023-04-29 03:43:32,742:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2023-04-29 03:43:32,743:INFO:create_model() successfully completed......................................
2023-04-29 03:43:32,966:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:32,966:INFO:Creating metrics dataframe
2023-04-29 03:43:32,997:INFO:Initializing Gradient Boosting Classifier
2023-04-29 03:43:32,997:INFO:Total runtime is 0.17117421627044677 minutes
2023-04-29 03:43:33,004:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:33,005:INFO:Initializing create_model()
2023-04-29 03:43:33,005:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:33,005:INFO:Checking exceptions
2023-04-29 03:43:33,005:INFO:Importing libraries
2023-04-29 03:43:33,005:INFO:Copying training dataset
2023-04-29 03:43:33,013:INFO:Defining folds
2023-04-29 03:43:33,013:INFO:Declaring metric variables
2023-04-29 03:43:33,024:INFO:Importing untrained model
2023-04-29 03:43:33,033:INFO:Gradient Boosting Classifier Imported successfully
2023-04-29 03:43:33,048:INFO:Starting cross validation
2023-04-29 03:43:33,051:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:34,936:INFO:Calculating mean and std
2023-04-29 03:43:34,940:INFO:Creating metrics dataframe
2023-04-29 03:43:34,947:INFO:Uploading results into container
2023-04-29 03:43:34,948:INFO:Uploading model into container now
2023-04-29 03:43:34,949:INFO:_master_model_container: 10
2023-04-29 03:43:34,949:INFO:_display_container: 2
2023-04-29 03:43:34,950:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-04-29 03:43:34,950:INFO:create_model() successfully completed......................................
2023-04-29 03:43:35,171:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:35,173:INFO:Creating metrics dataframe
2023-04-29 03:43:35,209:INFO:Initializing Linear Discriminant Analysis
2023-04-29 03:43:35,209:INFO:Total runtime is 0.2080421765645345 minutes
2023-04-29 03:43:35,220:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:35,221:INFO:Initializing create_model()
2023-04-29 03:43:35,221:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:35,222:INFO:Checking exceptions
2023-04-29 03:43:35,222:INFO:Importing libraries
2023-04-29 03:43:35,222:INFO:Copying training dataset
2023-04-29 03:43:35,235:INFO:Defining folds
2023-04-29 03:43:35,236:INFO:Declaring metric variables
2023-04-29 03:43:35,249:INFO:Importing untrained model
2023-04-29 03:43:35,263:INFO:Linear Discriminant Analysis Imported successfully
2023-04-29 03:43:35,287:INFO:Starting cross validation
2023-04-29 03:43:35,291:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:35,667:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:35,707:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:35,711:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:35,727:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:35,731:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:35,731:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:35,739:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:35,763:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:35,763:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:35,775:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:35,779:INFO:Calculating mean and std
2023-04-29 03:43:35,783:INFO:Creating metrics dataframe
2023-04-29 03:43:35,791:INFO:Uploading results into container
2023-04-29 03:43:35,795:INFO:Uploading model into container now
2023-04-29 03:43:35,795:INFO:_master_model_container: 11
2023-04-29 03:43:35,795:INFO:_display_container: 2
2023-04-29 03:43:35,795:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-04-29 03:43:35,795:INFO:create_model() successfully completed......................................
2023-04-29 03:43:36,136:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:36,136:INFO:Creating metrics dataframe
2023-04-29 03:43:36,188:INFO:Initializing Extra Trees Classifier
2023-04-29 03:43:36,188:INFO:Total runtime is 0.2243579904238383 minutes
2023-04-29 03:43:36,196:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:36,200:INFO:Initializing create_model()
2023-04-29 03:43:36,200:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:36,200:INFO:Checking exceptions
2023-04-29 03:43:36,200:INFO:Importing libraries
2023-04-29 03:43:36,200:INFO:Copying training dataset
2023-04-29 03:43:36,212:INFO:Defining folds
2023-04-29 03:43:36,212:INFO:Declaring metric variables
2023-04-29 03:43:36,220:INFO:Importing untrained model
2023-04-29 03:43:36,232:INFO:Extra Trees Classifier Imported successfully
2023-04-29 03:43:36,249:INFO:Starting cross validation
2023-04-29 03:43:36,253:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:37,371:INFO:Calculating mean and std
2023-04-29 03:43:37,375:INFO:Creating metrics dataframe
2023-04-29 03:43:37,383:INFO:Uploading results into container
2023-04-29 03:43:37,383:INFO:Uploading model into container now
2023-04-29 03:43:37,383:INFO:_master_model_container: 12
2023-04-29 03:43:37,383:INFO:_display_container: 2
2023-04-29 03:43:37,387:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 03:43:37,387:INFO:create_model() successfully completed......................................
2023-04-29 03:43:37,685:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:37,689:INFO:Creating metrics dataframe
2023-04-29 03:43:37,791:INFO:Initializing Light Gradient Boosting Machine
2023-04-29 03:43:37,791:INFO:Total runtime is 0.25107314189275104 minutes
2023-04-29 03:43:37,816:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:37,816:INFO:Initializing create_model()
2023-04-29 03:43:37,816:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:37,816:INFO:Checking exceptions
2023-04-29 03:43:37,816:INFO:Importing libraries
2023-04-29 03:43:37,816:INFO:Copying training dataset
2023-04-29 03:43:37,832:INFO:Defining folds
2023-04-29 03:43:37,832:INFO:Declaring metric variables
2023-04-29 03:43:37,848:INFO:Importing untrained model
2023-04-29 03:43:37,856:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 03:43:37,885:INFO:Starting cross validation
2023-04-29 03:43:37,889:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:40,417:INFO:Calculating mean and std
2023-04-29 03:43:40,423:INFO:Creating metrics dataframe
2023-04-29 03:43:40,432:INFO:Uploading results into container
2023-04-29 03:43:40,432:INFO:Uploading model into container now
2023-04-29 03:43:40,435:INFO:_master_model_container: 13
2023-04-29 03:43:40,435:INFO:_display_container: 2
2023-04-29 03:43:40,435:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 03:43:40,435:INFO:create_model() successfully completed......................................
2023-04-29 03:43:40,645:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:40,645:INFO:Creating metrics dataframe
2023-04-29 03:43:40,678:INFO:Initializing Dummy Classifier
2023-04-29 03:43:40,678:INFO:Total runtime is 0.29919657309850056 minutes
2023-04-29 03:43:40,684:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:40,687:INFO:Initializing create_model()
2023-04-29 03:43:40,687:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319F70>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:40,687:INFO:Checking exceptions
2023-04-29 03:43:40,687:INFO:Importing libraries
2023-04-29 03:43:40,687:INFO:Copying training dataset
2023-04-29 03:43:40,693:INFO:Defining folds
2023-04-29 03:43:40,696:INFO:Declaring metric variables
2023-04-29 03:43:40,702:INFO:Importing untrained model
2023-04-29 03:43:40,708:INFO:Dummy Classifier Imported successfully
2023-04-29 03:43:40,725:INFO:Starting cross validation
2023-04-29 03:43:40,728:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:41,113:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:41,125:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:41,137:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:41,140:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:41,143:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:41,146:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:41,146:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:41,158:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:41,164:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:41,173:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:41,179:INFO:Calculating mean and std
2023-04-29 03:43:41,185:INFO:Creating metrics dataframe
2023-04-29 03:43:41,191:INFO:Uploading results into container
2023-04-29 03:43:41,191:INFO:Uploading model into container now
2023-04-29 03:43:41,194:INFO:_master_model_container: 14
2023-04-29 03:43:41,194:INFO:_display_container: 2
2023-04-29 03:43:41,194:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2023-04-29 03:43:41,194:INFO:create_model() successfully completed......................................
2023-04-29 03:43:41,402:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:41,402:INFO:Creating metrics dataframe
2023-04-29 03:43:41,472:INFO:Initializing create_model()
2023-04-29 03:43:41,472:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:41,472:INFO:Checking exceptions
2023-04-29 03:43:41,476:INFO:Importing libraries
2023-04-29 03:43:41,476:INFO:Copying training dataset
2023-04-29 03:43:41,480:INFO:Defining folds
2023-04-29 03:43:41,480:INFO:Declaring metric variables
2023-04-29 03:43:41,480:INFO:Importing untrained model
2023-04-29 03:43:41,480:INFO:Declaring custom model
2023-04-29 03:43:41,480:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 03:43:41,484:INFO:Cross validation set to False
2023-04-29 03:43:41,484:INFO:Fitting Model
2023-04-29 03:43:41,989:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 03:43:41,992:INFO:create_model() successfully completed......................................
2023-04-29 03:43:42,298:INFO:_master_model_container: 14
2023-04-29 03:43:42,298:INFO:_display_container: 2
2023-04-29 03:43:42,301:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 03:43:42,301:INFO:compare_models() successfully completed......................................
2023-04-29 03:43:42,361:INFO:Initializing tune_model()
2023-04-29 03:43:42,361:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>)
2023-04-29 03:43:42,361:INFO:Checking exceptions
2023-04-29 03:43:42,476:INFO:Copying training dataset
2023-04-29 03:43:42,482:INFO:Checking base model
2023-04-29 03:43:42,482:INFO:Base model : Light Gradient Boosting Machine
2023-04-29 03:43:42,491:INFO:Declaring metric variables
2023-04-29 03:43:42,503:INFO:Defining Hyperparameters
2023-04-29 03:43:42,806:INFO:Tuning with n_jobs=-1
2023-04-29 03:43:42,807:INFO:Initializing RandomizedSearchCV
2023-04-29 03:43:46,251:INFO:best_params: {'actual_estimator__reg_lambda': 0.0005, 'actual_estimator__reg_alpha': 0.005, 'actual_estimator__num_leaves': 150, 'actual_estimator__n_estimators': 20, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.4, 'actual_estimator__feature_fraction': 0.5, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.9}
2023-04-29 03:43:46,255:INFO:Hyperparameter search completed
2023-04-29 03:43:46,255:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:46,255:INFO:Initializing create_model()
2023-04-29 03:43:46,255:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE4E3EEE0>, model_only=True, return_train_score=False, kwargs={'reg_lambda': 0.0005, 'reg_alpha': 0.005, 'num_leaves': 150, 'n_estimators': 20, 'min_split_gain': 0.3, 'min_child_samples': 6, 'learning_rate': 0.4, 'feature_fraction': 0.5, 'bagging_freq': 3, 'bagging_fraction': 0.9})
2023-04-29 03:43:46,255:INFO:Checking exceptions
2023-04-29 03:43:46,255:INFO:Importing libraries
2023-04-29 03:43:46,255:INFO:Copying training dataset
2023-04-29 03:43:46,264:INFO:Defining folds
2023-04-29 03:43:46,264:INFO:Declaring metric variables
2023-04-29 03:43:46,268:INFO:Importing untrained model
2023-04-29 03:43:46,268:INFO:Declaring custom model
2023-04-29 03:43:46,276:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 03:43:46,288:INFO:Starting cross validation
2023-04-29 03:43:46,292:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:46,763:INFO:Calculating mean and std
2023-04-29 03:43:46,766:INFO:Creating metrics dataframe
2023-04-29 03:43:46,778:INFO:Finalizing model
2023-04-29 03:43:46,927:INFO:Uploading results into container
2023-04-29 03:43:46,929:INFO:Uploading model into container now
2023-04-29 03:43:46,930:INFO:_master_model_container: 15
2023-04-29 03:43:46,930:INFO:_display_container: 3
2023-04-29 03:43:46,932:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.4, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=20, n_jobs=-1, num_leaves=150, objective=None,
               random_state=123, reg_alpha=0.005, reg_lambda=0.0005,
               silent='warn', subsample=1.0, subsample_for_bin=200000,
               subsample_freq=0)
2023-04-29 03:43:46,932:INFO:create_model() successfully completed......................................
2023-04-29 03:43:47,112:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:47,112:INFO:choose_better activated
2023-04-29 03:43:47,120:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:47,122:INFO:Initializing create_model()
2023-04-29 03:43:47,122:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:47,122:INFO:Checking exceptions
2023-04-29 03:43:47,124:INFO:Importing libraries
2023-04-29 03:43:47,124:INFO:Copying training dataset
2023-04-29 03:43:47,128:INFO:Defining folds
2023-04-29 03:43:47,128:INFO:Declaring metric variables
2023-04-29 03:43:47,128:INFO:Importing untrained model
2023-04-29 03:43:47,128:INFO:Declaring custom model
2023-04-29 03:43:47,132:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 03:43:47,132:INFO:Starting cross validation
2023-04-29 03:43:47,136:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:47,743:INFO:Calculating mean and std
2023-04-29 03:43:47,743:INFO:Creating metrics dataframe
2023-04-29 03:43:47,747:INFO:Finalizing model
2023-04-29 03:43:48,289:INFO:Uploading results into container
2023-04-29 03:43:48,293:INFO:Uploading model into container now
2023-04-29 03:43:48,293:INFO:_master_model_container: 16
2023-04-29 03:43:48,293:INFO:_display_container: 4
2023-04-29 03:43:48,293:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 03:43:48,293:INFO:create_model() successfully completed......................................
2023-04-29 03:43:48,606:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:48,606:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9408
2023-04-29 03:43:48,610:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.4, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=20, n_jobs=-1, num_leaves=150, objective=None,
               random_state=123, reg_alpha=0.005, reg_lambda=0.0005,
               silent='warn', subsample=1.0, subsample_for_bin=200000,
               subsample_freq=0) result for Accuracy is 0.9271
2023-04-29 03:43:48,610:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2023-04-29 03:43:48,610:INFO:choose_better completed
2023-04-29 03:43:48,610:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 03:43:48,646:INFO:_master_model_container: 16
2023-04-29 03:43:48,646:INFO:_display_container: 3
2023-04-29 03:43:48,646:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 03:43:48,646:INFO:tune_model() successfully completed......................................
2023-04-29 03:43:49,036:INFO:Initializing evaluate_model()
2023-04-29 03:43:49,036:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:43:49,088:INFO:Initializing plot_model()
2023-04-29 03:43:49,088:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 03:43:49,090:INFO:Checking exceptions
2023-04-29 03:43:49,094:INFO:Preloading libraries
2023-04-29 03:43:49,134:INFO:Copying training dataset
2023-04-29 03:43:49,136:INFO:Plot type: pipeline
2023-04-29 03:43:49,487:INFO:Visual Rendered Successfully
2023-04-29 03:43:49,802:INFO:plot_model() successfully completed......................................
2023-04-29 03:43:49,846:INFO:Initializing create_model()
2023-04-29 03:43:49,846:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=ridge, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:49,848:INFO:Checking exceptions
2023-04-29 03:43:49,924:INFO:Importing libraries
2023-04-29 03:43:49,924:INFO:Copying training dataset
2023-04-29 03:43:49,933:INFO:Defining folds
2023-04-29 03:43:49,933:INFO:Declaring metric variables
2023-04-29 03:43:49,941:INFO:Importing untrained model
2023-04-29 03:43:49,951:INFO:Ridge Classifier Imported successfully
2023-04-29 03:43:49,969:INFO:Starting cross validation
2023-04-29 03:43:49,971:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:50,297:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:50,303:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:50,321:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:50,327:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:50,331:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:50,335:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:50,337:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:50,373:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:50,389:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:50,393:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:50,401:INFO:Calculating mean and std
2023-04-29 03:43:50,403:INFO:Creating metrics dataframe
2023-04-29 03:43:50,419:INFO:Finalizing model
2023-04-29 03:43:50,541:INFO:Uploading results into container
2023-04-29 03:43:50,545:INFO:Uploading model into container now
2023-04-29 03:43:50,584:INFO:_master_model_container: 17
2023-04-29 03:43:50,586:INFO:_display_container: 4
2023-04-29 03:43:50,586:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:43:50,588:INFO:create_model() successfully completed......................................
2023-04-29 03:43:50,951:INFO:Initializing tune_model()
2023-04-29 03:43:50,951:INFO:tune_model(estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>)
2023-04-29 03:43:50,951:INFO:Checking exceptions
2023-04-29 03:43:51,081:INFO:Copying training dataset
2023-04-29 03:43:51,087:INFO:Checking base model
2023-04-29 03:43:51,090:INFO:Base model : Ridge Classifier
2023-04-29 03:43:51,099:INFO:Declaring metric variables
2023-04-29 03:43:51,108:INFO:Defining Hyperparameters
2023-04-29 03:43:51,416:INFO:Tuning with n_jobs=-1
2023-04-29 03:43:51,416:INFO:Initializing RandomizedSearchCV
2023-04-29 03:43:51,624:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,679:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,701:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,710:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,759:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,765:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,777:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,792:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,798:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,813:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,858:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,946:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,964:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:51,985:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,000:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,043:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,046:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,082:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,088:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,091:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,112:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,129:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,146:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,188:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,209:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,227:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,272:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,292:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,305:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,318:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,324:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,345:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,366:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,369:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,405:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,433:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,457:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,475:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,499:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,523:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,539:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,560:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,572:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,587:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,617:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,620:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,659:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,671:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,686:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:52,722:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,740:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,761:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,776:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,782:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,816:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,833:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,846:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,855:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,882:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,931:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,940:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,970:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,976:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:52,985:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,021:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,027:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,067:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,078:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,090:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,108:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,124:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,151:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,193:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,205:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,217:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,235:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,274:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,277:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,290:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,293:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,317:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,338:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,359:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,380:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,428:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,453:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,456:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,465:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,501:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:53,504:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:53,516:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:53,535:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:53,555:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:53,564:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:53,616:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:53,616:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:53,655:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:53,670:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 03:43:53,739:INFO:best_params: {'actual_estimator__normalize': False, 'actual_estimator__fit_intercept': True, 'actual_estimator__alpha': 8.6}
2023-04-29 03:43:53,742:INFO:Hyperparameter search completed
2023-04-29 03:43:53,745:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:53,745:INFO:Initializing create_model()
2023-04-29 03:43:53,745:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE5319B50>, model_only=True, return_train_score=False, kwargs={'normalize': False, 'fit_intercept': True, 'alpha': 8.6})
2023-04-29 03:43:53,745:INFO:Checking exceptions
2023-04-29 03:43:53,745:INFO:Importing libraries
2023-04-29 03:43:53,745:INFO:Copying training dataset
2023-04-29 03:43:53,751:INFO:Defining folds
2023-04-29 03:43:53,754:INFO:Declaring metric variables
2023-04-29 03:43:53,763:INFO:Importing untrained model
2023-04-29 03:43:53,763:INFO:Declaring custom model
2023-04-29 03:43:53,778:INFO:Ridge Classifier Imported successfully
2023-04-29 03:43:53,799:INFO:Starting cross validation
2023-04-29 03:43:53,805:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:54,008:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,032:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,068:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,092:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,101:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,119:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,128:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:54,131:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,131:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:54,140:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,159:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,168:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:54,171:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,189:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:54,198:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:54,216:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:54,231:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:54,243:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:54,255:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:54,264:INFO:Calculating mean and std
2023-04-29 03:43:54,267:INFO:Creating metrics dataframe
2023-04-29 03:43:54,282:INFO:Finalizing model
2023-04-29 03:43:54,397:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 03:43:54,424:INFO:Uploading results into container
2023-04-29 03:43:54,427:INFO:Uploading model into container now
2023-04-29 03:43:54,430:INFO:_master_model_container: 18
2023-04-29 03:43:54,430:INFO:_display_container: 5
2023-04-29 03:43:54,433:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:43:54,433:INFO:create_model() successfully completed......................................
2023-04-29 03:43:54,772:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:54,775:INFO:choose_better activated
2023-04-29 03:43:54,784:INFO:SubProcess create_model() called ==================================
2023-04-29 03:43:54,787:INFO:Initializing create_model()
2023-04-29 03:43:54,787:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:54,787:INFO:Checking exceptions
2023-04-29 03:43:54,793:INFO:Importing libraries
2023-04-29 03:43:54,793:INFO:Copying training dataset
2023-04-29 03:43:54,802:INFO:Defining folds
2023-04-29 03:43:54,802:INFO:Declaring metric variables
2023-04-29 03:43:54,802:INFO:Importing untrained model
2023-04-29 03:43:54,802:INFO:Declaring custom model
2023-04-29 03:43:54,805:INFO:Ridge Classifier Imported successfully
2023-04-29 03:43:54,805:INFO:Starting cross validation
2023-04-29 03:43:54,811:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:55,217:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:55,217:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:55,223:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:55,236:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:55,242:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:55,245:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:55,263:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:55,266:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:55,269:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:55,329:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:55,335:INFO:Calculating mean and std
2023-04-29 03:43:55,338:INFO:Creating metrics dataframe
2023-04-29 03:43:55,341:INFO:Finalizing model
2023-04-29 03:43:55,416:INFO:Uploading results into container
2023-04-29 03:43:55,419:INFO:Uploading model into container now
2023-04-29 03:43:55,419:INFO:_master_model_container: 19
2023-04-29 03:43:55,419:INFO:_display_container: 6
2023-04-29 03:43:55,419:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:43:55,419:INFO:create_model() successfully completed......................................
2023-04-29 03:43:55,627:INFO:SubProcess create_model() end ==================================
2023-04-29 03:43:55,627:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001) result for Accuracy is 0.51
2023-04-29 03:43:55,630:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001) result for Accuracy is 0.51
2023-04-29 03:43:55,630:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001) is best model
2023-04-29 03:43:55,630:INFO:choose_better completed
2023-04-29 03:43:55,630:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 03:43:55,661:INFO:_master_model_container: 19
2023-04-29 03:43:55,661:INFO:_display_container: 5
2023-04-29 03:43:55,664:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 03:43:55,664:INFO:tune_model() successfully completed......................................
2023-04-29 03:43:55,915:INFO:Initializing evaluate_model()
2023-04-29 03:43:55,915:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:43:55,961:INFO:Initializing plot_model()
2023-04-29 03:43:55,961:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 03:43:55,961:INFO:Checking exceptions
2023-04-29 03:43:55,967:INFO:Preloading libraries
2023-04-29 03:43:55,967:INFO:Copying training dataset
2023-04-29 03:43:55,967:INFO:Plot type: pipeline
2023-04-29 03:43:56,313:INFO:Visual Rendered Successfully
2023-04-29 03:43:56,524:INFO:plot_model() successfully completed......................................
2023-04-29 03:43:56,546:INFO:Initializing create_model()
2023-04-29 03:43:56,549:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=nb, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:43:56,549:INFO:Checking exceptions
2023-04-29 03:43:56,597:INFO:Importing libraries
2023-04-29 03:43:56,600:INFO:Copying training dataset
2023-04-29 03:43:56,606:INFO:Defining folds
2023-04-29 03:43:56,606:INFO:Declaring metric variables
2023-04-29 03:43:56,615:INFO:Importing untrained model
2023-04-29 03:43:56,621:INFO:Naive Bayes Imported successfully
2023-04-29 03:43:56,641:INFO:Starting cross validation
2023-04-29 03:43:56,642:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:43:57,151:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:57,208:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:57,247:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:43:57,271:INFO:Calculating mean and std
2023-04-29 03:43:57,274:INFO:Creating metrics dataframe
2023-04-29 03:43:57,286:INFO:Finalizing model
2023-04-29 03:43:57,374:INFO:Uploading results into container
2023-04-29 03:43:57,377:INFO:Uploading model into container now
2023-04-29 03:43:57,398:INFO:_master_model_container: 20
2023-04-29 03:43:57,398:INFO:_display_container: 6
2023-04-29 03:43:57,401:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:43:57,401:INFO:create_model() successfully completed......................................
2023-04-29 03:43:57,649:INFO:Initializing tune_model()
2023-04-29 03:43:57,649:INFO:tune_model(estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>)
2023-04-29 03:43:57,649:INFO:Checking exceptions
2023-04-29 03:43:57,765:INFO:Copying training dataset
2023-04-29 03:43:57,770:INFO:Checking base model
2023-04-29 03:43:57,770:INFO:Base model : Naive Bayes
2023-04-29 03:43:57,780:INFO:Declaring metric variables
2023-04-29 03:43:57,791:INFO:Defining Hyperparameters
2023-04-29 03:43:58,013:INFO:Tuning with n_jobs=-1
2023-04-29 03:43:58,013:INFO:Initializing RandomizedSearchCV
2023-04-29 03:44:00,251:INFO:best_params: {'actual_estimator__var_smoothing': 2e-09}
2023-04-29 03:44:00,253:INFO:Hyperparameter search completed
2023-04-29 03:44:00,254:INFO:SubProcess create_model() called ==================================
2023-04-29 03:44:00,255:INFO:Initializing create_model()
2023-04-29 03:44:00,255:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EEA90B970>, model_only=True, return_train_score=False, kwargs={'var_smoothing': 2e-09})
2023-04-29 03:44:00,255:INFO:Checking exceptions
2023-04-29 03:44:00,255:INFO:Importing libraries
2023-04-29 03:44:00,255:INFO:Copying training dataset
2023-04-29 03:44:00,260:INFO:Defining folds
2023-04-29 03:44:00,261:INFO:Declaring metric variables
2023-04-29 03:44:00,269:INFO:Importing untrained model
2023-04-29 03:44:00,270:INFO:Declaring custom model
2023-04-29 03:44:00,278:INFO:Naive Bayes Imported successfully
2023-04-29 03:44:00,296:INFO:Starting cross validation
2023-04-29 03:44:00,300:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:44:00,730:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:44:00,749:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:44:00,752:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:44:00,803:INFO:Calculating mean and std
2023-04-29 03:44:00,807:INFO:Creating metrics dataframe
2023-04-29 03:44:00,819:INFO:Finalizing model
2023-04-29 03:44:00,911:INFO:Uploading results into container
2023-04-29 03:44:00,915:INFO:Uploading model into container now
2023-04-29 03:44:00,915:INFO:_master_model_container: 21
2023-04-29 03:44:00,915:INFO:_display_container: 7
2023-04-29 03:44:00,915:INFO:GaussianNB(priors=None, var_smoothing=2e-09)
2023-04-29 03:44:00,915:INFO:create_model() successfully completed......................................
2023-04-29 03:44:01,168:INFO:SubProcess create_model() end ==================================
2023-04-29 03:44:01,168:INFO:choose_better activated
2023-04-29 03:44:01,177:INFO:SubProcess create_model() called ==================================
2023-04-29 03:44:01,180:INFO:Initializing create_model()
2023-04-29 03:44:01,180:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:44:01,180:INFO:Checking exceptions
2023-04-29 03:44:01,183:INFO:Importing libraries
2023-04-29 03:44:01,183:INFO:Copying training dataset
2023-04-29 03:44:01,192:INFO:Defining folds
2023-04-29 03:44:01,192:INFO:Declaring metric variables
2023-04-29 03:44:01,192:INFO:Importing untrained model
2023-04-29 03:44:01,192:INFO:Declaring custom model
2023-04-29 03:44:01,195:INFO:Naive Bayes Imported successfully
2023-04-29 03:44:01,195:INFO:Starting cross validation
2023-04-29 03:44:01,198:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:44:01,629:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:44:01,677:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:44:01,686:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:44:01,710:INFO:Calculating mean and std
2023-04-29 03:44:01,713:INFO:Creating metrics dataframe
2023-04-29 03:44:01,719:INFO:Finalizing model
2023-04-29 03:44:01,794:INFO:Uploading results into container
2023-04-29 03:44:01,794:INFO:Uploading model into container now
2023-04-29 03:44:01,794:INFO:_master_model_container: 22
2023-04-29 03:44:01,794:INFO:_display_container: 8
2023-04-29 03:44:01,794:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:44:01,794:INFO:create_model() successfully completed......................................
2023-04-29 03:44:02,014:INFO:SubProcess create_model() end ==================================
2023-04-29 03:44:02,014:INFO:GaussianNB(priors=None, var_smoothing=1e-09) result for Accuracy is 0.7683
2023-04-29 03:44:02,017:INFO:GaussianNB(priors=None, var_smoothing=2e-09) result for Accuracy is 0.7683
2023-04-29 03:44:02,017:INFO:GaussianNB(priors=None, var_smoothing=1e-09) is best model
2023-04-29 03:44:02,017:INFO:choose_better completed
2023-04-29 03:44:02,017:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 03:44:02,044:INFO:_master_model_container: 22
2023-04-29 03:44:02,044:INFO:_display_container: 7
2023-04-29 03:44:02,044:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 03:44:02,044:INFO:tune_model() successfully completed......................................
2023-04-29 03:44:02,329:INFO:Initializing evaluate_model()
2023-04-29 03:44:02,329:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:44:02,378:INFO:Initializing plot_model()
2023-04-29 03:44:02,378:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GaussianNB(priors=None, var_smoothing=1e-09), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 03:44:02,378:INFO:Checking exceptions
2023-04-29 03:44:02,384:INFO:Preloading libraries
2023-04-29 03:44:02,384:INFO:Copying training dataset
2023-04-29 03:44:02,384:INFO:Plot type: pipeline
2023-04-29 03:44:02,731:INFO:Visual Rendered Successfully
2023-04-29 03:44:02,948:INFO:plot_model() successfully completed......................................
2023-04-29 03:44:02,981:INFO:Initializing create_model()
2023-04-29 03:44:02,981:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:44:02,981:INFO:Checking exceptions
2023-04-29 03:44:03,053:INFO:Importing libraries
2023-04-29 03:44:03,053:INFO:Copying training dataset
2023-04-29 03:44:03,059:INFO:Defining folds
2023-04-29 03:44:03,059:INFO:Declaring metric variables
2023-04-29 03:44:03,065:INFO:Importing untrained model
2023-04-29 03:44:03,080:INFO:Logistic Regression Imported successfully
2023-04-29 03:44:03,095:INFO:Starting cross validation
2023-04-29 03:44:03,098:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:44:03,625:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:44:03,700:INFO:Calculating mean and std
2023-04-29 03:44:03,703:INFO:Creating metrics dataframe
2023-04-29 03:44:03,712:INFO:Finalizing model
2023-04-29 03:44:03,851:INFO:Uploading results into container
2023-04-29 03:44:03,854:INFO:Uploading model into container now
2023-04-29 03:44:03,884:INFO:_master_model_container: 23
2023-04-29 03:44:03,884:INFO:_display_container: 8
2023-04-29 03:44:03,887:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:44:03,887:INFO:create_model() successfully completed......................................
2023-04-29 03:44:04,116:INFO:Initializing tune_model()
2023-04-29 03:44:04,119:INFO:tune_model(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>)
2023-04-29 03:44:04,119:INFO:Checking exceptions
2023-04-29 03:44:04,211:INFO:Copying training dataset
2023-04-29 03:44:04,220:INFO:Checking base model
2023-04-29 03:44:04,220:INFO:Base model : Logistic Regression
2023-04-29 03:44:04,226:INFO:Declaring metric variables
2023-04-29 03:44:04,238:INFO:Defining Hyperparameters
2023-04-29 03:44:04,479:INFO:Tuning with n_jobs=-1
2023-04-29 03:44:04,479:INFO:Initializing RandomizedSearchCV
2023-04-29 03:44:07,844:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 3.882}
2023-04-29 03:44:07,847:INFO:Hyperparameter search completed
2023-04-29 03:44:07,850:INFO:SubProcess create_model() called ==================================
2023-04-29 03:44:07,850:INFO:Initializing create_model()
2023-04-29 03:44:07,850:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE4DE28E0>, model_only=True, return_train_score=False, kwargs={'class_weight': {}, 'C': 3.882})
2023-04-29 03:44:07,850:INFO:Checking exceptions
2023-04-29 03:44:07,850:INFO:Importing libraries
2023-04-29 03:44:07,850:INFO:Copying training dataset
2023-04-29 03:44:07,859:INFO:Defining folds
2023-04-29 03:44:07,859:INFO:Declaring metric variables
2023-04-29 03:44:07,866:INFO:Importing untrained model
2023-04-29 03:44:07,866:INFO:Declaring custom model
2023-04-29 03:44:07,875:INFO:Logistic Regression Imported successfully
2023-04-29 03:44:07,893:INFO:Starting cross validation
2023-04-29 03:44:07,899:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:44:08,498:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:44:08,540:INFO:Calculating mean and std
2023-04-29 03:44:08,543:INFO:Creating metrics dataframe
2023-04-29 03:44:08,558:INFO:Finalizing model
2023-04-29 03:44:08,813:INFO:Uploading results into container
2023-04-29 03:44:08,813:INFO:Uploading model into container now
2023-04-29 03:44:08,813:INFO:_master_model_container: 24
2023-04-29 03:44:08,813:INFO:_display_container: 9
2023-04-29 03:44:08,817:INFO:LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:44:08,817:INFO:create_model() successfully completed......................................
2023-04-29 03:44:09,138:INFO:SubProcess create_model() end ==================================
2023-04-29 03:44:09,138:INFO:choose_better activated
2023-04-29 03:44:09,150:INFO:SubProcess create_model() called ==================================
2023-04-29 03:44:09,154:INFO:Initializing create_model()
2023-04-29 03:44:09,154:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:44:09,154:INFO:Checking exceptions
2023-04-29 03:44:09,158:INFO:Importing libraries
2023-04-29 03:44:09,158:INFO:Copying training dataset
2023-04-29 03:44:09,171:INFO:Defining folds
2023-04-29 03:44:09,171:INFO:Declaring metric variables
2023-04-29 03:44:09,171:INFO:Importing untrained model
2023-04-29 03:44:09,171:INFO:Declaring custom model
2023-04-29 03:44:09,175:INFO:Logistic Regression Imported successfully
2023-04-29 03:44:09,175:INFO:Starting cross validation
2023-04-29 03:44:09,179:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:44:09,882:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 03:44:09,902:INFO:Calculating mean and std
2023-04-29 03:44:09,906:INFO:Creating metrics dataframe
2023-04-29 03:44:09,910:INFO:Finalizing model
2023-04-29 03:44:10,008:INFO:Uploading results into container
2023-04-29 03:44:10,008:INFO:Uploading model into container now
2023-04-29 03:44:10,008:INFO:_master_model_container: 25
2023-04-29 03:44:10,008:INFO:_display_container: 10
2023-04-29 03:44:10,008:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:44:10,008:INFO:create_model() successfully completed......................................
2023-04-29 03:44:10,190:INFO:SubProcess create_model() end ==================================
2023-04-29 03:44:10,190:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.7288
2023-04-29 03:44:10,194:INFO:LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.7888
2023-04-29 03:44:10,194:INFO:LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2023-04-29 03:44:10,194:INFO:choose_better completed
2023-04-29 03:44:10,215:INFO:_master_model_container: 25
2023-04-29 03:44:10,216:INFO:_display_container: 9
2023-04-29 03:44:10,216:INFO:LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 03:44:10,216:INFO:tune_model() successfully completed......................................
2023-04-29 03:44:10,468:INFO:Initializing evaluate_model()
2023-04-29 03:44:10,468:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:44:10,512:INFO:Initializing plot_model()
2023-04-29 03:44:10,512:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 03:44:10,512:INFO:Checking exceptions
2023-04-29 03:44:10,516:INFO:Preloading libraries
2023-04-29 03:44:10,516:INFO:Copying training dataset
2023-04-29 03:44:10,516:INFO:Plot type: pipeline
2023-04-29 03:44:10,865:INFO:Visual Rendered Successfully
2023-04-29 03:44:11,226:INFO:plot_model() successfully completed......................................
2023-04-29 03:44:11,254:INFO:Initializing create_model()
2023-04-29 03:44:11,254:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=et, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:44:11,254:INFO:Checking exceptions
2023-04-29 03:44:11,330:INFO:Importing libraries
2023-04-29 03:44:11,330:INFO:Copying training dataset
2023-04-29 03:44:11,338:INFO:Defining folds
2023-04-29 03:44:11,342:INFO:Declaring metric variables
2023-04-29 03:44:11,354:INFO:Importing untrained model
2023-04-29 03:44:11,366:INFO:Extra Trees Classifier Imported successfully
2023-04-29 03:44:11,383:INFO:Starting cross validation
2023-04-29 03:44:11,387:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:44:12,661:INFO:Calculating mean and std
2023-04-29 03:44:12,665:INFO:Creating metrics dataframe
2023-04-29 03:44:12,685:INFO:Finalizing model
2023-04-29 03:44:13,215:INFO:Uploading results into container
2023-04-29 03:44:13,220:INFO:Uploading model into container now
2023-04-29 03:44:13,269:INFO:_master_model_container: 26
2023-04-29 03:44:13,269:INFO:_display_container: 10
2023-04-29 03:44:13,272:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 03:44:13,272:INFO:create_model() successfully completed......................................
2023-04-29 03:44:13,626:INFO:Initializing tune_model()
2023-04-29 03:44:13,626:INFO:tune_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>)
2023-04-29 03:44:13,626:INFO:Checking exceptions
2023-04-29 03:44:13,764:INFO:Copying training dataset
2023-04-29 03:44:13,788:INFO:Checking base model
2023-04-29 03:44:13,788:INFO:Base model : Extra Trees Classifier
2023-04-29 03:44:13,804:INFO:Declaring metric variables
2023-04-29 03:44:13,816:INFO:Defining Hyperparameters
2023-04-29 03:44:14,052:INFO:Tuning with n_jobs=-1
2023-04-29 03:44:14,052:INFO:Initializing RandomizedSearchCV
2023-04-29 03:44:23,709:INFO:best_params: {'actual_estimator__n_estimators': 200, 'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 6, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': 'balanced_subsample', 'actual_estimator__bootstrap': False}
2023-04-29 03:44:23,712:INFO:Hyperparameter search completed
2023-04-29 03:44:23,712:INFO:SubProcess create_model() called ==================================
2023-04-29 03:44:23,715:INFO:Initializing create_model()
2023-04-29 03:44:23,715:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EE4A99A00>, model_only=True, return_train_score=False, kwargs={'n_estimators': 200, 'min_samples_split': 7, 'min_samples_leaf': 4, 'min_impurity_decrease': 0, 'max_features': 1.0, 'max_depth': 6, 'criterion': 'gini', 'class_weight': 'balanced_subsample', 'bootstrap': False})
2023-04-29 03:44:23,715:INFO:Checking exceptions
2023-04-29 03:44:23,715:INFO:Importing libraries
2023-04-29 03:44:23,715:INFO:Copying training dataset
2023-04-29 03:44:23,724:INFO:Defining folds
2023-04-29 03:44:23,724:INFO:Declaring metric variables
2023-04-29 03:44:23,733:INFO:Importing untrained model
2023-04-29 03:44:23,736:INFO:Declaring custom model
2023-04-29 03:44:23,748:INFO:Extra Trees Classifier Imported successfully
2023-04-29 03:44:23,775:INFO:Starting cross validation
2023-04-29 03:44:23,779:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:44:25,714:INFO:Calculating mean and std
2023-04-29 03:44:25,720:INFO:Creating metrics dataframe
2023-04-29 03:44:25,744:INFO:Finalizing model
2023-04-29 03:44:26,373:INFO:Uploading results into container
2023-04-29 03:44:26,377:INFO:Uploading model into container now
2023-04-29 03:44:26,379:INFO:_master_model_container: 27
2023-04-29 03:44:26,379:INFO:_display_container: 11
2023-04-29 03:44:26,381:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                     class_weight='balanced_subsample', criterion='gini',
                     max_depth=6, max_features=1.0, max_leaf_nodes=None,
                     max_samples=None, min_impurity_decrease=0,
                     min_samples_leaf=4, min_samples_split=7,
                     min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2023-04-29 03:44:26,381:INFO:create_model() successfully completed......................................
2023-04-29 03:44:26,729:INFO:SubProcess create_model() end ==================================
2023-04-29 03:44:26,729:INFO:choose_better activated
2023-04-29 03:44:26,739:INFO:SubProcess create_model() called ==================================
2023-04-29 03:44:26,741:INFO:Initializing create_model()
2023-04-29 03:44:26,741:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 03:44:26,741:INFO:Checking exceptions
2023-04-29 03:44:26,747:INFO:Importing libraries
2023-04-29 03:44:26,749:INFO:Copying training dataset
2023-04-29 03:44:26,757:INFO:Defining folds
2023-04-29 03:44:26,759:INFO:Declaring metric variables
2023-04-29 03:44:26,759:INFO:Importing untrained model
2023-04-29 03:44:26,762:INFO:Declaring custom model
2023-04-29 03:44:26,764:INFO:Extra Trees Classifier Imported successfully
2023-04-29 03:44:26,764:INFO:Starting cross validation
2023-04-29 03:44:26,768:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 03:44:27,871:INFO:Calculating mean and std
2023-04-29 03:44:27,871:INFO:Creating metrics dataframe
2023-04-29 03:44:27,877:INFO:Finalizing model
2023-04-29 03:44:28,219:INFO:Uploading results into container
2023-04-29 03:44:28,219:INFO:Uploading model into container now
2023-04-29 03:44:28,222:INFO:_master_model_container: 28
2023-04-29 03:44:28,222:INFO:_display_container: 12
2023-04-29 03:44:28,222:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 03:44:28,222:INFO:create_model() successfully completed......................................
2023-04-29 03:44:28,541:INFO:SubProcess create_model() end ==================================
2023-04-29 03:44:28,544:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False) result for Accuracy is 0.9338
2023-04-29 03:44:28,544:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                     class_weight='balanced_subsample', criterion='gini',
                     max_depth=6, max_features=1.0, max_leaf_nodes=None,
                     max_samples=None, min_impurity_decrease=0,
                     min_samples_leaf=4, min_samples_split=7,
                     min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False) result for Accuracy is 0.8679
2023-04-29 03:44:28,547:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False) is best model
2023-04-29 03:44:28,547:INFO:choose_better completed
2023-04-29 03:44:28,547:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 03:44:28,577:INFO:_master_model_container: 28
2023-04-29 03:44:28,580:INFO:_display_container: 11
2023-04-29 03:44:28,580:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 03:44:28,580:INFO:tune_model() successfully completed......................................
2023-04-29 03:44:28,950:INFO:Initializing evaluate_model()
2023-04-29 03:44:28,950:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 03:44:29,008:INFO:Initializing plot_model()
2023-04-29 03:44:29,008:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 03:44:29,008:INFO:Checking exceptions
2023-04-29 03:44:29,054:INFO:Preloading libraries
2023-04-29 03:44:29,075:INFO:Copying training dataset
2023-04-29 03:44:29,078:INFO:Plot type: pipeline
2023-04-29 03:44:29,430:INFO:Visual Rendered Successfully
2023-04-29 03:44:29,761:INFO:plot_model() successfully completed......................................
2023-04-29 03:44:35,639:INFO:Initializing interpret_model()
2023-04-29 03:44:35,640:INFO:interpret_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>)
2023-04-29 03:44:35,640:INFO:Checking exceptions
2023-04-29 03:44:35,640:INFO:Soft dependency imported: shap: 0.41.0
2023-04-29 03:44:35,704:INFO:plot type: summary
2023-04-29 03:44:35,704:INFO:Creating TreeExplainer
2023-04-29 03:44:35,713:INFO:Compiling shap values
2023-04-29 03:44:36,240:INFO:Visual Rendered Successfully
2023-04-29 03:44:36,241:INFO:interpret_model() successfully completed......................................
2023-04-29 03:44:36,465:INFO:Initializing interpret_model()
2023-04-29 03:44:36,466:INFO:interpret_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=32, plot=reason, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>)
2023-04-29 03:44:36,466:INFO:Checking exceptions
2023-04-29 03:44:36,466:INFO:Soft dependency imported: shap: 0.41.0
2023-04-29 03:44:36,538:INFO:plot type: reason
2023-04-29 03:44:36,538:INFO:model type detected: type 1
2023-04-29 03:44:36,539:INFO:Creating TreeExplainer
2023-04-29 03:44:36,547:INFO:Compiling shap values
2023-04-29 03:44:36,548:INFO:model type detected: Unknown
2023-04-29 03:44:36,566:INFO:Visual Rendered Successfully
2023-04-29 03:44:36,566:INFO:interpret_model() successfully completed......................................
2023-04-29 03:44:36,953:INFO:Initializing interpret_model()
2023-04-29 03:44:36,954:INFO:interpret_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=reason, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>)
2023-04-29 03:44:36,954:INFO:Checking exceptions
2023-04-29 03:44:36,954:INFO:Soft dependency imported: shap: 0.41.0
2023-04-29 03:44:37,035:INFO:plot type: reason
2023-04-29 03:44:37,036:INFO:model type detected: type 1
2023-04-29 03:44:37,036:INFO:Creating TreeExplainer
2023-04-29 03:44:37,045:INFO:Compiling shap values
2023-04-29 03:44:37,045:WARNING:Observation set to None. Model agnostic plot will be rendered.
2023-04-29 03:44:37,141:INFO:Visual Rendered Successfully
2023-04-29 03:44:37,142:INFO:interpret_model() successfully completed......................................
2023-04-29 16:37:06,111:INFO:Initializing plot_model()
2023-04-29 16:37:06,119:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 16:37:06,121:INFO:Checking exceptions
2023-04-29 16:37:06,306:INFO:Preloading libraries
2023-04-29 16:37:06,342:INFO:Copying training dataset
2023-04-29 16:37:06,342:INFO:Plot type: parameter
2023-04-29 16:37:06,382:INFO:Visual Rendered Successfully
2023-04-29 16:37:07,091:INFO:plot_model() successfully completed......................................
2023-04-29 16:37:10,292:INFO:Initializing plot_model()
2023-04-29 16:37:10,292:INFO:plot_model(plot=pr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 16:37:10,292:INFO:Checking exceptions
2023-04-29 16:37:10,331:INFO:Preloading libraries
2023-04-29 16:37:10,356:INFO:Copying training dataset
2023-04-29 16:37:10,356:INFO:Plot type: pr
2023-04-29 16:37:10,611:INFO:Fitting Model
2023-04-29 16:37:17,541:INFO:Scoring test/hold-out set
2023-04-29 16:37:18,651:INFO:Visual Rendered Successfully
2023-04-29 16:37:19,021:INFO:plot_model() successfully completed......................................
2023-04-29 16:37:19,061:INFO:Initializing plot_model()
2023-04-29 16:37:19,061:INFO:plot_model(plot=vc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 16:37:19,061:INFO:Checking exceptions
2023-04-29 16:37:19,137:INFO:Preloading libraries
2023-04-29 16:37:19,161:INFO:Copying training dataset
2023-04-29 16:37:19,170:INFO:Plot type: vc
2023-04-29 16:37:19,171:INFO:Determining param_name
2023-04-29 16:37:19,171:INFO:param_name: max_depth
2023-04-29 16:37:19,502:INFO:Fitting Model
2023-04-29 16:37:28,245:INFO:Visual Rendered Successfully
2023-04-29 16:37:28,437:INFO:plot_model() successfully completed......................................
2023-04-29 16:37:28,471:INFO:Initializing plot_model()
2023-04-29 16:37:28,471:INFO:plot_model(plot=dimension, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 16:37:28,471:INFO:Checking exceptions
2023-04-29 16:37:28,531:INFO:Preloading libraries
2023-04-29 16:37:28,551:INFO:Copying training dataset
2023-04-29 16:37:28,551:INFO:Plot type: dimension
2023-04-29 16:37:28,771:INFO:Fitting StandardScaler()
2023-04-29 16:37:28,841:INFO:Fitting PCA()
2023-04-29 16:37:29,105:INFO:Fitting & Transforming Model
2023-04-29 16:37:29,131:WARNING:invalid value encountered in true_divide

2023-04-29 16:37:29,453:INFO:Visual Rendered Successfully
2023-04-29 16:37:29,628:INFO:plot_model() successfully completed......................................
2023-04-29 16:37:29,641:INFO:Initializing plot_model()
2023-04-29 16:37:29,641:INFO:plot_model(plot=gain, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 16:37:29,644:INFO:Checking exceptions
2023-04-29 16:37:29,681:INFO:Preloading libraries
2023-04-29 16:37:29,701:INFO:Copying training dataset
2023-04-29 16:37:29,707:INFO:Plot type: gain
2023-04-29 16:37:29,707:INFO:Generating predictions / predict_proba on X_test
2023-04-29 16:37:32,840:INFO:Initializing plot_model()
2023-04-29 16:37:32,840:INFO:plot_model(plot=tree, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 16:37:32,840:INFO:Checking exceptions
2023-04-29 16:37:32,881:INFO:Preloading libraries
2023-04-29 16:37:32,903:INFO:Copying training dataset
2023-04-29 16:37:32,903:INFO:Plot type: tree
2023-04-29 16:37:34,927:INFO:Plotting decision trees
2023-04-29 16:50:03,375:INFO:Initializing plot_model()
2023-04-29 16:50:03,375:INFO:plot_model(plot=gain, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 16:50:03,375:INFO:Checking exceptions
2023-04-29 16:50:03,415:INFO:Preloading libraries
2023-04-29 16:50:03,435:INFO:Copying training dataset
2023-04-29 16:50:03,435:INFO:Plot type: gain
2023-04-29 16:50:03,436:INFO:Generating predictions / predict_proba on X_test
2023-04-29 16:50:07,555:INFO:Initializing plot_model()
2023-04-29 16:50:07,555:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 16:50:07,555:INFO:Checking exceptions
2023-04-29 16:50:07,595:INFO:Preloading libraries
2023-04-29 16:50:07,618:INFO:Copying training dataset
2023-04-29 16:50:07,618:INFO:Plot type: pipeline
2023-04-29 16:50:07,865:INFO:Visual Rendered Successfully
2023-04-29 16:50:08,736:INFO:plot_model() successfully completed......................................
2023-04-29 22:24:44,989:INFO:Initializing plot_model()
2023-04-29 22:24:44,994:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:24:44,994:INFO:Checking exceptions
2023-04-29 22:24:45,040:INFO:Preloading libraries
2023-04-29 22:24:45,045:INFO:Copying training dataset
2023-04-29 22:24:45,045:INFO:Plot type: error
2023-04-29 22:24:45,343:INFO:Fitting Model
2023-04-29 22:24:45,346:WARNING:X does not have valid feature names, but RidgeClassifier was fitted with feature names

2023-04-29 22:24:45,353:INFO:Scoring test/hold-out set
2023-04-29 22:24:46,088:INFO:Visual Rendered Successfully
2023-04-29 22:24:46,938:INFO:plot_model() successfully completed......................................
2023-04-29 22:25:03,774:INFO:Initializing plot_model()
2023-04-29 22:25:03,775:INFO:plot_model(plot=calibration, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:25:03,775:INFO:Checking exceptions
2023-04-29 22:25:05,254:INFO:Initializing plot_model()
2023-04-29 22:25:05,255:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:25:05,255:INFO:Checking exceptions
2023-04-29 22:25:05,259:INFO:Preloading libraries
2023-04-29 22:25:05,260:INFO:Copying training dataset
2023-04-29 22:25:05,260:INFO:Plot type: boundary
2023-04-29 22:25:05,403:INFO:Fitting StandardScaler()
2023-04-29 22:25:05,416:INFO:Fitting PCA()
2023-04-29 22:25:05,531:INFO:Fitting Model
2023-04-29 22:25:05,542:WARNING:'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.

2023-04-29 22:25:08,499:INFO:Visual Rendered Successfully
2023-04-29 22:25:08,870:INFO:plot_model() successfully completed......................................
2023-04-29 22:25:08,886:INFO:Initializing plot_model()
2023-04-29 22:25:08,886:INFO:plot_model(plot=feature_all, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:25:08,887:INFO:Checking exceptions
2023-04-29 22:25:08,891:INFO:Preloading libraries
2023-04-29 22:25:08,892:INFO:Copying training dataset
2023-04-29 22:25:08,892:INFO:Plot type: feature_all
2023-04-29 22:25:09,783:INFO:Visual Rendered Successfully
2023-04-29 22:25:10,231:INFO:plot_model() successfully completed......................................
2023-04-29 22:27:56,935:INFO:Initializing create_model()
2023-04-29 22:27:56,935:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=qda, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 22:27:56,935:INFO:Checking exceptions
2023-04-29 22:27:56,991:INFO:Importing libraries
2023-04-29 22:27:56,991:INFO:Copying training dataset
2023-04-29 22:27:56,996:INFO:Defining folds
2023-04-29 22:27:56,996:INFO:Declaring metric variables
2023-04-29 22:27:57,002:INFO:Importing untrained model
2023-04-29 22:27:57,009:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 22:27:57,023:INFO:Starting cross validation
2023-04-29 22:27:57,026:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 22:28:10,062:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:10,063:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:10,063:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:10,063:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:10,292:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:10,319:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:10,366:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:10,379:INFO:Calculating mean and std
2023-04-29 22:28:10,386:INFO:Creating metrics dataframe
2023-04-29 22:28:10,407:INFO:Finalizing model
2023-04-29 22:28:10,539:WARNING:Variables are collinear

2023-04-29 22:28:10,557:INFO:Uploading results into container
2023-04-29 22:28:10,559:INFO:Uploading model into container now
2023-04-29 22:28:10,591:INFO:_master_model_container: 29
2023-04-29 22:28:10,591:INFO:_display_container: 12
2023-04-29 22:28:10,593:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 22:28:10,594:INFO:create_model() successfully completed......................................
2023-04-29 22:28:13,167:INFO:Initializing tune_model()
2023-04-29 22:28:13,167:INFO:tune_model(estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>)
2023-04-29 22:28:13,168:INFO:Checking exceptions
2023-04-29 22:28:13,274:INFO:Copying training dataset
2023-04-29 22:28:13,289:INFO:Checking base model
2023-04-29 22:28:13,289:INFO:Base model : Quadratic Discriminant Analysis
2023-04-29 22:28:13,310:INFO:Declaring metric variables
2023-04-29 22:28:13,323:INFO:Defining Hyperparameters
2023-04-29 22:28:13,750:INFO:Tuning with n_jobs=-1
2023-04-29 22:28:13,750:INFO:Initializing RandomizedSearchCV
2023-04-29 22:28:14,052:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,081:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,174:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,199:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,223:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,257:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,266:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,208:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,292:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,294:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,587:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,735:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,780:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,785:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,785:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,821:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,849:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,852:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,894:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:14,914:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,087:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,119:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,132:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,177:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,179:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,181:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,189:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,196:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,213:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,238:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,324:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,344:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,365:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,378:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,378:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,419:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,421:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,434:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,458:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,485:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,547:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,559:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,563:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,569:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,611:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,628:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,633:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,661:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,678:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,683:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,769:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,780:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,794:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,795:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,815:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,831:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,869:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,882:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,913:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,935:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:15,960:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,007:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,008:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,022:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,032:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,038:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,100:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,125:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,129:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,141:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,191:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,201:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,221:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,225:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,259:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,273:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,311:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,317:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,330:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,333:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,414:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,415:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,417:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,482:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,483:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,512:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,517:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,531:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,550:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,554:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,593:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,617:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,620:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,682:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,700:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,702:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,713:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:16,724:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,335:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,472:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,547:INFO:best_params: {'actual_estimator__reg_param': 0.17}
2023-04-29 22:28:20,550:INFO:Hyperparameter search completed
2023-04-29 22:28:20,551:INFO:SubProcess create_model() called ==================================
2023-04-29 22:28:20,552:INFO:Initializing create_model()
2023-04-29 22:28:20,553:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027EEA9002E0>, model_only=True, return_train_score=False, kwargs={'reg_param': 0.17})
2023-04-29 22:28:20,553:INFO:Checking exceptions
2023-04-29 22:28:20,554:INFO:Importing libraries
2023-04-29 22:28:20,554:INFO:Copying training dataset
2023-04-29 22:28:20,563:INFO:Defining folds
2023-04-29 22:28:20,563:INFO:Declaring metric variables
2023-04-29 22:28:20,571:INFO:Importing untrained model
2023-04-29 22:28:20,572:INFO:Declaring custom model
2023-04-29 22:28:20,589:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 22:28:20,608:INFO:Starting cross validation
2023-04-29 22:28:20,612:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 22:28:20,761:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,765:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,838:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,870:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,911:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,945:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,956:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,962:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:20,981:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:21,062:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:21,121:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:21,162:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:21,173:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:21,190:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:21,208:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:21,287:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:21,295:INFO:Calculating mean and std
2023-04-29 22:28:21,300:INFO:Creating metrics dataframe
2023-04-29 22:28:21,314:INFO:Finalizing model
2023-04-29 22:28:21,456:WARNING:Variables are collinear

2023-04-29 22:28:21,502:INFO:Uploading results into container
2023-04-29 22:28:21,507:INFO:Uploading model into container now
2023-04-29 22:28:21,510:INFO:_master_model_container: 30
2023-04-29 22:28:21,510:INFO:_display_container: 13
2023-04-29 22:28:21,511:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.17,
                              store_covariance=False, tol=0.0001)
2023-04-29 22:28:21,512:INFO:create_model() successfully completed......................................
2023-04-29 22:28:21,921:INFO:SubProcess create_model() end ==================================
2023-04-29 22:28:21,922:INFO:choose_better activated
2023-04-29 22:28:21,935:INFO:SubProcess create_model() called ==================================
2023-04-29 22:28:21,938:INFO:Initializing create_model()
2023-04-29 22:28:21,939:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 22:28:21,939:INFO:Checking exceptions
2023-04-29 22:28:21,946:INFO:Importing libraries
2023-04-29 22:28:21,946:INFO:Copying training dataset
2023-04-29 22:28:21,959:INFO:Defining folds
2023-04-29 22:28:21,960:INFO:Declaring metric variables
2023-04-29 22:28:21,961:INFO:Importing untrained model
2023-04-29 22:28:21,962:INFO:Declaring custom model
2023-04-29 22:28:21,963:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 22:28:21,964:INFO:Starting cross validation
2023-04-29 22:28:21,969:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 22:28:22,199:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:22,213:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:22,243:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:22,246:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:22,254:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:22,266:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:22,291:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:22,320:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:22,331:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:22,332:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 22:28:22,426:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:22,480:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:22,530:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 22:28:22,539:INFO:Calculating mean and std
2023-04-29 22:28:22,540:INFO:Creating metrics dataframe
2023-04-29 22:28:22,545:INFO:Finalizing model
2023-04-29 22:28:22,760:WARNING:Variables are collinear

2023-04-29 22:28:22,767:INFO:Uploading results into container
2023-04-29 22:28:22,770:INFO:Uploading model into container now
2023-04-29 22:28:22,771:INFO:_master_model_container: 31
2023-04-29 22:28:22,771:INFO:_display_container: 14
2023-04-29 22:28:22,772:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 22:28:22,772:INFO:create_model() successfully completed......................................
2023-04-29 22:28:22,997:INFO:SubProcess create_model() end ==================================
2023-04-29 22:28:22,999:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001) result for Accuracy is 0.8154
2023-04-29 22:28:23,000:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.17,
                              store_covariance=False, tol=0.0001) result for Accuracy is 0.7096
2023-04-29 22:28:23,001:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001) is best model
2023-04-29 22:28:23,001:INFO:choose_better completed
2023-04-29 22:28:23,001:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 22:28:23,031:INFO:_master_model_container: 31
2023-04-29 22:28:23,032:INFO:_display_container: 13
2023-04-29 22:28:23,033:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 22:28:23,033:INFO:tune_model() successfully completed......................................
2023-04-29 22:28:23,294:INFO:Initializing evaluate_model()
2023-04-29 22:28:23,294:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 22:28:23,333:INFO:Initializing plot_model()
2023-04-29 22:28:23,334:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 22:28:23,334:INFO:Checking exceptions
2023-04-29 22:28:23,338:INFO:Preloading libraries
2023-04-29 22:28:23,339:INFO:Copying training dataset
2023-04-29 22:28:23,340:INFO:Plot type: pipeline
2023-04-29 22:28:23,665:INFO:Visual Rendered Successfully
2023-04-29 22:28:23,978:INFO:plot_model() successfully completed......................................
2023-04-29 22:32:13,201:INFO:Initializing plot_model()
2023-04-29 22:32:13,201:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:32:13,201:INFO:Checking exceptions
2023-04-29 22:32:13,209:INFO:Preloading libraries
2023-04-29 22:32:13,210:INFO:Copying training dataset
2023-04-29 22:32:13,210:INFO:Plot type: boundary
2023-04-29 22:32:13,354:INFO:Fitting StandardScaler()
2023-04-29 22:32:13,363:INFO:Fitting PCA()
2023-04-29 22:32:13,461:INFO:Fitting Model
2023-04-29 22:32:13,463:WARNING:'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.

2023-04-29 22:32:16,686:INFO:Visual Rendered Successfully
2023-04-29 22:32:17,294:INFO:plot_model() successfully completed......................................
2023-04-29 22:32:50,306:INFO:Initializing plot_model()
2023-04-29 22:32:50,306:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GaussianNB(priors=None, var_smoothing=1e-09), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:32:50,306:INFO:Checking exceptions
2023-04-29 22:32:50,309:INFO:Preloading libraries
2023-04-29 22:32:50,312:INFO:Copying training dataset
2023-04-29 22:32:50,312:INFO:Plot type: boundary
2023-04-29 22:32:50,403:INFO:Fitting StandardScaler()
2023-04-29 22:32:50,412:INFO:Fitting PCA()
2023-04-29 22:32:50,499:INFO:Fitting Model
2023-04-29 22:32:53,193:INFO:Visual Rendered Successfully
2023-04-29 22:32:53,615:INFO:plot_model() successfully completed......................................
2023-04-29 22:33:10,596:INFO:Initializing plot_model()
2023-04-29 22:33:10,596:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:33:10,596:INFO:Checking exceptions
2023-04-29 22:33:10,598:INFO:Preloading libraries
2023-04-29 22:33:10,598:INFO:Copying training dataset
2023-04-29 22:33:10,599:INFO:Plot type: boundary
2023-04-29 22:33:10,696:INFO:Fitting StandardScaler()
2023-04-29 22:33:10,705:INFO:Fitting PCA()
2023-04-29 22:33:10,788:INFO:Fitting Model
2023-04-29 22:33:13,767:INFO:Visual Rendered Successfully
2023-04-29 22:33:14,341:INFO:plot_model() successfully completed......................................
2023-04-29 22:34:04,233:INFO:Initializing plot_model()
2023-04-29 22:34:04,233:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:34:04,233:INFO:Checking exceptions
2023-04-29 22:34:04,237:INFO:Preloading libraries
2023-04-29 22:34:04,238:INFO:Copying training dataset
2023-04-29 22:34:04,238:INFO:Plot type: error
2023-04-29 22:34:04,421:INFO:Fitting Model
2023-04-29 22:34:04,421:WARNING:X does not have valid feature names, but LogisticRegression was fitted with feature names

2023-04-29 22:34:04,421:INFO:Scoring test/hold-out set
2023-04-29 22:34:04,990:INFO:Visual Rendered Successfully
2023-04-29 22:34:05,231:INFO:plot_model() successfully completed......................................
2023-04-29 22:38:38,488:INFO:Initializing plot_model()
2023-04-29 22:38:38,488:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:38:38,488:INFO:Checking exceptions
2023-04-29 22:38:38,491:INFO:Preloading libraries
2023-04-29 22:38:38,492:INFO:Copying training dataset
2023-04-29 22:38:38,492:INFO:Plot type: boundary
2023-04-29 22:38:38,579:INFO:Fitting StandardScaler()
2023-04-29 22:38:38,588:INFO:Fitting PCA()
2023-04-29 22:38:38,672:INFO:Fitting Model
2023-04-29 22:38:41,332:INFO:Visual Rendered Successfully
2023-04-29 22:38:41,808:INFO:plot_model() successfully completed......................................
2023-04-29 22:39:01,260:INFO:Initializing plot_model()
2023-04-29 22:39:01,261:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:39:01,261:INFO:Checking exceptions
2023-04-29 22:39:01,264:INFO:Preloading libraries
2023-04-29 22:39:01,264:INFO:Copying training dataset
2023-04-29 22:39:01,264:INFO:Plot type: error
2023-04-29 22:39:01,404:INFO:Fitting Model
2023-04-29 22:39:01,404:WARNING:X does not have valid feature names, but LogisticRegression was fitted with feature names

2023-04-29 22:39:01,405:INFO:Scoring test/hold-out set
2023-04-29 22:39:01,962:INFO:Visual Rendered Successfully
2023-04-29 22:39:02,184:INFO:plot_model() successfully completed......................................
2023-04-29 22:39:15,377:INFO:Initializing plot_model()
2023-04-29 22:39:15,378:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GaussianNB(priors=None, var_smoothing=1e-09), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:39:15,378:INFO:Checking exceptions
2023-04-29 22:39:15,383:INFO:Preloading libraries
2023-04-29 22:39:15,385:INFO:Copying training dataset
2023-04-29 22:39:15,385:INFO:Plot type: error
2023-04-29 22:39:15,630:INFO:Fitting Model
2023-04-29 22:39:15,631:WARNING:X does not have valid feature names, but GaussianNB was fitted with feature names

2023-04-29 22:39:15,631:INFO:Scoring test/hold-out set
2023-04-29 22:39:16,288:INFO:Visual Rendered Successfully
2023-04-29 22:39:16,674:INFO:plot_model() successfully completed......................................
2023-04-29 22:47:25,918:INFO:Initializing plot_model()
2023-04-29 22:47:25,919:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EDC0B2430>, system=True)
2023-04-29 22:47:25,920:INFO:Checking exceptions
2023-04-29 22:47:25,928:INFO:Preloading libraries
2023-04-29 22:47:25,928:INFO:Copying training dataset
2023-04-29 22:47:25,929:INFO:Plot type: error
2023-04-29 22:47:26,115:INFO:Fitting Model
2023-04-29 22:47:26,115:WARNING:X does not have valid feature names, but RidgeClassifier was fitted with feature names

2023-04-29 22:47:26,116:INFO:Scoring test/hold-out set
2023-04-29 22:47:26,716:INFO:Visual Rendered Successfully
2023-04-29 22:47:26,988:INFO:plot_model() successfully completed......................................
2023-04-29 22:51:15,344:INFO:Initializing plot_model()
2023-04-29 22:51:15,345:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 22:51:15,346:INFO:Checking exceptions
2023-04-29 22:51:15,378:INFO:Preloading libraries
2023-04-29 22:51:15,444:INFO:Copying training dataset
2023-04-29 22:51:15,444:INFO:Plot type: boundary
2023-04-29 22:51:15,615:INFO:Fitting StandardScaler()
2023-04-29 22:51:15,625:INFO:Fitting PCA()
2023-04-29 22:51:15,794:INFO:Fitting Model
2023-04-29 22:51:20,901:INFO:Visual Rendered Successfully
2023-04-29 22:51:21,497:INFO:plot_model() successfully completed......................................
2023-04-29 22:51:23,510:INFO:Initializing plot_model()
2023-04-29 22:51:23,510:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 22:51:23,510:INFO:Checking exceptions
2023-04-29 22:51:23,515:INFO:Preloading libraries
2023-04-29 22:51:23,516:INFO:Copying training dataset
2023-04-29 22:51:23,517:INFO:Plot type: boundary
2023-04-29 22:51:23,703:INFO:Fitting StandardScaler()
2023-04-29 22:51:23,712:INFO:Fitting PCA()
2023-04-29 22:51:23,882:INFO:Fitting Model
2023-04-29 22:51:26,579:INFO:Visual Rendered Successfully
2023-04-29 22:51:26,963:INFO:plot_model() successfully completed......................................
2023-04-29 22:51:41,466:INFO:Initializing plot_model()
2023-04-29 22:51:41,466:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GaussianNB(priors=None, var_smoothing=1e-09), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 22:51:41,466:INFO:Checking exceptions
2023-04-29 22:51:41,469:INFO:Preloading libraries
2023-04-29 22:51:41,469:INFO:Copying training dataset
2023-04-29 22:51:41,469:INFO:Plot type: boundary
2023-04-29 22:51:41,580:INFO:Fitting StandardScaler()
2023-04-29 22:51:41,587:INFO:Fitting PCA()
2023-04-29 22:51:41,715:INFO:Fitting Model
2023-04-29 22:51:44,417:INFO:Visual Rendered Successfully
2023-04-29 22:51:44,799:INFO:plot_model() successfully completed......................................
2023-04-29 22:51:55,818:INFO:Initializing plot_model()
2023-04-29 22:51:55,819:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 22:51:55,819:INFO:Checking exceptions
2023-04-29 22:51:55,822:INFO:Preloading libraries
2023-04-29 22:51:55,823:INFO:Copying training dataset
2023-04-29 22:51:55,823:INFO:Plot type: boundary
2023-04-29 22:51:55,964:INFO:Fitting StandardScaler()
2023-04-29 22:51:55,972:INFO:Fitting PCA()
2023-04-29 22:51:56,093:INFO:Fitting Model
2023-04-29 22:51:58,679:INFO:Visual Rendered Successfully
2023-04-29 22:51:59,080:INFO:plot_model() successfully completed......................................
2023-04-29 22:52:25,765:INFO:Initializing plot_model()
2023-04-29 22:52:25,766:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 22:52:25,766:INFO:Checking exceptions
2023-04-29 22:52:25,769:INFO:Preloading libraries
2023-04-29 22:52:25,769:INFO:Copying training dataset
2023-04-29 22:52:25,770:INFO:Plot type: error
2023-04-29 22:52:26,030:INFO:Fitting Model
2023-04-29 22:52:26,030:WARNING:X does not have valid feature names, but LogisticRegression was fitted with feature names

2023-04-29 22:52:26,032:INFO:Scoring test/hold-out set
2023-04-29 22:52:26,622:INFO:Visual Rendered Successfully
2023-04-29 22:52:26,913:INFO:plot_model() successfully completed......................................
2023-04-29 22:52:39,996:INFO:Initializing plot_model()
2023-04-29 22:52:39,997:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GaussianNB(priors=None, var_smoothing=1e-09), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 22:52:39,997:INFO:Checking exceptions
2023-04-29 22:52:40,000:INFO:Preloading libraries
2023-04-29 22:52:40,003:INFO:Copying training dataset
2023-04-29 22:52:40,003:INFO:Plot type: error
2023-04-29 22:52:40,262:INFO:Fitting Model
2023-04-29 22:52:40,262:WARNING:X does not have valid feature names, but GaussianNB was fitted with feature names

2023-04-29 22:52:40,263:INFO:Scoring test/hold-out set
2023-04-29 22:52:40,893:INFO:Visual Rendered Successfully
2023-04-29 22:52:41,147:INFO:plot_model() successfully completed......................................
2023-04-29 22:52:59,101:INFO:Initializing plot_model()
2023-04-29 22:52:59,101:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 22:52:59,101:INFO:Checking exceptions
2023-04-29 22:52:59,105:INFO:Preloading libraries
2023-04-29 22:52:59,105:INFO:Copying training dataset
2023-04-29 22:52:59,106:INFO:Plot type: parameter
2023-04-29 22:52:59,112:INFO:Visual Rendered Successfully
2023-04-29 22:52:59,360:INFO:plot_model() successfully completed......................................
2023-04-29 22:53:00,064:INFO:Initializing plot_model()
2023-04-29 22:53:00,064:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027EE4A999A0>, system=True)
2023-04-29 22:53:00,064:INFO:Checking exceptions
2023-04-29 22:53:00,069:INFO:Preloading libraries
2023-04-29 22:53:00,069:INFO:Copying training dataset
2023-04-29 22:53:00,069:INFO:Plot type: error
2023-04-29 22:53:00,345:INFO:Fitting Model
2023-04-29 22:53:00,345:WARNING:X does not have valid feature names, but QuadraticDiscriminantAnalysis was fitted with feature names

2023-04-29 22:53:00,346:INFO:Scoring test/hold-out set
2023-04-29 22:53:00,920:INFO:Visual Rendered Successfully
2023-04-29 22:53:01,170:INFO:plot_model() successfully completed......................................
2023-04-29 23:46:45,048:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_7a7fb43a0aa246c7928fd15fad50f79e

2023-04-29 23:46:45,056:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_200aa2481cc7448b89e4828eb9f4c067_fe89c53528a3444d9d9d5212c57b4dac

2023-04-29 23:46:45,057:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_67ca9ae11e7647beb3b7a17843e9811d

2023-04-29 23:46:45,057:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_58c1dcf5a10946ae91406670b7b9f86c_ddf576adaebc4ce6bc5d6f242bff3643

2023-04-29 23:46:45,058:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_d1eb54d098fa46cb8d06ee5e9a17e76d

2023-04-29 23:46:45,058:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_0dccb302239f472a8e4945c88c3388c3_e8d39e11c20d4c83996adb6754cc5814

2023-04-29 23:46:45,059:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_c2b2f590a94e47189ed48e82e5024945

2023-04-29 23:46:45,059:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_c99bfea233ce4924a684eb9f03a7bfc9_139a94aad285411f9b587b81c991dcf4

2023-04-29 23:46:45,060:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_34fc9d949de74478bdda6b1d4d1932f6

2023-04-29 23:46:45,060:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_be34266d013c49ecae54e112b101dbb7_10344dc6eda8446aa576b2a2d8a987e1

2023-04-29 23:46:45,061:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_9c1545d1f5ef4d5d84883b1ca99824cc

2023-04-29 23:46:45,061:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_e6b080a86f5d49a0ab7d3edbd9bd3e6f_275fd4598ef040c7888e4cc9f1392741

2023-04-29 23:46:45,061:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_ef33a04ce28c416abb9356363637247c

2023-04-29 23:46:45,062:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_6f6f8a21ca2445b8957719984205aa2c_2499a3ea87af4108a47ad18c44f7e5e9

2023-04-29 23:46:45,062:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_66e9239e0dcd402892dfd02f04e7ed32

2023-04-29 23:46:45,062:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_4eb5c06b2069447c81ec993a804be0b2_b445cc17df4f41cf8f96c6e1f3cbb94a

2023-04-29 23:46:45,063:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_0f66a24dfd904a46b0b7d72f9c12fada

2023-04-29 23:46:45,063:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_0cf7963301784256825977670c630eb7_ecd027066ec04bddbd9a3eeda6c04cce

2023-04-29 23:46:45,064:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_897ba4b19da8494ea0761c914fc9b4d0

2023-04-29 23:46:45,064:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_965e2548cad7477680affb031ade5115_e8d149cb7dc14790a81f20425ee06ebf

2023-04-29 23:46:45,065:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_d9512b1f8a2a4808b29d714a78bae6e6

2023-04-29 23:46:45,065:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_67e6a957c88441088aeee47c6fd63e89_5b71bd289e1e4259b61d5a0e5b8c6b44

2023-04-29 23:46:45,067:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_aafbad59f80a46eba5d729437ae0914b

2023-04-29 23:46:45,068:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_f3a34d068c2d4514bfc8188c915206ec_9a396048c80c451abcd90072c14e781e

2023-04-29 23:46:45,068:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_5eadc107244c404b8af9ef4fdb3b25a0

2023-04-29 23:46:45,068:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_572d6449b4fe402db455cb6ae140451a_a812b6bf6a9348c9896cc2b240f4e72e

2023-04-29 23:46:45,069:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_86190acd87c04d4daaf97cbeb8b623a9

2023-04-29 23:46:45,069:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_3bd8359becd54f7da5b8c38c5c0e10b3_e46d55a9c7a04e37b715de38bea293da

2023-04-29 23:46:45,070:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_7e2c39309c764397855b1028cdf9ad81

2023-04-29 23:46:45,070:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_137c9be375c946079623af6a7c6137f9_21fcf7c20eb74c958a28d08cd70f4044

2023-04-29 23:46:45,072:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_e5005ecd39c14a5fa0f00c3382f6e980

2023-04-29 23:46:45,072:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_823b2db7ebb247b48fc3f91717888ae2_2c20966ef677459bb890b0e5578c6c4d

2023-04-29 23:46:45,073:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_69c9322099cb499a87608e75e1c2b993

2023-04-29 23:46:45,073:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_c58f9589be3944f5bbedea4a18905020_4291a4c386fa4c02b5fdd8c951dc64d2

2023-04-29 23:46:45,073:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_1c57b020ff4143d69962d3de0ec2d040

2023-04-29 23:46:45,074:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_8c44d834344a4983a5f3828c0c9ca15a_062c8f880da64eeaa3050644d6bd8db8

2023-04-29 23:46:45,074:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_25deafa5ec0b4f208448326178793ede

2023-04-29 23:46:45,074:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_96cc3ea64ce94d358bb5b7fc51a445b5_63f3c2d0e3f946ca9d2be833ea7f9e1b

2023-04-29 23:46:45,075:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_14273d622f044ac7839a3247a31b816b

2023-04-29 23:46:45,075:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_1f1608c9b47c4fd78cdef2572324bcde_8956cc312fbe4de596c4adeb27c421db

2023-04-29 23:46:45,076:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_71ffc46de01e4b2b84969b4e7435ae31

2023-04-29 23:46:45,076:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_2cd171ba6938457cb99344efd83380ef_4e426815236b4586ab84a03b744fa7ff

2023-04-29 23:46:45,077:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_710d5a4aa38947078f8d6836988bd87e

2023-04-29 23:46:45,077:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_1a78745047354e06b268d7ccf8cf78e1_01928d81fdd14cd985c3261a66a0813a

2023-04-29 23:46:45,077:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_892f855845f94d23b7f3cc0934cb8507

2023-04-29 23:46:45,078:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_1c9ccc13a002489d93c9fef0c7807e5e_e3a95f59007b46c0a4f8a34f59846927

2023-04-29 23:46:45,078:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_c29ac7c87009478384d0b6c2afd05758

2023-04-29 23:46:45,078:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_1c5c229387b441d8a3ea2c266c791aa5_af53a9c654dd40dd80ae5e2d7eab6c44

2023-04-29 23:46:45,078:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_13237c17697447a1ac665d7467b4020b

2023-04-29 23:46:45,079:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_fed86e702c7f4bdf88b2312e14efe60f_9a26f7c4672e4e1db9d2a83c718212f6

2023-04-29 23:46:45,079:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_ded2c46df0d64de39980e29e67bd57a5

2023-04-29 23:46:45,079:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_2e1c1e4ecbed48eba0bcc3e122d5b3d3_82c966f895d34ab2ba410f3ad3e1a83c

2023-04-29 23:46:45,080:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_42232498911443d588cecd8f8368a7b1

2023-04-29 23:46:45,080:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_a136cd602a1a40e8b97a9175b7204925_3b9d467335354f41bbdb5f7a6613a472

2023-04-29 23:46:45,081:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_06180613d04c43db925b1299725b4958

2023-04-29 23:46:45,081:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_9d13fa98968c4f108a5a80e476d36004_4fec754291094a4b8f2ab1004a9d5180

2023-04-29 23:46:45,082:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_3d059556912649ae8f47d5cae80ff842

2023-04-29 23:46:45,082:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_a14a5c93a3d542e7888560bcc8e4d7f8_b45d2362f76e4f06a1df97a02920ae37

2023-04-29 23:46:45,082:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_1db614010a724ff6bd9b8299ce63183a

2023-04-29 23:46:45,083:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_038a4be84f4d4512b800e6fd2ed16ad7_1712752937f346f1a1f678ea44c7c62a

2023-04-29 23:46:45,083:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_fba6d48ce7fe4154b33ce564d97da503

2023-04-29 23:46:45,083:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_f21fd9c9a94042df9df3aadcdf1d3268_b762fb3f9e2a4a86ae0d3acfb8f30a1a

2023-04-29 23:46:45,084:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_b54714aa348d4bb697c4b7ce672cc561

2023-04-29 23:46:45,084:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_330cb42e6176418eac544630f0951f32_822f407cfaf64b838bb0fa2529839944

2023-04-29 23:46:45,085:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_e1b2ccf89ebc4769a8b4d4515eb79cdb

2023-04-29 23:46:45,085:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_42833a28c47a4f899f688c52bd9f9252_c1636ba3103e4b43be5a0b332b3c7987

2023-04-29 23:46:45,086:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_cc4390d94a7546e495825b23610cafa6

2023-04-29 23:46:45,087:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_5572e722313f4e1ea5527c7348679cf2_d16b6cb3fda046f7b9638c9f4bb060a8

2023-04-29 23:46:45,088:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_5f1ce2413b724aeb8f4761cf4a6d18cd

2023-04-29 23:46:45,088:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_1ce4bc1fba91486db8e983c58715c1ba_78807830a6e144958e55e3a1faa42033

2023-04-29 23:46:45,089:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_5d2dc3c51c2e47df9b4f6e20118c85cb

2023-04-29 23:46:45,089:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_499d2e2b0d3e45abbad472a063e06219_6ecbd2a477e7422da9a85c71212da1ae

2023-04-29 23:46:45,090:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_b026bb54f06843efbd3a8ece60ffc63c

2023-04-29 23:46:45,090:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_4f2f7381395a47e0ab0715dac0897e7d_6b73d516b9ba460f99b745c37a38e162

2023-04-29 23:46:45,091:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_4a335da04c5745ccb6d212f1b8e1dd88

2023-04-29 23:46:45,091:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_42dfec7ba92a43aea9e51c9c836356fc_685d21b8cbe94d6ba4ee468790d68b5d

2023-04-29 23:46:45,091:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_a702a4a043ed4db1b54ac102f999c2b8

2023-04-29 23:46:45,092:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_a27249e8e6ff42038c53de73d27e566b_06afff6d35804db39ddbeb4ad318723e

2023-04-29 23:46:45,092:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_428b2119f94d4d2e89a49e34a4e352af

2023-04-29 23:46:45,092:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_58abaf8497c04ca2b401e40d572f8fae_4c7051e60ec24c0db58cc8123ce39a50

2023-04-29 23:46:45,092:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_ce1595236100473d8e27df8c353b29a2

2023-04-29 23:46:45,093:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_4aea141baf304171906d03d0d99e5023_beea22ce3a294c439c1ba477322b3d2b

2023-04-29 23:46:45,093:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_f6e48e034273428daace8b9daa68f6a2

2023-04-29 23:46:45,094:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_f2b95553dc3f4caab4ca08103affa553_79a199915aca4f17b28ca30d7f867f15

2023-04-29 23:46:45,094:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_0e3bb5742a9c41a0b15e90a71f1d51c9

2023-04-29 23:46:45,094:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_0c9e88cc6d1d47d0b89cf50aa8ead009_51eea7fc9a1244b0b26aaadd2044cce2

2023-04-29 23:46:45,094:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_af1513c6db4b4a76a2db75b0aa85fdd0

2023-04-29 23:46:45,095:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_b884caa1e87f4014824d0e53bb93a47a_e5e633ae5a0746358de33e4a66f6fe6a

2023-04-29 23:46:45,095:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_66ea81f6676c4de8a79b544c1d6e3d1e

2023-04-29 23:46:45,096:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_3e41e2c63a1d41d1afc5c4e44632c6c5_8187072faf3d4e52ba8f671039bcc9ef

2023-04-29 23:46:45,096:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_72ad9603b7a242bcb3475afaaff388f1

2023-04-29 23:46:45,097:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_3d9ea505f80d4d869736904d44d78424_52ae9cf4665241429c30909fa5c771b7

2023-04-29 23:46:45,097:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_dd341d44f2e54aedbc4410153268d92a

2023-04-29 23:46:45,098:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_1402aa69f48f4b73ad32e30073e56081_a2c9e9ef6c45488bbc7806414392a473

2023-04-29 23:46:45,098:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_07b0055c7c4741c8aef70b009dcad725

2023-04-29 23:46:45,098:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_1fc85853dbf54911a29ca8e757d74697_609f3a233f6b4afab8e7a85696b6051f

2023-04-29 23:46:45,099:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_7df43d417f7043549e5db0a2b75e902e

2023-04-29 23:46:45,099:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_90ca1ff460c04dcd8ffb4f322bd54e4c_8c2b899de4b840e8b0c4649b56ec7823

2023-04-29 23:46:45,099:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_55f6c90a7b5e4fb3857641129198a779

2023-04-29 23:46:45,099:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_4ccbbc44499942e893947bca3ba324dc_d68eb04c985f4419ba4eae02edd914fb

2023-04-29 23:46:45,099:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_26f73c69a3b846378cfe3617f5a04f8e

2023-04-29 23:46:45,101:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_37a7d85345614af7842eac503351e31d_14331e57373d4b47a76a5090f1002769

2023-04-29 23:46:45,101:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_03313695898c48c7aacb2a8d0f9f2e03

2023-04-29 23:46:45,101:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_afd12a7ce129496a8351a74995add12f_4470aba310734b548450358b6ffa2dbb

2023-04-29 23:46:45,101:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_ec20c70c760943888c1e2f5bf4259ea9

2023-04-29 23:46:45,101:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_d961e4ac566f42f6b83c5d88343680a2_c43bfee452104545b331f3a16ccaf8f8

2023-04-29 23:46:45,103:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_fb85ccc4c2094ed0b0bfbe97930ffc55

2023-04-29 23:46:45,103:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_d60ab4ee6f4849bd94858657109f7270_82b081ac387a4cf1913fbf5a5d2b94c3

2023-04-29 23:46:45,104:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_de0a16f7cc204a3fb72d2e8a88894c3d

2023-04-29 23:46:45,104:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_e892dbc853a94467b86068741fc6abde_53cd62be333343949ef661dd75021bc0

2023-04-29 23:46:45,105:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_ab581140e160417f9240d06f3ce43dd2

2023-04-29 23:46:45,105:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_b0a6c319faaa4e958634f11dc6133e51_794bf0e94b414c2184145eb5822739ac

2023-04-29 23:46:45,106:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_3ce50e63d86743c1bdef8aef73fe4a97

2023-04-29 23:46:45,106:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_1d01ad202bea4d9194a3d87af8d06367_e465acd263944a509c7ef00bd5fb9b20

2023-04-29 23:46:45,106:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_b0b7a4a6f7a641aa8757f42f558eeded

2023-04-29 23:46:45,107:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_7077dae39fc74b61908effcfff6eedfc_f7b2bf79ac4543b889ebeb2e32660646

2023-04-29 23:46:45,108:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_3048bc2641d248dd89aba43b90fcdd69

2023-04-29 23:46:45,108:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_2abafebc69184dfb8c003306fa375d87_f95e0bc137ac4855a5ac69c322f38fc9

2023-04-29 23:46:45,108:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_c5a6ab5362694d24a2bd9525974f938f

2023-04-29 23:46:45,109:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_efbc31c71fd0418fa60f8da24441883a_9d9274b4fe68488d8ec04e408d72fdb0

2023-04-29 23:46:45,109:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_0a701376abc24baf85f07cb6a6c39d02

2023-04-29 23:46:45,109:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_3daf51c2f7234ffcb7d03b9d567c4f3c_a0612519c5b54d87b963e05d00052856

2023-04-29 23:46:45,110:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_c2e0f56962434c80a9b3193334adb22b

2023-04-29 23:46:45,110:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_09eab76a8d494b2dbb0eb65e7d36cf8b_7836bd8e5c8b4a66846acb20035df09e

2023-04-29 23:46:45,111:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_164b83fcd30e4e76897fac50ffb0cbdc

2023-04-29 23:46:45,111:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_83c7067f4e9746c787b6d0f1e853f514_857498bf6c7d4fb1b264b48080d1b4b0

2023-04-29 23:46:45,111:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_904c741b71c840be9fff1b8332d4f3d3

2023-04-29 23:46:45,111:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_4b329503aefc471dbb50bc0492a383c3_8b119f3b7c214852a9118be2019384b7

2023-04-29 23:46:45,112:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_5dcf2c39e8174e52b9a6473969516d77

2023-04-29 23:46:45,112:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_adc068fa63c64023aae8e3f76c7fb1da_c06b2b543f8b49d79967a029c14e1217

2023-04-29 23:46:45,112:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_c1ccb0493ea84c61bcc24a6fc9bab395

2023-04-29 23:46:45,113:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_a2a39b9c3fd042eb811ca5451acfd549_4b1e99b8ecb242c0934b64c7b865528c

2023-04-29 23:46:45,113:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_b0c6b4b4ee2b4513b1ef69cbfb8ec829

2023-04-29 23:46:45,113:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_fcf93825499d4479bd2900cd8c5e875f_d8b08d634f83459fba14164740fcfd86

2023-04-29 23:46:45,113:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_9ae25b8ead0a4045b60e80922a45a274

2023-04-29 23:46:45,114:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_c42c00491b124f169590f93313d13a23_521f26d55c704be3b7d55a50b87618e2

2023-04-29 23:46:45,114:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_997380c97327473e8f7b9d820369b5aa

2023-04-29 23:46:45,114:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_b0d03f91fccb4c3794602186fe0de453_cfd1ac3fb97b4293a2e0fd16112f19cb

2023-04-29 23:46:45,115:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_5b4abd87a1184c618b3b0ac09be90c2c

2023-04-29 23:46:45,115:WARNING:Failed to delete temporary folder: C:\Users\eeman\AppData\Local\Temp\joblib_memmapping_folder_27540_057013cfae634015b7417d9b9eb47ad6_1ae703b7c1e54962829e483b7cc2a323

2023-04-29 23:47:21,018:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 23:47:21,019:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 23:47:21,019:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 23:47:21,019:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-29 23:47:23,198:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-04-29 23:47:23,595:INFO:PyCaret ClassificationExperiment
2023-04-29 23:47:23,595:INFO:Logging name: clf-default-name
2023-04-29 23:47:23,595:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-04-29 23:47:23,595:INFO:version 3.0.0.rc8
2023-04-29 23:47:23,595:INFO:Initializing setup()
2023-04-29 23:47:23,595:INFO:self.USI: 5af9
2023-04-29 23:47:23,595:INFO:self._variable_keys: {'y_test', 'seed', 'X_train', 'pipeline', 'logging_param', 'data', 'fold_groups_param', '_ml_usecase', 'exp_id', 'is_multiclass', 'y_train', 'gpu_n_jobs_param', 'target_param', 'memory', 'fold_shuffle_param', 'y', 'html_param', 'X', 'idx', 'log_plots_param', 'X_test', 'fold_generator', 'exp_name_log', 'USI', 'n_jobs_param', 'fix_imbalance', '_available_plots', 'gpu_param'}
2023-04-29 23:47:23,596:INFO:Checking environment
2023-04-29 23:47:23,596:INFO:python_version: 3.9.13
2023-04-29 23:47:23,596:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-04-29 23:47:23,596:INFO:machine: AMD64
2023-04-29 23:47:23,596:INFO:platform: Windows-10-10.0.22621-SP0
2023-04-29 23:47:23,596:INFO:Memory: svmem(total=16782143488, available=2122297344, percent=87.4, used=14659846144, free=2122297344)
2023-04-29 23:47:23,596:INFO:Physical Core: 10
2023-04-29 23:47:23,596:INFO:Logical Core: 12
2023-04-29 23:47:23,596:INFO:Checking libraries
2023-04-29 23:47:23,596:INFO:System:
2023-04-29 23:47:23,596:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-04-29 23:47:23,596:INFO:executable: C:\Users\eeman\anaconda3\python.exe
2023-04-29 23:47:23,596:INFO:   machine: Windows-10-10.0.22621-SP0
2023-04-29 23:47:23,596:INFO:PyCaret required dependencies:
2023-04-29 23:47:23,596:INFO:                 pip: 22.2.2
2023-04-29 23:47:23,596:INFO:          setuptools: 63.4.1
2023-04-29 23:47:23,596:INFO:             pycaret: 3.0.0rc8
2023-04-29 23:47:23,596:INFO:             IPython: 7.31.1
2023-04-29 23:47:23,596:INFO:          ipywidgets: 7.6.5
2023-04-29 23:47:23,596:INFO:                tqdm: 4.64.1
2023-04-29 23:47:23,596:INFO:               numpy: 1.21.5
2023-04-29 23:47:23,597:INFO:              pandas: 1.4.4
2023-04-29 23:47:23,597:INFO:              jinja2: 2.11.3
2023-04-29 23:47:23,597:INFO:               scipy: 1.9.1
2023-04-29 23:47:23,597:INFO:              joblib: 1.2.0
2023-04-29 23:47:23,597:INFO:             sklearn: 1.0.2
2023-04-29 23:47:23,597:INFO:                pyod: 1.0.7
2023-04-29 23:47:23,597:INFO:            imblearn: 0.10.1
2023-04-29 23:47:23,597:INFO:   category_encoders: 2.6.0
2023-04-29 23:47:23,597:INFO:            lightgbm: 3.3.4
2023-04-29 23:47:23,597:INFO:               numba: 0.55.1
2023-04-29 23:47:23,597:INFO:            requests: 2.28.1
2023-04-29 23:47:23,597:INFO:          matplotlib: 3.5.2
2023-04-29 23:47:23,597:INFO:          scikitplot: 0.3.7
2023-04-29 23:47:23,597:INFO:         yellowbrick: 1.5
2023-04-29 23:47:23,598:INFO:              plotly: 5.9.0
2023-04-29 23:47:23,598:INFO:             kaleido: 0.2.1
2023-04-29 23:47:23,598:INFO:         statsmodels: 0.13.5
2023-04-29 23:47:23,598:INFO:              sktime: 0.15.1
2023-04-29 23:47:23,598:INFO:               tbats: 1.1.2
2023-04-29 23:47:23,598:INFO:            pmdarima: 2.0.2
2023-04-29 23:47:23,598:INFO:              psutil: 5.9.0
2023-04-29 23:47:23,598:INFO:PyCaret optional dependencies:
2023-04-29 23:47:23,632:INFO:                shap: 0.41.0
2023-04-29 23:47:23,632:INFO:           interpret: Not installed
2023-04-29 23:47:23,632:INFO:                umap: Not installed
2023-04-29 23:47:23,632:INFO:    pandas_profiling: Not installed
2023-04-29 23:47:23,632:INFO:  explainerdashboard: Not installed
2023-04-29 23:47:23,632:INFO:             autoviz: Not installed
2023-04-29 23:47:23,632:INFO:           fairlearn: Not installed
2023-04-29 23:47:23,632:INFO:             xgboost: Not installed
2023-04-29 23:47:23,632:INFO:            catboost: Not installed
2023-04-29 23:47:23,632:INFO:              kmodes: Not installed
2023-04-29 23:47:23,633:INFO:             mlxtend: Not installed
2023-04-29 23:47:23,633:INFO:       statsforecast: Not installed
2023-04-29 23:47:23,633:INFO:        tune_sklearn: Not installed
2023-04-29 23:47:23,633:INFO:                 ray: Not installed
2023-04-29 23:47:23,633:INFO:            hyperopt: Not installed
2023-04-29 23:47:23,633:INFO:              optuna: Not installed
2023-04-29 23:47:23,633:INFO:               skopt: Not installed
2023-04-29 23:47:23,633:INFO:              mlflow: Not installed
2023-04-29 23:47:23,633:INFO:              gradio: Not installed
2023-04-29 23:47:23,633:INFO:             fastapi: Not installed
2023-04-29 23:47:23,633:INFO:             uvicorn: Not installed
2023-04-29 23:47:23,633:INFO:              m2cgen: Not installed
2023-04-29 23:47:23,633:INFO:           evidently: Not installed
2023-04-29 23:47:23,633:INFO:                nltk: 3.7
2023-04-29 23:47:23,633:INFO:            pyLDAvis: Not installed
2023-04-29 23:47:23,634:INFO:              gensim: 4.1.2
2023-04-29 23:47:23,634:INFO:               spacy: Not installed
2023-04-29 23:47:23,634:INFO:           wordcloud: Not installed
2023-04-29 23:47:23,634:INFO:            textblob: Not installed
2023-04-29 23:47:23,634:INFO:               fugue: Not installed
2023-04-29 23:47:23,634:INFO:           streamlit: Not installed
2023-04-29 23:47:23,634:INFO:             prophet: Not installed
2023-04-29 23:47:23,634:INFO:None
2023-04-29 23:47:23,634:INFO:Set up data.
2023-04-29 23:47:23,641:INFO:Set up train/test split.
2023-04-29 23:47:23,649:INFO:Set up index.
2023-04-29 23:47:23,649:INFO:Set up folding strategy.
2023-04-29 23:47:23,650:INFO:Assigning column types.
2023-04-29 23:47:23,655:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-29 23:47:23,776:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-29 23:47:23,782:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 23:47:23,873:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:23,929:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,051:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-29 23:47:24,053:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 23:47:24,122:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,123:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,123:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-29 23:47:24,225:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 23:47:24,416:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,416:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,534:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 23:47:24,604:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,605:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,605:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-04-29 23:47:24,810:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,811:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,997:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,998:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:24,999:INFO:Preparing preprocessing pipeline...
2023-04-29 23:47:25,001:INFO:Set up simple imputation.
2023-04-29 23:47:25,004:INFO:Set up encoding of categorical features.
2023-04-29 23:47:25,073:INFO:Finished creating preprocessing pipeline.
2023-04-29 23:47:25,087:INFO:Pipeline: Pipeline(memory=Memory(location=C:\Users\eeman\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'L', 'R', 'A_M'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose=0))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    inc...
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Color', 'Spectral_Class'],
                                    transformer=OneHotEncoder(cols=['Color',
                                                                    'Spectral_Class'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2023-04-29 23:47:25,087:INFO:Creating final display dataframe.
2023-04-29 23:47:25,515:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target              Type
2                   Target type        Multiclass
3           Original data shape          (216, 7)
4        Transformed data shape         (216, 24)
5   Transformed train set shape         (151, 24)
6    Transformed test set shape          (65, 24)
7              Numeric features                 4
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              5af9
2023-04-29 23:47:25,739:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:25,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:25,927:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:25,928:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:47:25,929:INFO:setup() successfully completed in 2.34s...............
2023-04-29 23:47:25,950:INFO:Initializing compare_models()
2023-04-29 23:47:25,951:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-04-29 23:47:25,951:INFO:Checking exceptions
2023-04-29 23:47:25,959:INFO:Preparing display monitor
2023-04-29 23:47:26,041:INFO:Initializing Logistic Regression
2023-04-29 23:47:26,041:INFO:Total runtime is 0.0 minutes
2023-04-29 23:47:26,049:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:26,050:INFO:Initializing create_model()
2023-04-29 23:47:26,050:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:26,050:INFO:Checking exceptions
2023-04-29 23:47:26,051:INFO:Importing libraries
2023-04-29 23:47:26,051:INFO:Copying training dataset
2023-04-29 23:47:26,057:INFO:Defining folds
2023-04-29 23:47:26,057:INFO:Declaring metric variables
2023-04-29 23:47:26,064:INFO:Importing untrained model
2023-04-29 23:47:26,073:INFO:Logistic Regression Imported successfully
2023-04-29 23:47:26,092:INFO:Starting cross validation
2023-04-29 23:47:26,095:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:36,867:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:47:36,924:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:47:37,092:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:47:37,102:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:47:37,106:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:47:37,144:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:47:37,509:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:47:37,513:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:47:37,597:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:47:37,817:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:47:38,106:INFO:Calculating mean and std
2023-04-29 23:47:38,109:INFO:Creating metrics dataframe
2023-04-29 23:47:38,115:INFO:Uploading results into container
2023-04-29 23:47:38,116:INFO:Uploading model into container now
2023-04-29 23:47:38,117:INFO:_master_model_container: 1
2023-04-29 23:47:38,117:INFO:_display_container: 2
2023-04-29 23:47:38,118:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 23:47:38,118:INFO:create_model() successfully completed......................................
2023-04-29 23:47:38,308:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:38,310:INFO:Creating metrics dataframe
2023-04-29 23:47:38,324:INFO:Initializing K Neighbors Classifier
2023-04-29 23:47:38,325:INFO:Total runtime is 0.20473451614379884 minutes
2023-04-29 23:47:38,330:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:38,331:INFO:Initializing create_model()
2023-04-29 23:47:38,331:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:38,331:INFO:Checking exceptions
2023-04-29 23:47:38,331:INFO:Importing libraries
2023-04-29 23:47:38,331:INFO:Copying training dataset
2023-04-29 23:47:38,338:INFO:Defining folds
2023-04-29 23:47:38,340:INFO:Declaring metric variables
2023-04-29 23:47:38,346:INFO:Importing untrained model
2023-04-29 23:47:38,352:INFO:K Neighbors Classifier Imported successfully
2023-04-29 23:47:38,364:INFO:Starting cross validation
2023-04-29 23:47:38,366:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:38,660:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:47:38,663:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:47:38,678:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:47:38,681:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:47:38,702:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:47:38,882:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:47:38,889:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:47:38,959:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:47:39,009:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:39,447:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:46,566:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:47:46,566:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:47:46,668:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:46,678:INFO:Calculating mean and std
2023-04-29 23:47:46,683:INFO:Creating metrics dataframe
2023-04-29 23:47:46,699:INFO:Uploading results into container
2023-04-29 23:47:46,702:INFO:Uploading model into container now
2023-04-29 23:47:46,704:INFO:_master_model_container: 2
2023-04-29 23:47:46,707:INFO:_display_container: 2
2023-04-29 23:47:46,709:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-04-29 23:47:46,709:INFO:create_model() successfully completed......................................
2023-04-29 23:47:47,034:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:47,034:INFO:Creating metrics dataframe
2023-04-29 23:47:47,063:INFO:Initializing Naive Bayes
2023-04-29 23:47:47,064:INFO:Total runtime is 0.35038299163182574 minutes
2023-04-29 23:47:47,075:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:47,076:INFO:Initializing create_model()
2023-04-29 23:47:47,076:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:47,077:INFO:Checking exceptions
2023-04-29 23:47:47,077:INFO:Importing libraries
2023-04-29 23:47:47,078:INFO:Copying training dataset
2023-04-29 23:47:47,088:INFO:Defining folds
2023-04-29 23:47:47,088:INFO:Declaring metric variables
2023-04-29 23:47:47,098:INFO:Importing untrained model
2023-04-29 23:47:47,108:INFO:Naive Bayes Imported successfully
2023-04-29 23:47:47,137:INFO:Starting cross validation
2023-04-29 23:47:47,141:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:47,662:INFO:Calculating mean and std
2023-04-29 23:47:47,668:INFO:Creating metrics dataframe
2023-04-29 23:47:47,679:INFO:Uploading results into container
2023-04-29 23:47:47,680:INFO:Uploading model into container now
2023-04-29 23:47:47,683:INFO:_master_model_container: 3
2023-04-29 23:47:47,683:INFO:_display_container: 2
2023-04-29 23:47:47,684:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 23:47:47,684:INFO:create_model() successfully completed......................................
2023-04-29 23:47:47,920:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:47,921:INFO:Creating metrics dataframe
2023-04-29 23:47:47,959:INFO:Initializing Decision Tree Classifier
2023-04-29 23:47:47,959:INFO:Total runtime is 0.3652884721755981 minutes
2023-04-29 23:47:47,977:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:47,978:INFO:Initializing create_model()
2023-04-29 23:47:47,979:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:47,980:INFO:Checking exceptions
2023-04-29 23:47:47,981:INFO:Importing libraries
2023-04-29 23:47:47,981:INFO:Copying training dataset
2023-04-29 23:47:48,004:INFO:Defining folds
2023-04-29 23:47:48,004:INFO:Declaring metric variables
2023-04-29 23:47:48,070:INFO:Importing untrained model
2023-04-29 23:47:48,085:INFO:Decision Tree Classifier Imported successfully
2023-04-29 23:47:48,108:INFO:Starting cross validation
2023-04-29 23:47:48,110:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:48,549:INFO:Calculating mean and std
2023-04-29 23:47:48,551:INFO:Creating metrics dataframe
2023-04-29 23:47:48,556:INFO:Uploading results into container
2023-04-29 23:47:48,557:INFO:Uploading model into container now
2023-04-29 23:47:48,558:INFO:_master_model_container: 4
2023-04-29 23:47:48,558:INFO:_display_container: 2
2023-04-29 23:47:48,559:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=123, splitter='best')
2023-04-29 23:47:48,559:INFO:create_model() successfully completed......................................
2023-04-29 23:47:48,717:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:48,717:INFO:Creating metrics dataframe
2023-04-29 23:47:48,735:INFO:Initializing SVM - Linear Kernel
2023-04-29 23:47:48,736:INFO:Total runtime is 0.378238840897878 minutes
2023-04-29 23:47:48,743:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:48,744:INFO:Initializing create_model()
2023-04-29 23:47:48,744:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:48,744:INFO:Checking exceptions
2023-04-29 23:47:48,744:INFO:Importing libraries
2023-04-29 23:47:48,744:INFO:Copying training dataset
2023-04-29 23:47:48,753:INFO:Defining folds
2023-04-29 23:47:48,754:INFO:Declaring metric variables
2023-04-29 23:47:48,762:INFO:Importing untrained model
2023-04-29 23:47:48,772:INFO:SVM - Linear Kernel Imported successfully
2023-04-29 23:47:48,787:INFO:Starting cross validation
2023-04-29 23:47:48,791:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:48,950:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:48,976:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:48,977:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:48,985:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:49,005:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:49,015:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:49,028:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:49,030:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:49,045:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:49,049:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:49,054:INFO:Calculating mean and std
2023-04-29 23:47:49,057:INFO:Creating metrics dataframe
2023-04-29 23:47:49,061:INFO:Uploading results into container
2023-04-29 23:47:49,062:INFO:Uploading model into container now
2023-04-29 23:47:49,063:INFO:_master_model_container: 5
2023-04-29 23:47:49,063:INFO:_display_container: 2
2023-04-29 23:47:49,064:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-04-29 23:47:49,064:INFO:create_model() successfully completed......................................
2023-04-29 23:47:49,224:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:49,224:INFO:Creating metrics dataframe
2023-04-29 23:47:49,246:INFO:Initializing Ridge Classifier
2023-04-29 23:47:49,247:INFO:Total runtime is 0.38675994873046876 minutes
2023-04-29 23:47:49,255:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:49,255:INFO:Initializing create_model()
2023-04-29 23:47:49,256:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:49,256:INFO:Checking exceptions
2023-04-29 23:47:49,256:INFO:Importing libraries
2023-04-29 23:47:49,256:INFO:Copying training dataset
2023-04-29 23:47:49,263:INFO:Defining folds
2023-04-29 23:47:49,264:INFO:Declaring metric variables
2023-04-29 23:47:49,271:INFO:Importing untrained model
2023-04-29 23:47:49,278:INFO:Ridge Classifier Imported successfully
2023-04-29 23:47:49,288:INFO:Starting cross validation
2023-04-29 23:47:49,292:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:49,470:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:49,476:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:49,518:INFO:Calculating mean and std
2023-04-29 23:47:49,520:INFO:Creating metrics dataframe
2023-04-29 23:47:49,526:INFO:Uploading results into container
2023-04-29 23:47:49,526:INFO:Uploading model into container now
2023-04-29 23:47:49,527:INFO:_master_model_container: 6
2023-04-29 23:47:49,527:INFO:_display_container: 2
2023-04-29 23:47:49,528:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 23:47:49,528:INFO:create_model() successfully completed......................................
2023-04-29 23:47:49,734:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:49,735:INFO:Creating metrics dataframe
2023-04-29 23:47:49,767:INFO:Initializing Random Forest Classifier
2023-04-29 23:47:49,767:INFO:Total runtime is 0.39542383750279747 minutes
2023-04-29 23:47:49,776:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:49,777:INFO:Initializing create_model()
2023-04-29 23:47:49,777:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:49,778:INFO:Checking exceptions
2023-04-29 23:47:49,778:INFO:Importing libraries
2023-04-29 23:47:49,779:INFO:Copying training dataset
2023-04-29 23:47:49,792:INFO:Defining folds
2023-04-29 23:47:49,792:INFO:Declaring metric variables
2023-04-29 23:47:49,805:INFO:Importing untrained model
2023-04-29 23:47:49,819:INFO:Random Forest Classifier Imported successfully
2023-04-29 23:47:49,843:INFO:Starting cross validation
2023-04-29 23:47:49,849:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:51,221:INFO:Calculating mean and std
2023-04-29 23:47:51,225:INFO:Creating metrics dataframe
2023-04-29 23:47:51,233:INFO:Uploading results into container
2023-04-29 23:47:51,234:INFO:Uploading model into container now
2023-04-29 23:47:51,235:INFO:_master_model_container: 7
2023-04-29 23:47:51,235:INFO:_display_container: 2
2023-04-29 23:47:51,236:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 23:47:51,236:INFO:create_model() successfully completed......................................
2023-04-29 23:47:51,450:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:51,451:INFO:Creating metrics dataframe
2023-04-29 23:47:51,488:INFO:Initializing Quadratic Discriminant Analysis
2023-04-29 23:47:51,488:INFO:Total runtime is 0.42410883506139124 minutes
2023-04-29 23:47:51,495:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:51,496:INFO:Initializing create_model()
2023-04-29 23:47:51,496:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:51,496:INFO:Checking exceptions
2023-04-29 23:47:51,496:INFO:Importing libraries
2023-04-29 23:47:51,496:INFO:Copying training dataset
2023-04-29 23:47:51,513:INFO:Defining folds
2023-04-29 23:47:51,514:INFO:Declaring metric variables
2023-04-29 23:47:51,525:INFO:Importing untrained model
2023-04-29 23:47:51,535:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 23:47:51,574:INFO:Starting cross validation
2023-04-29 23:47:51,576:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:51,715:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:47:51,764:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:47:51,781:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,781:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,783:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,784:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:47:51,799:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:47:51,826:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:47:51,844:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,844:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,845:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,846:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,846:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,847:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,878:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:47:51,879:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,879:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,881:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,887:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:51,887:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,888:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,889:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,909:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:47:51,924:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,925:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,926:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,950:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:47:51,951:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,951:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,952:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,954:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:47:51,955:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,956:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,957:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,957:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:51,963:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,964:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,965:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,971:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,971:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,972:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:51,972:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,975:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,975:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,976:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:51,977:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:51,981:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:47:51,992:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,993:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:51,994:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:52,004:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:52,015:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,016:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,017:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:52,017:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,018:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,018:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:52,026:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,027:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,027:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:52,033:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:52,034:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,034:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,035:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:52,039:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:52,044:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,045:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,045:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:52,076:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,076:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,077:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:52,080:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,080:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,081:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:52,086:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:52,091:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:52,109:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,109:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-04-29 23:47:52,110:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-04-29 23:47:52,115:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:52,122:INFO:Calculating mean and std
2023-04-29 23:47:52,126:INFO:Creating metrics dataframe
2023-04-29 23:47:52,134:INFO:Uploading results into container
2023-04-29 23:47:52,136:INFO:Uploading model into container now
2023-04-29 23:47:52,137:INFO:_master_model_container: 8
2023-04-29 23:47:52,138:INFO:_display_container: 2
2023-04-29 23:47:52,139:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 23:47:52,139:INFO:create_model() successfully completed......................................
2023-04-29 23:47:52,374:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:52,374:INFO:Creating metrics dataframe
2023-04-29 23:47:52,420:INFO:Initializing Ada Boost Classifier
2023-04-29 23:47:52,421:INFO:Total runtime is 0.43967070976893113 minutes
2023-04-29 23:47:52,432:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:52,433:INFO:Initializing create_model()
2023-04-29 23:47:52,436:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:52,436:INFO:Checking exceptions
2023-04-29 23:47:52,436:INFO:Importing libraries
2023-04-29 23:47:52,436:INFO:Copying training dataset
2023-04-29 23:47:52,447:INFO:Defining folds
2023-04-29 23:47:52,448:INFO:Declaring metric variables
2023-04-29 23:47:52,477:INFO:Importing untrained model
2023-04-29 23:47:52,496:INFO:Ada Boost Classifier Imported successfully
2023-04-29 23:47:52,542:INFO:Starting cross validation
2023-04-29 23:47:52,546:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:53,304:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:53,332:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:53,355:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:53,372:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:53,386:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:53,394:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:53,419:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:53,437:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:53,451:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:53,487:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:47:53,496:INFO:Calculating mean and std
2023-04-29 23:47:53,502:INFO:Creating metrics dataframe
2023-04-29 23:47:53,511:INFO:Uploading results into container
2023-04-29 23:47:53,513:INFO:Uploading model into container now
2023-04-29 23:47:53,514:INFO:_master_model_container: 9
2023-04-29 23:47:53,514:INFO:_display_container: 2
2023-04-29 23:47:53,515:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2023-04-29 23:47:53,516:INFO:create_model() successfully completed......................................
2023-04-29 23:47:53,783:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:53,783:INFO:Creating metrics dataframe
2023-04-29 23:47:53,833:INFO:Initializing Gradient Boosting Classifier
2023-04-29 23:47:53,834:INFO:Total runtime is 0.463212275505066 minutes
2023-04-29 23:47:53,852:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:53,853:INFO:Initializing create_model()
2023-04-29 23:47:53,853:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:53,853:INFO:Checking exceptions
2023-04-29 23:47:53,854:INFO:Importing libraries
2023-04-29 23:47:53,854:INFO:Copying training dataset
2023-04-29 23:47:53,871:INFO:Defining folds
2023-04-29 23:47:53,872:INFO:Declaring metric variables
2023-04-29 23:47:53,885:INFO:Importing untrained model
2023-04-29 23:47:53,899:INFO:Gradient Boosting Classifier Imported successfully
2023-04-29 23:47:53,922:INFO:Starting cross validation
2023-04-29 23:47:53,924:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:55,933:INFO:Calculating mean and std
2023-04-29 23:47:55,937:INFO:Creating metrics dataframe
2023-04-29 23:47:55,945:INFO:Uploading results into container
2023-04-29 23:47:55,946:INFO:Uploading model into container now
2023-04-29 23:47:55,947:INFO:_master_model_container: 10
2023-04-29 23:47:55,948:INFO:_display_container: 2
2023-04-29 23:47:55,949:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-04-29 23:47:55,949:INFO:create_model() successfully completed......................................
2023-04-29 23:47:56,211:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:56,212:INFO:Creating metrics dataframe
2023-04-29 23:47:56,277:INFO:Initializing Linear Discriminant Analysis
2023-04-29 23:47:56,277:INFO:Total runtime is 0.5039236783981325 minutes
2023-04-29 23:47:56,290:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:56,293:INFO:Initializing create_model()
2023-04-29 23:47:56,294:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:56,294:INFO:Checking exceptions
2023-04-29 23:47:56,295:INFO:Importing libraries
2023-04-29 23:47:56,295:INFO:Copying training dataset
2023-04-29 23:47:56,311:INFO:Defining folds
2023-04-29 23:47:56,311:INFO:Declaring metric variables
2023-04-29 23:47:56,329:INFO:Importing untrained model
2023-04-29 23:47:56,345:INFO:Linear Discriminant Analysis Imported successfully
2023-04-29 23:47:56,370:INFO:Starting cross validation
2023-04-29 23:47:56,374:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:56,888:INFO:Calculating mean and std
2023-04-29 23:47:56,893:INFO:Creating metrics dataframe
2023-04-29 23:47:56,904:INFO:Uploading results into container
2023-04-29 23:47:56,906:INFO:Uploading model into container now
2023-04-29 23:47:56,907:INFO:_master_model_container: 11
2023-04-29 23:47:56,907:INFO:_display_container: 2
2023-04-29 23:47:56,908:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-04-29 23:47:56,908:INFO:create_model() successfully completed......................................
2023-04-29 23:47:57,179:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:57,180:INFO:Creating metrics dataframe
2023-04-29 23:47:57,235:INFO:Initializing Extra Trees Classifier
2023-04-29 23:47:57,237:INFO:Total runtime is 0.5199247360229493 minutes
2023-04-29 23:47:57,253:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:57,255:INFO:Initializing create_model()
2023-04-29 23:47:57,256:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:57,256:INFO:Checking exceptions
2023-04-29 23:47:57,256:INFO:Importing libraries
2023-04-29 23:47:57,257:INFO:Copying training dataset
2023-04-29 23:47:57,272:INFO:Defining folds
2023-04-29 23:47:57,273:INFO:Declaring metric variables
2023-04-29 23:47:57,288:INFO:Importing untrained model
2023-04-29 23:47:57,302:INFO:Extra Trees Classifier Imported successfully
2023-04-29 23:47:57,330:INFO:Starting cross validation
2023-04-29 23:47:57,333:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:47:58,680:INFO:Calculating mean and std
2023-04-29 23:47:58,686:INFO:Creating metrics dataframe
2023-04-29 23:47:58,701:INFO:Uploading results into container
2023-04-29 23:47:58,703:INFO:Uploading model into container now
2023-04-29 23:47:58,705:INFO:_master_model_container: 12
2023-04-29 23:47:58,706:INFO:_display_container: 2
2023-04-29 23:47:58,708:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 23:47:58,708:INFO:create_model() successfully completed......................................
2023-04-29 23:47:58,992:INFO:SubProcess create_model() end ==================================
2023-04-29 23:47:58,993:INFO:Creating metrics dataframe
2023-04-29 23:47:59,047:INFO:Initializing Light Gradient Boosting Machine
2023-04-29 23:47:59,048:INFO:Total runtime is 0.5501145760218303 minutes
2023-04-29 23:47:59,060:INFO:SubProcess create_model() called ==================================
2023-04-29 23:47:59,062:INFO:Initializing create_model()
2023-04-29 23:47:59,063:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:47:59,063:INFO:Checking exceptions
2023-04-29 23:47:59,064:INFO:Importing libraries
2023-04-29 23:47:59,065:INFO:Copying training dataset
2023-04-29 23:47:59,078:INFO:Defining folds
2023-04-29 23:47:59,080:INFO:Declaring metric variables
2023-04-29 23:47:59,095:INFO:Importing untrained model
2023-04-29 23:47:59,114:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 23:47:59,147:INFO:Starting cross validation
2023-04-29 23:47:59,152:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:03,012:INFO:Calculating mean and std
2023-04-29 23:48:03,016:INFO:Creating metrics dataframe
2023-04-29 23:48:03,026:INFO:Uploading results into container
2023-04-29 23:48:03,028:INFO:Uploading model into container now
2023-04-29 23:48:03,029:INFO:_master_model_container: 13
2023-04-29 23:48:03,029:INFO:_display_container: 2
2023-04-29 23:48:03,031:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 23:48:03,032:INFO:create_model() successfully completed......................................
2023-04-29 23:48:03,259:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:03,261:INFO:Creating metrics dataframe
2023-04-29 23:48:03,296:INFO:Initializing Dummy Classifier
2023-04-29 23:48:03,297:INFO:Total runtime is 0.6209326704343161 minutes
2023-04-29 23:48:03,308:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:03,310:INFO:Initializing create_model()
2023-04-29 23:48:03,310:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD0AF7BB0>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:03,310:INFO:Checking exceptions
2023-04-29 23:48:03,310:INFO:Importing libraries
2023-04-29 23:48:03,310:INFO:Copying training dataset
2023-04-29 23:48:03,322:INFO:Defining folds
2023-04-29 23:48:03,323:INFO:Declaring metric variables
2023-04-29 23:48:03,335:INFO:Importing untrained model
2023-04-29 23:48:03,347:INFO:Dummy Classifier Imported successfully
2023-04-29 23:48:03,372:INFO:Starting cross validation
2023-04-29 23:48:03,375:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:03,752:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:03,778:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:03,796:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:03,804:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:03,815:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:03,815:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:03,858:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:03,874:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:03,880:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:03,893:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:03,902:INFO:Calculating mean and std
2023-04-29 23:48:03,905:INFO:Creating metrics dataframe
2023-04-29 23:48:03,915:INFO:Uploading results into container
2023-04-29 23:48:03,917:INFO:Uploading model into container now
2023-04-29 23:48:03,917:INFO:_master_model_container: 14
2023-04-29 23:48:03,918:INFO:_display_container: 2
2023-04-29 23:48:03,918:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2023-04-29 23:48:03,920:INFO:create_model() successfully completed......................................
2023-04-29 23:48:04,140:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:04,140:INFO:Creating metrics dataframe
2023-04-29 23:48:04,228:INFO:Initializing create_model()
2023-04-29 23:48:04,229:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:04,229:INFO:Checking exceptions
2023-04-29 23:48:04,236:INFO:Importing libraries
2023-04-29 23:48:04,237:INFO:Copying training dataset
2023-04-29 23:48:04,248:INFO:Defining folds
2023-04-29 23:48:04,250:INFO:Declaring metric variables
2023-04-29 23:48:04,251:INFO:Importing untrained model
2023-04-29 23:48:04,252:INFO:Declaring custom model
2023-04-29 23:48:04,255:INFO:Random Forest Classifier Imported successfully
2023-04-29 23:48:04,263:INFO:Cross validation set to False
2023-04-29 23:48:04,263:INFO:Fitting Model
2023-04-29 23:48:04,832:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 23:48:04,833:INFO:create_model() successfully completed......................................
2023-04-29 23:48:05,116:INFO:_master_model_container: 14
2023-04-29 23:48:05,117:INFO:_display_container: 2
2023-04-29 23:48:05,117:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 23:48:05,118:INFO:compare_models() successfully completed......................................
2023-04-29 23:48:05,158:INFO:Initializing create_model()
2023-04-29 23:48:05,158:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=ridge, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:05,159:INFO:Checking exceptions
2023-04-29 23:48:05,204:INFO:Importing libraries
2023-04-29 23:48:05,204:INFO:Copying training dataset
2023-04-29 23:48:05,208:INFO:Defining folds
2023-04-29 23:48:05,210:INFO:Declaring metric variables
2023-04-29 23:48:05,218:INFO:Importing untrained model
2023-04-29 23:48:05,223:INFO:Ridge Classifier Imported successfully
2023-04-29 23:48:05,236:INFO:Starting cross validation
2023-04-29 23:48:05,238:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:05,369:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:05,417:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:05,468:INFO:Calculating mean and std
2023-04-29 23:48:05,469:INFO:Creating metrics dataframe
2023-04-29 23:48:05,476:INFO:Finalizing model
2023-04-29 23:48:05,529:INFO:Uploading results into container
2023-04-29 23:48:05,530:INFO:Uploading model into container now
2023-04-29 23:48:05,546:INFO:_master_model_container: 15
2023-04-29 23:48:05,546:INFO:_display_container: 3
2023-04-29 23:48:05,547:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 23:48:05,548:INFO:create_model() successfully completed......................................
2023-04-29 23:48:05,724:INFO:Initializing tune_model()
2023-04-29 23:48:05,725:INFO:tune_model(estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>)
2023-04-29 23:48:05,725:INFO:Checking exceptions
2023-04-29 23:48:05,780:INFO:Copying training dataset
2023-04-29 23:48:05,786:INFO:Checking base model
2023-04-29 23:48:05,787:INFO:Base model : Ridge Classifier
2023-04-29 23:48:05,794:INFO:Declaring metric variables
2023-04-29 23:48:05,804:INFO:Defining Hyperparameters
2023-04-29 23:48:05,957:INFO:Tuning with n_jobs=-1
2023-04-29 23:48:05,958:INFO:Initializing RandomizedSearchCV
2023-04-29 23:48:06,156:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,177:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,193:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,228:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,243:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,256:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,325:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,347:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,352:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,370:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,420:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,432:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,455:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,456:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,487:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,511:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,522:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,547:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,580:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,598:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,629:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,652:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,670:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,690:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,710:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,742:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,756:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,764:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,788:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,812:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,829:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,862:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,893:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,911:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,935:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,962:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,973:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:06,991:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,009:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,029:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,060:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,089:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,101:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,112:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,145:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,160:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,180:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,205:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,234:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,243:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:07,268:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,289:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,306:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,327:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,350:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,368:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,396:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,432:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,452:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,482:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,493:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,520:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,553:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,587:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,605:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,631:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,646:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,670:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,703:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,728:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,748:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,765:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,784:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,804:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,830:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,843:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,870:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,884:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,902:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,927:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,948:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,970:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:07,986:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,014:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,036:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,052:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,079:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,110:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,117:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,140:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,176:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:08,191:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:08,222:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:08,245:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:08,246:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:08,269:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:08,315:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:08,321:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:08,337:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:08,361:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * n_samples. 
  warnings.warn(

2023-04-29 23:48:08,409:INFO:best_params: {'actual_estimator__normalize': False, 'actual_estimator__fit_intercept': True, 'actual_estimator__alpha': 8.6}
2023-04-29 23:48:08,411:INFO:Hyperparameter search completed
2023-04-29 23:48:08,411:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:08,412:INFO:Initializing create_model()
2023-04-29 23:48:08,412:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD3E8FE80>, model_only=True, return_train_score=False, kwargs={'normalize': False, 'fit_intercept': True, 'alpha': 8.6})
2023-04-29 23:48:08,413:INFO:Checking exceptions
2023-04-29 23:48:08,413:INFO:Importing libraries
2023-04-29 23:48:08,413:INFO:Copying training dataset
2023-04-29 23:48:08,419:INFO:Defining folds
2023-04-29 23:48:08,419:INFO:Declaring metric variables
2023-04-29 23:48:08,427:INFO:Importing untrained model
2023-04-29 23:48:08,427:INFO:Declaring custom model
2023-04-29 23:48:08,436:INFO:Ridge Classifier Imported successfully
2023-04-29 23:48:08,448:INFO:Starting cross validation
2023-04-29 23:48:08,450:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:08,528:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,553:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,563:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,566:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,568:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:08,576:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,600:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,610:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,624:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,626:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,632:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,642:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:08,672:INFO:Calculating mean and std
2023-04-29 23:48:08,675:INFO:Creating metrics dataframe
2023-04-29 23:48:08,685:INFO:Finalizing model
2023-04-29 23:48:08,726:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
  warnings.warn(

2023-04-29 23:48:08,737:INFO:Uploading results into container
2023-04-29 23:48:08,738:INFO:Uploading model into container now
2023-04-29 23:48:08,739:INFO:_master_model_container: 16
2023-04-29 23:48:08,739:INFO:_display_container: 4
2023-04-29 23:48:08,740:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 23:48:08,740:INFO:create_model() successfully completed......................................
2023-04-29 23:48:08,924:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:08,924:INFO:choose_better activated
2023-04-29 23:48:08,933:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:08,934:INFO:Initializing create_model()
2023-04-29 23:48:08,934:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:08,934:INFO:Checking exceptions
2023-04-29 23:48:08,938:INFO:Importing libraries
2023-04-29 23:48:08,938:INFO:Copying training dataset
2023-04-29 23:48:08,944:INFO:Defining folds
2023-04-29 23:48:08,944:INFO:Declaring metric variables
2023-04-29 23:48:08,944:INFO:Importing untrained model
2023-04-29 23:48:08,944:INFO:Declaring custom model
2023-04-29 23:48:08,945:INFO:Ridge Classifier Imported successfully
2023-04-29 23:48:08,947:INFO:Starting cross validation
2023-04-29 23:48:08,948:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:09,055:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:09,123:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:09,178:INFO:Calculating mean and std
2023-04-29 23:48:09,179:INFO:Creating metrics dataframe
2023-04-29 23:48:09,185:INFO:Finalizing model
2023-04-29 23:48:09,229:INFO:Uploading results into container
2023-04-29 23:48:09,229:INFO:Uploading model into container now
2023-04-29 23:48:09,230:INFO:_master_model_container: 17
2023-04-29 23:48:09,230:INFO:_display_container: 5
2023-04-29 23:48:09,230:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 23:48:09,230:INFO:create_model() successfully completed......................................
2023-04-29 23:48:09,401:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:09,403:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001) result for Accuracy is 0.8946
2023-04-29 23:48:09,403:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001) result for Accuracy is 0.9013
2023-04-29 23:48:09,404:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001) is best model
2023-04-29 23:48:09,404:INFO:choose_better completed
2023-04-29 23:48:09,420:INFO:_master_model_container: 17
2023-04-29 23:48:09,420:INFO:_display_container: 4
2023-04-29 23:48:09,421:INFO:RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 23:48:09,421:INFO:tune_model() successfully completed......................................
2023-04-29 23:48:09,633:INFO:Initializing evaluate_model()
2023-04-29 23:48:09,633:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 23:48:09,671:INFO:Initializing plot_model()
2023-04-29 23:48:09,671:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=RidgeClassifier(alpha=8.6, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize=False, positive=False,
                random_state=123, solver='auto', tol=0.001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, system=True)
2023-04-29 23:48:09,672:INFO:Checking exceptions
2023-04-29 23:48:09,674:INFO:Preloading libraries
2023-04-29 23:48:09,675:INFO:Copying training dataset
2023-04-29 23:48:09,675:INFO:Plot type: pipeline
2023-04-29 23:48:10,006:INFO:Visual Rendered Successfully
2023-04-29 23:48:10,157:INFO:plot_model() successfully completed......................................
2023-04-29 23:48:10,180:INFO:Initializing create_model()
2023-04-29 23:48:10,181:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=nb, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:10,181:INFO:Checking exceptions
2023-04-29 23:48:10,225:INFO:Importing libraries
2023-04-29 23:48:10,226:INFO:Copying training dataset
2023-04-29 23:48:10,230:INFO:Defining folds
2023-04-29 23:48:10,230:INFO:Declaring metric variables
2023-04-29 23:48:10,237:INFO:Importing untrained model
2023-04-29 23:48:10,243:INFO:Naive Bayes Imported successfully
2023-04-29 23:48:10,256:INFO:Starting cross validation
2023-04-29 23:48:10,258:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:10,591:INFO:Calculating mean and std
2023-04-29 23:48:10,592:INFO:Creating metrics dataframe
2023-04-29 23:48:10,600:INFO:Finalizing model
2023-04-29 23:48:10,650:INFO:Uploading results into container
2023-04-29 23:48:10,651:INFO:Uploading model into container now
2023-04-29 23:48:10,665:INFO:_master_model_container: 18
2023-04-29 23:48:10,666:INFO:_display_container: 5
2023-04-29 23:48:10,666:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 23:48:10,666:INFO:create_model() successfully completed......................................
2023-04-29 23:48:10,844:INFO:Initializing tune_model()
2023-04-29 23:48:10,844:INFO:tune_model(estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>)
2023-04-29 23:48:10,844:INFO:Checking exceptions
2023-04-29 23:48:10,890:INFO:Copying training dataset
2023-04-29 23:48:10,894:INFO:Checking base model
2023-04-29 23:48:10,894:INFO:Base model : Naive Bayes
2023-04-29 23:48:10,901:INFO:Declaring metric variables
2023-04-29 23:48:10,906:INFO:Defining Hyperparameters
2023-04-29 23:48:11,071:INFO:Tuning with n_jobs=-1
2023-04-29 23:48:11,071:INFO:Initializing RandomizedSearchCV
2023-04-29 23:48:13,105:INFO:best_params: {'actual_estimator__var_smoothing': 2e-09}
2023-04-29 23:48:13,107:INFO:Hyperparameter search completed
2023-04-29 23:48:13,107:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:13,108:INFO:Initializing create_model()
2023-04-29 23:48:13,108:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4065FA0>, model_only=True, return_train_score=False, kwargs={'var_smoothing': 2e-09})
2023-04-29 23:48:13,108:INFO:Checking exceptions
2023-04-29 23:48:13,109:INFO:Importing libraries
2023-04-29 23:48:13,109:INFO:Copying training dataset
2023-04-29 23:48:13,113:INFO:Defining folds
2023-04-29 23:48:13,113:INFO:Declaring metric variables
2023-04-29 23:48:13,119:INFO:Importing untrained model
2023-04-29 23:48:13,119:INFO:Declaring custom model
2023-04-29 23:48:13,124:INFO:Naive Bayes Imported successfully
2023-04-29 23:48:13,138:INFO:Starting cross validation
2023-04-29 23:48:13,139:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:13,366:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:13,383:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:13,466:INFO:Calculating mean and std
2023-04-29 23:48:13,469:INFO:Creating metrics dataframe
2023-04-29 23:48:13,477:INFO:Finalizing model
2023-04-29 23:48:13,524:INFO:Uploading results into container
2023-04-29 23:48:13,525:INFO:Uploading model into container now
2023-04-29 23:48:13,526:INFO:_master_model_container: 19
2023-04-29 23:48:13,526:INFO:_display_container: 6
2023-04-29 23:48:13,526:INFO:GaussianNB(priors=None, var_smoothing=2e-09)
2023-04-29 23:48:13,526:INFO:create_model() successfully completed......................................
2023-04-29 23:48:13,682:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:13,682:INFO:choose_better activated
2023-04-29 23:48:13,688:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:13,689:INFO:Initializing create_model()
2023-04-29 23:48:13,689:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:13,689:INFO:Checking exceptions
2023-04-29 23:48:13,693:INFO:Importing libraries
2023-04-29 23:48:13,693:INFO:Copying training dataset
2023-04-29 23:48:13,698:INFO:Defining folds
2023-04-29 23:48:13,698:INFO:Declaring metric variables
2023-04-29 23:48:13,698:INFO:Importing untrained model
2023-04-29 23:48:13,698:INFO:Declaring custom model
2023-04-29 23:48:13,699:INFO:Naive Bayes Imported successfully
2023-04-29 23:48:13,699:INFO:Starting cross validation
2023-04-29 23:48:13,701:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:13,951:INFO:Calculating mean and std
2023-04-29 23:48:13,951:INFO:Creating metrics dataframe
2023-04-29 23:48:13,955:INFO:Finalizing model
2023-04-29 23:48:14,010:INFO:Uploading results into container
2023-04-29 23:48:14,012:INFO:Uploading model into container now
2023-04-29 23:48:14,013:INFO:_master_model_container: 20
2023-04-29 23:48:14,013:INFO:_display_container: 7
2023-04-29 23:48:14,014:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 23:48:14,014:INFO:create_model() successfully completed......................................
2023-04-29 23:48:14,208:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:14,209:INFO:GaussianNB(priors=None, var_smoothing=1e-09) result for Accuracy is 0.8408
2023-04-29 23:48:14,210:INFO:GaussianNB(priors=None, var_smoothing=2e-09) result for Accuracy is 0.8142
2023-04-29 23:48:14,210:INFO:GaussianNB(priors=None, var_smoothing=1e-09) is best model
2023-04-29 23:48:14,210:INFO:choose_better completed
2023-04-29 23:48:14,211:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 23:48:14,238:INFO:_master_model_container: 20
2023-04-29 23:48:14,238:INFO:_display_container: 6
2023-04-29 23:48:14,239:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 23:48:14,239:INFO:tune_model() successfully completed......................................
2023-04-29 23:48:14,516:INFO:Initializing evaluate_model()
2023-04-29 23:48:14,516:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 23:48:14,570:INFO:Initializing plot_model()
2023-04-29 23:48:14,570:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GaussianNB(priors=None, var_smoothing=1e-09), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, system=True)
2023-04-29 23:48:14,570:INFO:Checking exceptions
2023-04-29 23:48:14,576:INFO:Preloading libraries
2023-04-29 23:48:14,578:INFO:Copying training dataset
2023-04-29 23:48:14,579:INFO:Plot type: pipeline
2023-04-29 23:48:14,992:INFO:Visual Rendered Successfully
2023-04-29 23:48:15,173:INFO:plot_model() successfully completed......................................
2023-04-29 23:48:15,192:INFO:Initializing create_model()
2023-04-29 23:48:15,192:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:15,192:INFO:Checking exceptions
2023-04-29 23:48:15,232:INFO:Importing libraries
2023-04-29 23:48:15,234:INFO:Copying training dataset
2023-04-29 23:48:15,239:INFO:Defining folds
2023-04-29 23:48:15,239:INFO:Declaring metric variables
2023-04-29 23:48:15,245:INFO:Importing untrained model
2023-04-29 23:48:15,251:INFO:Logistic Regression Imported successfully
2023-04-29 23:48:15,265:INFO:Starting cross validation
2023-04-29 23:48:15,267:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:16,863:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:16,881:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:16,904:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:16,922:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:16,926:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:16,987:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:17,055:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:17,109:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:17,221:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:17,382:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:17,497:INFO:Calculating mean and std
2023-04-29 23:48:17,501:INFO:Creating metrics dataframe
2023-04-29 23:48:17,514:INFO:Finalizing model
2023-04-29 23:48:18,448:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:18,458:INFO:Uploading results into container
2023-04-29 23:48:18,461:INFO:Uploading model into container now
2023-04-29 23:48:18,480:INFO:_master_model_container: 21
2023-04-29 23:48:18,480:INFO:_display_container: 7
2023-04-29 23:48:18,482:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 23:48:18,482:INFO:create_model() successfully completed......................................
2023-04-29 23:48:18,673:INFO:Initializing tune_model()
2023-04-29 23:48:18,673:INFO:tune_model(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>)
2023-04-29 23:48:18,673:INFO:Checking exceptions
2023-04-29 23:48:18,727:INFO:Copying training dataset
2023-04-29 23:48:18,732:INFO:Checking base model
2023-04-29 23:48:18,732:INFO:Base model : Logistic Regression
2023-04-29 23:48:18,740:INFO:Declaring metric variables
2023-04-29 23:48:18,748:INFO:Defining Hyperparameters
2023-04-29 23:48:18,915:INFO:Tuning with n_jobs=-1
2023-04-29 23:48:18,915:INFO:Initializing RandomizedSearchCV
2023-04-29 23:48:20,340:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:20,378:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:20,465:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:20,510:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:20,545:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:20,666:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:20,753:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:20,787:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:20,800:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:20,811:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:22,187:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:22,338:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:22,375:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:22,381:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:22,437:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:22,617:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:22,718:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:22,738:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:22,757:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:22,766:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:24,026:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:24,159:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:24,173:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:24,183:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:24,260:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:24,314:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:24,330:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:24,439:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:24,757:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:25,418:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:25,623:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:25,768:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:25,780:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:25,794:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:25,867:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:25,871:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:25,902:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:26,247:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:26,844:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:26,927:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:27,124:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:27,203:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:27,209:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:27,254:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:27,293:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:27,393:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:27,402:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:27,536:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:27,706:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:28,460:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:28,570:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:28,575:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:28,580:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:28,662:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:28,700:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:28,822:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:28,913:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:29,362:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:29,521:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:29,768:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:30,308:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:30,358:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:30,391:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:30,457:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:30,485:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:30,725:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:30,732:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:31,415:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:31,843:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:31,901:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:32,012:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:32,249:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:32,386:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:32,561:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:32,591:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:32,605:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:32,624:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:33,244:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:33,393:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:33,489:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:33,891:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:33,899:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:33,980:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:34,275:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:34,397:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:34,409:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:34,474:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:34,932:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:35,079:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:35,087:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:35,205:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:35,282:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:35,346:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:35,408:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 7.689}
2023-04-29 23:48:35,411:INFO:Hyperparameter search completed
2023-04-29 23:48:35,412:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:35,414:INFO:Initializing create_model()
2023-04-29 23:48:35,414:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD40F6A30>, model_only=True, return_train_score=False, kwargs={'class_weight': {}, 'C': 7.689})
2023-04-29 23:48:35,415:INFO:Checking exceptions
2023-04-29 23:48:35,415:INFO:Importing libraries
2023-04-29 23:48:35,416:INFO:Copying training dataset
2023-04-29 23:48:35,424:INFO:Defining folds
2023-04-29 23:48:35,425:INFO:Declaring metric variables
2023-04-29 23:48:35,434:INFO:Importing untrained model
2023-04-29 23:48:35,434:INFO:Declaring custom model
2023-04-29 23:48:35,445:INFO:Logistic Regression Imported successfully
2023-04-29 23:48:35,467:INFO:Starting cross validation
2023-04-29 23:48:35,470:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:37,239:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:37,268:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:37,278:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:37,331:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:37,454:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:37,477:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:37,525:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:37,579:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:37,722:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:37,728:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:37,852:INFO:Calculating mean and std
2023-04-29 23:48:37,857:INFO:Creating metrics dataframe
2023-04-29 23:48:37,873:INFO:Finalizing model
2023-04-29 23:48:39,096:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:39,114:INFO:Uploading results into container
2023-04-29 23:48:39,117:INFO:Uploading model into container now
2023-04-29 23:48:39,118:INFO:_master_model_container: 22
2023-04-29 23:48:39,118:INFO:_display_container: 8
2023-04-29 23:48:39,119:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 23:48:39,119:INFO:create_model() successfully completed......................................
2023-04-29 23:48:39,334:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:39,334:INFO:choose_better activated
2023-04-29 23:48:39,343:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:39,345:INFO:Initializing create_model()
2023-04-29 23:48:39,345:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:39,345:INFO:Checking exceptions
2023-04-29 23:48:39,348:INFO:Importing libraries
2023-04-29 23:48:39,348:INFO:Copying training dataset
2023-04-29 23:48:39,357:INFO:Defining folds
2023-04-29 23:48:39,358:INFO:Declaring metric variables
2023-04-29 23:48:39,358:INFO:Importing untrained model
2023-04-29 23:48:39,358:INFO:Declaring custom model
2023-04-29 23:48:39,360:INFO:Logistic Regression Imported successfully
2023-04-29 23:48:39,360:INFO:Starting cross validation
2023-04-29 23:48:39,363:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:41,262:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:41,292:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:41,316:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:41,322:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:41,365:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:41,418:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:41,453:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:41,467:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:41,644:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:41,795:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:41,976:INFO:Calculating mean and std
2023-04-29 23:48:41,978:INFO:Creating metrics dataframe
2023-04-29 23:48:41,990:INFO:Finalizing model
2023-04-29 23:48:43,293:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-04-29 23:48:43,296:INFO:Uploading results into container
2023-04-29 23:48:43,297:INFO:Uploading model into container now
2023-04-29 23:48:43,297:INFO:_master_model_container: 23
2023-04-29 23:48:43,297:INFO:_display_container: 9
2023-04-29 23:48:43,298:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 23:48:43,298:INFO:create_model() successfully completed......................................
2023-04-29 23:48:43,467:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:43,469:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.9404
2023-04-29 23:48:43,469:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.9604
2023-04-29 23:48:43,470:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2023-04-29 23:48:43,470:INFO:choose_better completed
2023-04-29 23:48:43,489:INFO:_master_model_container: 23
2023-04-29 23:48:43,489:INFO:_display_container: 8
2023-04-29 23:48:43,490:INFO:LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 23:48:43,490:INFO:tune_model() successfully completed......................................
2023-04-29 23:48:43,724:INFO:Initializing evaluate_model()
2023-04-29 23:48:43,725:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, estimator=LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 23:48:43,765:INFO:Initializing plot_model()
2023-04-29 23:48:43,766:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=7.689, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017ACEFB0280>, system=True)
2023-04-29 23:48:43,766:INFO:Checking exceptions
2023-04-29 23:48:43,769:INFO:Preloading libraries
2023-04-29 23:48:43,770:INFO:Copying training dataset
2023-04-29 23:48:43,770:INFO:Plot type: pipeline
2023-04-29 23:48:44,051:INFO:Visual Rendered Successfully
2023-04-29 23:48:44,238:INFO:plot_model() successfully completed......................................
2023-04-29 23:48:51,076:INFO:PyCaret ClassificationExperiment
2023-04-29 23:48:51,077:INFO:Logging name: clf-default-name
2023-04-29 23:48:51,077:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-04-29 23:48:51,078:INFO:version 3.0.0.rc8
2023-04-29 23:48:51,078:INFO:Initializing setup()
2023-04-29 23:48:51,078:INFO:self.USI: 802d
2023-04-29 23:48:51,080:INFO:self._variable_keys: {'y_test', 'seed', 'X_train', 'pipeline', 'logging_param', 'data', 'fold_groups_param', '_ml_usecase', 'exp_id', 'is_multiclass', 'y_train', 'gpu_n_jobs_param', 'target_param', 'memory', 'fold_shuffle_param', 'y', 'html_param', 'X', 'idx', 'log_plots_param', 'X_test', 'fold_generator', 'exp_name_log', 'USI', 'n_jobs_param', 'fix_imbalance', '_available_plots', 'gpu_param'}
2023-04-29 23:48:51,080:INFO:Checking environment
2023-04-29 23:48:51,080:INFO:python_version: 3.9.13
2023-04-29 23:48:51,081:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-04-29 23:48:51,081:INFO:machine: AMD64
2023-04-29 23:48:51,082:INFO:platform: Windows-10-10.0.22621-SP0
2023-04-29 23:48:51,082:INFO:Memory: svmem(total=16782143488, available=2062643200, percent=87.7, used=14719500288, free=2062643200)
2023-04-29 23:48:51,083:INFO:Physical Core: 10
2023-04-29 23:48:51,083:INFO:Logical Core: 12
2023-04-29 23:48:51,084:INFO:Checking libraries
2023-04-29 23:48:51,084:INFO:System:
2023-04-29 23:48:51,085:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-04-29 23:48:51,085:INFO:executable: C:\Users\eeman\anaconda3\python.exe
2023-04-29 23:48:51,086:INFO:   machine: Windows-10-10.0.22621-SP0
2023-04-29 23:48:51,087:INFO:PyCaret required dependencies:
2023-04-29 23:48:51,087:INFO:                 pip: 22.2.2
2023-04-29 23:48:51,087:INFO:          setuptools: 63.4.1
2023-04-29 23:48:51,087:INFO:             pycaret: 3.0.0rc8
2023-04-29 23:48:51,089:INFO:             IPython: 7.31.1
2023-04-29 23:48:51,089:INFO:          ipywidgets: 7.6.5
2023-04-29 23:48:51,090:INFO:                tqdm: 4.64.1
2023-04-29 23:48:51,090:INFO:               numpy: 1.21.5
2023-04-29 23:48:51,090:INFO:              pandas: 1.4.4
2023-04-29 23:48:51,090:INFO:              jinja2: 2.11.3
2023-04-29 23:48:51,090:INFO:               scipy: 1.9.1
2023-04-29 23:48:51,091:INFO:              joblib: 1.2.0
2023-04-29 23:48:51,091:INFO:             sklearn: 1.0.2
2023-04-29 23:48:51,091:INFO:                pyod: 1.0.7
2023-04-29 23:48:51,091:INFO:            imblearn: 0.10.1
2023-04-29 23:48:51,091:INFO:   category_encoders: 2.6.0
2023-04-29 23:48:51,092:INFO:            lightgbm: 3.3.4
2023-04-29 23:48:51,092:INFO:               numba: 0.55.1
2023-04-29 23:48:51,092:INFO:            requests: 2.28.1
2023-04-29 23:48:51,092:INFO:          matplotlib: 3.5.2
2023-04-29 23:48:51,092:INFO:          scikitplot: 0.3.7
2023-04-29 23:48:51,092:INFO:         yellowbrick: 1.5
2023-04-29 23:48:51,093:INFO:              plotly: 5.9.0
2023-04-29 23:48:51,093:INFO:             kaleido: 0.2.1
2023-04-29 23:48:51,093:INFO:         statsmodels: 0.13.5
2023-04-29 23:48:51,093:INFO:              sktime: 0.15.1
2023-04-29 23:48:51,093:INFO:               tbats: 1.1.2
2023-04-29 23:48:51,094:INFO:            pmdarima: 2.0.2
2023-04-29 23:48:51,094:INFO:              psutil: 5.9.0
2023-04-29 23:48:51,094:INFO:PyCaret optional dependencies:
2023-04-29 23:48:51,094:INFO:                shap: 0.41.0
2023-04-29 23:48:51,094:INFO:           interpret: Not installed
2023-04-29 23:48:51,095:INFO:                umap: Not installed
2023-04-29 23:48:51,095:INFO:    pandas_profiling: Not installed
2023-04-29 23:48:51,095:INFO:  explainerdashboard: Not installed
2023-04-29 23:48:51,095:INFO:             autoviz: Not installed
2023-04-29 23:48:51,095:INFO:           fairlearn: Not installed
2023-04-29 23:48:51,096:INFO:             xgboost: Not installed
2023-04-29 23:48:51,096:INFO:            catboost: Not installed
2023-04-29 23:48:51,096:INFO:              kmodes: Not installed
2023-04-29 23:48:51,096:INFO:             mlxtend: Not installed
2023-04-29 23:48:51,096:INFO:       statsforecast: Not installed
2023-04-29 23:48:51,096:INFO:        tune_sklearn: Not installed
2023-04-29 23:48:51,097:INFO:                 ray: Not installed
2023-04-29 23:48:51,097:INFO:            hyperopt: Not installed
2023-04-29 23:48:51,098:INFO:              optuna: Not installed
2023-04-29 23:48:51,098:INFO:               skopt: Not installed
2023-04-29 23:48:51,098:INFO:              mlflow: Not installed
2023-04-29 23:48:51,100:INFO:              gradio: Not installed
2023-04-29 23:48:51,100:INFO:             fastapi: Not installed
2023-04-29 23:48:51,100:INFO:             uvicorn: Not installed
2023-04-29 23:48:51,100:INFO:              m2cgen: Not installed
2023-04-29 23:48:51,100:INFO:           evidently: Not installed
2023-04-29 23:48:51,100:INFO:                nltk: 3.7
2023-04-29 23:48:51,101:INFO:            pyLDAvis: Not installed
2023-04-29 23:48:51,101:INFO:              gensim: 4.1.2
2023-04-29 23:48:51,101:INFO:               spacy: Not installed
2023-04-29 23:48:51,101:INFO:           wordcloud: Not installed
2023-04-29 23:48:51,102:INFO:            textblob: Not installed
2023-04-29 23:48:51,102:INFO:               fugue: Not installed
2023-04-29 23:48:51,103:INFO:           streamlit: Not installed
2023-04-29 23:48:51,103:INFO:             prophet: Not installed
2023-04-29 23:48:51,104:INFO:None
2023-04-29 23:48:51,105:INFO:Set up data.
2023-04-29 23:48:51,139:INFO:Set up train/test split.
2023-04-29 23:48:51,162:INFO:Set up index.
2023-04-29 23:48:51,163:INFO:Set up folding strategy.
2023-04-29 23:48:51,163:INFO:Assigning column types.
2023-04-29 23:48:51,172:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-29 23:48:51,355:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-29 23:48:51,357:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 23:48:51,491:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:51,492:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:51,812:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-29 23:48:51,818:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 23:48:52,005:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:52,006:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:52,007:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-29 23:48:52,294:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 23:48:52,488:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:52,490:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:52,720:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-04-29 23:48:52,866:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:52,868:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:52,870:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-04-29 23:48:53,227:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:53,227:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:53,658:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:53,661:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:53,667:INFO:Preparing preprocessing pipeline...
2023-04-29 23:48:53,671:INFO:Set up simple imputation.
2023-04-29 23:48:53,678:INFO:Set up encoding of categorical features.
2023-04-29 23:48:53,678:INFO:Set up feature normalization.
2023-04-29 23:48:53,680:INFO:Set up PCA.
2023-04-29 23:48:53,809:INFO:Finished creating preprocessing pipeline.
2023-04-29 23:48:53,835:INFO:Pipeline: Pipeline(memory=Memory(location=C:\Users\eeman\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Temperature', 'L', 'R', 'A_M'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose=0))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    inc...
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('pca',
                 TransformerWrapper(exclude=[], include=None,
                                    transformer=PCA(copy=True,
                                                    iterated_power='auto',
                                                    n_components=3,
                                                    random_state=None,
                                                    svd_solver='auto', tol=0.0,
                                                    whiten=False)))],
         verbose=False)
2023-04-29 23:48:53,836:INFO:Creating final display dataframe.
2023-04-29 23:48:54,600:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target              Type
2                   Target type        Multiclass
3           Original data shape          (216, 7)
4        Transformed data shape          (216, 4)
5   Transformed train set shape          (151, 4)
6    Transformed test set shape           (65, 4)
7              Numeric features                 4
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15                    Normalize              True
16             Normalize method            zscore
17                          PCA              True
18                   PCA method            linear
19               PCA components                 3
20               Fold Generator   StratifiedKFold
21                  Fold Number                10
22                     CPU Jobs                -1
23                      Use GPU             False
24               Log Experiment             False
25              Experiment Name  clf-default-name
26                          USI              802d
2023-04-29 23:48:54,925:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:54,926:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:55,208:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:55,209:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-29 23:48:55,210:INFO:setup() successfully completed in 4.14s...............
2023-04-29 23:48:55,230:INFO:Initializing compare_models()
2023-04-29 23:48:55,230:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-04-29 23:48:55,230:INFO:Checking exceptions
2023-04-29 23:48:55,247:INFO:Preparing display monitor
2023-04-29 23:48:55,373:INFO:Initializing Logistic Regression
2023-04-29 23:48:55,374:INFO:Total runtime is 1.655419667561849e-05 minutes
2023-04-29 23:48:55,387:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:55,388:INFO:Initializing create_model()
2023-04-29 23:48:55,388:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:55,389:INFO:Checking exceptions
2023-04-29 23:48:55,390:INFO:Importing libraries
2023-04-29 23:48:55,390:INFO:Copying training dataset
2023-04-29 23:48:55,400:INFO:Defining folds
2023-04-29 23:48:55,400:INFO:Declaring metric variables
2023-04-29 23:48:55,414:INFO:Importing untrained model
2023-04-29 23:48:55,429:INFO:Logistic Regression Imported successfully
2023-04-29 23:48:55,452:INFO:Starting cross validation
2023-04-29 23:48:55,456:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:56,030:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:56,041:INFO:Calculating mean and std
2023-04-29 23:48:56,042:INFO:Creating metrics dataframe
2023-04-29 23:48:56,047:INFO:Uploading results into container
2023-04-29 23:48:56,048:INFO:Uploading model into container now
2023-04-29 23:48:56,048:INFO:_master_model_container: 1
2023-04-29 23:48:56,048:INFO:_display_container: 2
2023-04-29 23:48:56,049:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 23:48:56,049:INFO:create_model() successfully completed......................................
2023-04-29 23:48:56,231:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:56,232:INFO:Creating metrics dataframe
2023-04-29 23:48:56,248:INFO:Initializing K Neighbors Classifier
2023-04-29 23:48:56,248:INFO:Total runtime is 0.014576522509257 minutes
2023-04-29 23:48:56,253:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:56,255:INFO:Initializing create_model()
2023-04-29 23:48:56,255:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:56,255:INFO:Checking exceptions
2023-04-29 23:48:56,255:INFO:Importing libraries
2023-04-29 23:48:56,255:INFO:Copying training dataset
2023-04-29 23:48:56,259:INFO:Defining folds
2023-04-29 23:48:56,260:INFO:Declaring metric variables
2023-04-29 23:48:56,267:INFO:Importing untrained model
2023-04-29 23:48:56,276:INFO:K Neighbors Classifier Imported successfully
2023-04-29 23:48:56,293:INFO:Starting cross validation
2023-04-29 23:48:56,296:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:56,585:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:48:56,591:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:48:56,601:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:48:56,606:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:48:56,628:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:48:56,636:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:48:56,650:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:48:56,655:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:48:56,673:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:48:56,696:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)

2023-04-29 23:48:56,726:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:56,783:INFO:Calculating mean and std
2023-04-29 23:48:56,785:INFO:Creating metrics dataframe
2023-04-29 23:48:56,792:INFO:Uploading results into container
2023-04-29 23:48:56,793:INFO:Uploading model into container now
2023-04-29 23:48:56,793:INFO:_master_model_container: 2
2023-04-29 23:48:56,793:INFO:_display_container: 2
2023-04-29 23:48:56,794:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-04-29 23:48:56,794:INFO:create_model() successfully completed......................................
2023-04-29 23:48:56,997:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:56,997:INFO:Creating metrics dataframe
2023-04-29 23:48:57,025:INFO:Initializing Naive Bayes
2023-04-29 23:48:57,025:INFO:Total runtime is 0.027523159980773926 minutes
2023-04-29 23:48:57,031:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:57,032:INFO:Initializing create_model()
2023-04-29 23:48:57,032:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:57,032:INFO:Checking exceptions
2023-04-29 23:48:57,032:INFO:Importing libraries
2023-04-29 23:48:57,033:INFO:Copying training dataset
2023-04-29 23:48:57,040:INFO:Defining folds
2023-04-29 23:48:57,041:INFO:Declaring metric variables
2023-04-29 23:48:57,047:INFO:Importing untrained model
2023-04-29 23:48:57,053:INFO:Naive Bayes Imported successfully
2023-04-29 23:48:57,066:INFO:Starting cross validation
2023-04-29 23:48:57,070:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:57,482:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:57,491:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:57,529:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:57,536:INFO:Calculating mean and std
2023-04-29 23:48:57,538:INFO:Creating metrics dataframe
2023-04-29 23:48:57,548:INFO:Uploading results into container
2023-04-29 23:48:57,549:INFO:Uploading model into container now
2023-04-29 23:48:57,551:INFO:_master_model_container: 3
2023-04-29 23:48:57,551:INFO:_display_container: 2
2023-04-29 23:48:57,551:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 23:48:57,551:INFO:create_model() successfully completed......................................
2023-04-29 23:48:57,765:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:57,766:INFO:Creating metrics dataframe
2023-04-29 23:48:57,795:INFO:Initializing Decision Tree Classifier
2023-04-29 23:48:57,795:INFO:Total runtime is 0.04035644133885702 minutes
2023-04-29 23:48:57,806:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:57,807:INFO:Initializing create_model()
2023-04-29 23:48:57,807:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:57,807:INFO:Checking exceptions
2023-04-29 23:48:57,808:INFO:Importing libraries
2023-04-29 23:48:57,808:INFO:Copying training dataset
2023-04-29 23:48:57,818:INFO:Defining folds
2023-04-29 23:48:57,818:INFO:Declaring metric variables
2023-04-29 23:48:57,826:INFO:Importing untrained model
2023-04-29 23:48:57,836:INFO:Decision Tree Classifier Imported successfully
2023-04-29 23:48:57,853:INFO:Starting cross validation
2023-04-29 23:48:57,856:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:58,431:INFO:Calculating mean and std
2023-04-29 23:48:58,434:INFO:Creating metrics dataframe
2023-04-29 23:48:58,442:INFO:Uploading results into container
2023-04-29 23:48:58,443:INFO:Uploading model into container now
2023-04-29 23:48:58,444:INFO:_master_model_container: 4
2023-04-29 23:48:58,444:INFO:_display_container: 2
2023-04-29 23:48:58,445:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=123, splitter='best')
2023-04-29 23:48:58,446:INFO:create_model() successfully completed......................................
2023-04-29 23:48:58,672:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:58,673:INFO:Creating metrics dataframe
2023-04-29 23:48:58,700:INFO:Initializing SVM - Linear Kernel
2023-04-29 23:48:58,700:INFO:Total runtime is 0.05545181830724081 minutes
2023-04-29 23:48:58,708:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:58,710:INFO:Initializing create_model()
2023-04-29 23:48:58,711:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:58,711:INFO:Checking exceptions
2023-04-29 23:48:58,711:INFO:Importing libraries
2023-04-29 23:48:58,711:INFO:Copying training dataset
2023-04-29 23:48:58,719:INFO:Defining folds
2023-04-29 23:48:58,720:INFO:Declaring metric variables
2023-04-29 23:48:58,728:INFO:Importing untrained model
2023-04-29 23:48:58,737:INFO:SVM - Linear Kernel Imported successfully
2023-04-29 23:48:58,756:INFO:Starting cross validation
2023-04-29 23:48:58,760:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:48:59,061:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:59,129:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:59,164:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:59,171:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:59,182:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:59,190:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:59,197:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:59,225:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:48:59,262:INFO:Calculating mean and std
2023-04-29 23:48:59,267:INFO:Creating metrics dataframe
2023-04-29 23:48:59,276:INFO:Uploading results into container
2023-04-29 23:48:59,278:INFO:Uploading model into container now
2023-04-29 23:48:59,279:INFO:_master_model_container: 5
2023-04-29 23:48:59,279:INFO:_display_container: 2
2023-04-29 23:48:59,281:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-04-29 23:48:59,281:INFO:create_model() successfully completed......................................
2023-04-29 23:48:59,547:INFO:SubProcess create_model() end ==================================
2023-04-29 23:48:59,548:INFO:Creating metrics dataframe
2023-04-29 23:48:59,585:INFO:Initializing Ridge Classifier
2023-04-29 23:48:59,586:INFO:Total runtime is 0.07019888559977214 minutes
2023-04-29 23:48:59,597:INFO:SubProcess create_model() called ==================================
2023-04-29 23:48:59,599:INFO:Initializing create_model()
2023-04-29 23:48:59,599:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:48:59,600:INFO:Checking exceptions
2023-04-29 23:48:59,600:INFO:Importing libraries
2023-04-29 23:48:59,600:INFO:Copying training dataset
2023-04-29 23:48:59,613:INFO:Defining folds
2023-04-29 23:48:59,614:INFO:Declaring metric variables
2023-04-29 23:48:59,626:INFO:Importing untrained model
2023-04-29 23:48:59,644:INFO:Ridge Classifier Imported successfully
2023-04-29 23:48:59,674:INFO:Starting cross validation
2023-04-29 23:48:59,679:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:00,006:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:00,038:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:00,072:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:00,081:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:00,084:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:00,096:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:00,109:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:00,115:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:00,124:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:00,133:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:00,141:INFO:Calculating mean and std
2023-04-29 23:49:00,145:INFO:Creating metrics dataframe
2023-04-29 23:49:00,152:INFO:Uploading results into container
2023-04-29 23:49:00,154:INFO:Uploading model into container now
2023-04-29 23:49:00,155:INFO:_master_model_container: 6
2023-04-29 23:49:00,155:INFO:_display_container: 2
2023-04-29 23:49:00,156:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, normalize='deprecated', positive=False,
                random_state=123, solver='auto', tol=0.001)
2023-04-29 23:49:00,156:INFO:create_model() successfully completed......................................
2023-04-29 23:49:00,396:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:00,396:INFO:Creating metrics dataframe
2023-04-29 23:49:00,430:INFO:Initializing Random Forest Classifier
2023-04-29 23:49:00,430:INFO:Total runtime is 0.08427814642588298 minutes
2023-04-29 23:49:00,439:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:00,439:INFO:Initializing create_model()
2023-04-29 23:49:00,440:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:00,440:INFO:Checking exceptions
2023-04-29 23:49:00,440:INFO:Importing libraries
2023-04-29 23:49:00,440:INFO:Copying training dataset
2023-04-29 23:49:00,450:INFO:Defining folds
2023-04-29 23:49:00,450:INFO:Declaring metric variables
2023-04-29 23:49:00,464:INFO:Importing untrained model
2023-04-29 23:49:00,477:INFO:Random Forest Classifier Imported successfully
2023-04-29 23:49:00,509:INFO:Starting cross validation
2023-04-29 23:49:00,514:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:01,980:INFO:Calculating mean and std
2023-04-29 23:49:01,984:INFO:Creating metrics dataframe
2023-04-29 23:49:01,993:INFO:Uploading results into container
2023-04-29 23:49:01,994:INFO:Uploading model into container now
2023-04-29 23:49:01,996:INFO:_master_model_container: 7
2023-04-29 23:49:01,996:INFO:_display_container: 2
2023-04-29 23:49:01,997:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2023-04-29 23:49:01,997:INFO:create_model() successfully completed......................................
2023-04-29 23:49:02,270:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:02,271:INFO:Creating metrics dataframe
2023-04-29 23:49:02,321:INFO:Initializing Quadratic Discriminant Analysis
2023-04-29 23:49:02,322:INFO:Total runtime is 0.11581977208455405 minutes
2023-04-29 23:49:02,333:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:02,335:INFO:Initializing create_model()
2023-04-29 23:49:02,335:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:02,335:INFO:Checking exceptions
2023-04-29 23:49:02,335:INFO:Importing libraries
2023-04-29 23:49:02,335:INFO:Copying training dataset
2023-04-29 23:49:02,345:INFO:Defining folds
2023-04-29 23:49:02,346:INFO:Declaring metric variables
2023-04-29 23:49:02,356:INFO:Importing untrained model
2023-04-29 23:49:02,367:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 23:49:02,391:INFO:Starting cross validation
2023-04-29 23:49:02,395:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:02,567:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:02,581:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:02,638:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:02,643:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:02,696:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:02,716:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:02,723:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:02,752:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:02,769:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:02,775:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:02,860:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:02,911:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:02,953:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:02,960:INFO:Calculating mean and std
2023-04-29 23:49:02,964:INFO:Creating metrics dataframe
2023-04-29 23:49:02,970:INFO:Uploading results into container
2023-04-29 23:49:02,972:INFO:Uploading model into container now
2023-04-29 23:49:02,973:INFO:_master_model_container: 8
2023-04-29 23:49:02,974:INFO:_display_container: 2
2023-04-29 23:49:02,975:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 23:49:02,975:INFO:create_model() successfully completed......................................
2023-04-29 23:49:03,246:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:03,247:INFO:Creating metrics dataframe
2023-04-29 23:49:03,270:INFO:Initializing Ada Boost Classifier
2023-04-29 23:49:03,271:INFO:Total runtime is 0.13163394530614217 minutes
2023-04-29 23:49:03,277:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:03,278:INFO:Initializing create_model()
2023-04-29 23:49:03,278:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:03,278:INFO:Checking exceptions
2023-04-29 23:49:03,278:INFO:Importing libraries
2023-04-29 23:49:03,278:INFO:Copying training dataset
2023-04-29 23:49:03,285:INFO:Defining folds
2023-04-29 23:49:03,285:INFO:Declaring metric variables
2023-04-29 23:49:03,291:INFO:Importing untrained model
2023-04-29 23:49:03,298:INFO:Ada Boost Classifier Imported successfully
2023-04-29 23:49:03,314:INFO:Starting cross validation
2023-04-29 23:49:03,317:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:03,980:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:04,010:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:04,035:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:04,036:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:04,036:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:04,040:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:04,066:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:04,089:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:04,150:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:04,177:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:04,185:INFO:Calculating mean and std
2023-04-29 23:49:04,188:INFO:Creating metrics dataframe
2023-04-29 23:49:04,200:INFO:Uploading results into container
2023-04-29 23:49:04,202:INFO:Uploading model into container now
2023-04-29 23:49:04,203:INFO:_master_model_container: 9
2023-04-29 23:49:04,204:INFO:_display_container: 2
2023-04-29 23:49:04,205:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2023-04-29 23:49:04,206:INFO:create_model() successfully completed......................................
2023-04-29 23:49:04,395:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:04,395:INFO:Creating metrics dataframe
2023-04-29 23:49:04,448:INFO:Initializing Gradient Boosting Classifier
2023-04-29 23:49:04,449:INFO:Total runtime is 0.15125622749328613 minutes
2023-04-29 23:49:04,462:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:04,463:INFO:Initializing create_model()
2023-04-29 23:49:04,463:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:04,464:INFO:Checking exceptions
2023-04-29 23:49:04,464:INFO:Importing libraries
2023-04-29 23:49:04,464:INFO:Copying training dataset
2023-04-29 23:49:04,469:INFO:Defining folds
2023-04-29 23:49:04,470:INFO:Declaring metric variables
2023-04-29 23:49:04,475:INFO:Importing untrained model
2023-04-29 23:49:04,482:INFO:Gradient Boosting Classifier Imported successfully
2023-04-29 23:49:04,493:INFO:Starting cross validation
2023-04-29 23:49:04,495:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:06,893:INFO:Calculating mean and std
2023-04-29 23:49:06,897:INFO:Creating metrics dataframe
2023-04-29 23:49:06,906:INFO:Uploading results into container
2023-04-29 23:49:06,907:INFO:Uploading model into container now
2023-04-29 23:49:06,908:INFO:_master_model_container: 10
2023-04-29 23:49:06,910:INFO:_display_container: 2
2023-04-29 23:49:06,912:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-04-29 23:49:06,912:INFO:create_model() successfully completed......................................
2023-04-29 23:49:07,214:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:07,215:INFO:Creating metrics dataframe
2023-04-29 23:49:07,262:INFO:Initializing Linear Discriminant Analysis
2023-04-29 23:49:07,263:INFO:Total runtime is 0.19814562797546387 minutes
2023-04-29 23:49:07,282:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:07,283:INFO:Initializing create_model()
2023-04-29 23:49:07,284:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:07,284:INFO:Checking exceptions
2023-04-29 23:49:07,285:INFO:Importing libraries
2023-04-29 23:49:07,285:INFO:Copying training dataset
2023-04-29 23:49:07,303:INFO:Defining folds
2023-04-29 23:49:07,304:INFO:Declaring metric variables
2023-04-29 23:49:07,322:INFO:Importing untrained model
2023-04-29 23:49:07,338:INFO:Linear Discriminant Analysis Imported successfully
2023-04-29 23:49:07,369:INFO:Starting cross validation
2023-04-29 23:49:07,373:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:07,820:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:07,853:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:07,883:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:07,917:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:07,934:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:07,944:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:07,946:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:07,961:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:07,972:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:07,990:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:07,996:INFO:Calculating mean and std
2023-04-29 23:49:07,998:INFO:Creating metrics dataframe
2023-04-29 23:49:08,007:INFO:Uploading results into container
2023-04-29 23:49:08,008:INFO:Uploading model into container now
2023-04-29 23:49:08,009:INFO:_master_model_container: 11
2023-04-29 23:49:08,009:INFO:_display_container: 2
2023-04-29 23:49:08,010:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-04-29 23:49:08,010:INFO:create_model() successfully completed......................................
2023-04-29 23:49:08,203:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:08,203:INFO:Creating metrics dataframe
2023-04-29 23:49:08,223:INFO:Initializing Extra Trees Classifier
2023-04-29 23:49:08,224:INFO:Total runtime is 0.2141841491063436 minutes
2023-04-29 23:49:08,232:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:08,232:INFO:Initializing create_model()
2023-04-29 23:49:08,233:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:08,233:INFO:Checking exceptions
2023-04-29 23:49:08,233:INFO:Importing libraries
2023-04-29 23:49:08,233:INFO:Copying training dataset
2023-04-29 23:49:08,243:INFO:Defining folds
2023-04-29 23:49:08,243:INFO:Declaring metric variables
2023-04-29 23:49:08,250:INFO:Importing untrained model
2023-04-29 23:49:08,258:INFO:Extra Trees Classifier Imported successfully
2023-04-29 23:49:08,273:INFO:Starting cross validation
2023-04-29 23:49:08,274:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:09,655:INFO:Calculating mean and std
2023-04-29 23:49:09,658:INFO:Creating metrics dataframe
2023-04-29 23:49:09,664:INFO:Uploading results into container
2023-04-29 23:49:09,666:INFO:Uploading model into container now
2023-04-29 23:49:09,667:INFO:_master_model_container: 12
2023-04-29 23:49:09,667:INFO:_display_container: 2
2023-04-29 23:49:09,668:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 23:49:09,668:INFO:create_model() successfully completed......................................
2023-04-29 23:49:09,848:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:09,848:INFO:Creating metrics dataframe
2023-04-29 23:49:09,872:INFO:Initializing Light Gradient Boosting Machine
2023-04-29 23:49:09,872:INFO:Total runtime is 0.24163791735967 minutes
2023-04-29 23:49:09,875:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:09,877:INFO:Initializing create_model()
2023-04-29 23:49:09,877:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:09,877:INFO:Checking exceptions
2023-04-29 23:49:09,878:INFO:Importing libraries
2023-04-29 23:49:09,878:INFO:Copying training dataset
2023-04-29 23:49:09,885:INFO:Defining folds
2023-04-29 23:49:09,885:INFO:Declaring metric variables
2023-04-29 23:49:09,893:INFO:Importing untrained model
2023-04-29 23:49:09,900:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 23:49:09,916:INFO:Starting cross validation
2023-04-29 23:49:09,919:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:12,335:INFO:Calculating mean and std
2023-04-29 23:49:12,339:INFO:Creating metrics dataframe
2023-04-29 23:49:12,346:INFO:Uploading results into container
2023-04-29 23:49:12,348:INFO:Uploading model into container now
2023-04-29 23:49:12,348:INFO:_master_model_container: 13
2023-04-29 23:49:12,348:INFO:_display_container: 2
2023-04-29 23:49:12,350:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 23:49:12,350:INFO:create_model() successfully completed......................................
2023-04-29 23:49:12,527:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:12,528:INFO:Creating metrics dataframe
2023-04-29 23:49:12,549:INFO:Initializing Dummy Classifier
2023-04-29 23:49:12,549:INFO:Total runtime is 0.2862561424573262 minutes
2023-04-29 23:49:12,554:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:12,554:INFO:Initializing create_model()
2023-04-29 23:49:12,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4262160>, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:12,554:INFO:Checking exceptions
2023-04-29 23:49:12,554:INFO:Importing libraries
2023-04-29 23:49:12,554:INFO:Copying training dataset
2023-04-29 23:49:12,565:INFO:Defining folds
2023-04-29 23:49:12,565:INFO:Declaring metric variables
2023-04-29 23:49:12,572:INFO:Importing untrained model
2023-04-29 23:49:12,580:INFO:Dummy Classifier Imported successfully
2023-04-29 23:49:12,594:INFO:Starting cross validation
2023-04-29 23:49:12,597:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:13,107:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:13,133:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:13,135:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:13,157:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:13,167:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:13,177:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:13,193:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:13,223:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:13,264:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:13,272:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:13,284:INFO:Calculating mean and std
2023-04-29 23:49:13,289:INFO:Creating metrics dataframe
2023-04-29 23:49:13,298:INFO:Uploading results into container
2023-04-29 23:49:13,300:INFO:Uploading model into container now
2023-04-29 23:49:13,302:INFO:_master_model_container: 14
2023-04-29 23:49:13,302:INFO:_display_container: 2
2023-04-29 23:49:13,302:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2023-04-29 23:49:13,303:INFO:create_model() successfully completed......................................
2023-04-29 23:49:13,559:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:13,561:INFO:Creating metrics dataframe
2023-04-29 23:49:13,637:INFO:Initializing create_model()
2023-04-29 23:49:13,637:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:13,638:INFO:Checking exceptions
2023-04-29 23:49:13,642:INFO:Importing libraries
2023-04-29 23:49:13,643:INFO:Copying training dataset
2023-04-29 23:49:13,648:INFO:Defining folds
2023-04-29 23:49:13,650:INFO:Declaring metric variables
2023-04-29 23:49:13,650:INFO:Importing untrained model
2023-04-29 23:49:13,650:INFO:Declaring custom model
2023-04-29 23:49:13,652:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 23:49:13,655:INFO:Cross validation set to False
2023-04-29 23:49:13,655:INFO:Fitting Model
2023-04-29 23:49:14,445:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 23:49:14,445:INFO:create_model() successfully completed......................................
2023-04-29 23:49:14,704:INFO:_master_model_container: 14
2023-04-29 23:49:14,704:INFO:_display_container: 2
2023-04-29 23:49:14,705:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 23:49:14,705:INFO:compare_models() successfully completed......................................
2023-04-29 23:49:14,741:INFO:Initializing tune_model()
2023-04-29 23:49:14,741:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>)
2023-04-29 23:49:14,741:INFO:Checking exceptions
2023-04-29 23:49:14,793:INFO:Copying training dataset
2023-04-29 23:49:14,798:INFO:Checking base model
2023-04-29 23:49:14,798:INFO:Base model : Light Gradient Boosting Machine
2023-04-29 23:49:14,807:INFO:Declaring metric variables
2023-04-29 23:49:14,815:INFO:Defining Hyperparameters
2023-04-29 23:49:15,037:INFO:Tuning with n_jobs=-1
2023-04-29 23:49:15,037:INFO:Initializing RandomizedSearchCV
2023-04-29 23:49:20,782:INFO:best_params: {'actual_estimator__reg_lambda': 0.0005, 'actual_estimator__reg_alpha': 0.005, 'actual_estimator__num_leaves': 150, 'actual_estimator__n_estimators': 20, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.4, 'actual_estimator__feature_fraction': 0.5, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.9}
2023-04-29 23:49:20,784:INFO:Hyperparameter search completed
2023-04-29 23:49:20,784:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:20,786:INFO:Initializing create_model()
2023-04-29 23:49:20,786:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4071DF0>, model_only=True, return_train_score=False, kwargs={'reg_lambda': 0.0005, 'reg_alpha': 0.005, 'num_leaves': 150, 'n_estimators': 20, 'min_split_gain': 0.3, 'min_child_samples': 6, 'learning_rate': 0.4, 'feature_fraction': 0.5, 'bagging_freq': 3, 'bagging_fraction': 0.9})
2023-04-29 23:49:20,787:INFO:Checking exceptions
2023-04-29 23:49:20,787:INFO:Importing libraries
2023-04-29 23:49:20,787:INFO:Copying training dataset
2023-04-29 23:49:20,791:INFO:Defining folds
2023-04-29 23:49:20,792:INFO:Declaring metric variables
2023-04-29 23:49:20,797:INFO:Importing untrained model
2023-04-29 23:49:20,798:INFO:Declaring custom model
2023-04-29 23:49:20,803:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 23:49:20,814:INFO:Starting cross validation
2023-04-29 23:49:20,817:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:21,648:INFO:Calculating mean and std
2023-04-29 23:49:21,651:INFO:Creating metrics dataframe
2023-04-29 23:49:21,664:INFO:Finalizing model
2023-04-29 23:49:21,856:INFO:Uploading results into container
2023-04-29 23:49:21,858:INFO:Uploading model into container now
2023-04-29 23:49:21,859:INFO:_master_model_container: 15
2023-04-29 23:49:21,859:INFO:_display_container: 3
2023-04-29 23:49:21,860:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.4, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=20, n_jobs=-1, num_leaves=150, objective=None,
               random_state=123, reg_alpha=0.005, reg_lambda=0.0005,
               silent='warn', subsample=1.0, subsample_for_bin=200000,
               subsample_freq=0)
2023-04-29 23:49:21,860:INFO:create_model() successfully completed......................................
2023-04-29 23:49:22,091:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:22,091:INFO:choose_better activated
2023-04-29 23:49:22,099:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:22,103:INFO:Initializing create_model()
2023-04-29 23:49:22,104:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:22,105:INFO:Checking exceptions
2023-04-29 23:49:22,111:INFO:Importing libraries
2023-04-29 23:49:22,111:INFO:Copying training dataset
2023-04-29 23:49:22,117:INFO:Defining folds
2023-04-29 23:49:22,117:INFO:Declaring metric variables
2023-04-29 23:49:22,118:INFO:Importing untrained model
2023-04-29 23:49:22,118:INFO:Declaring custom model
2023-04-29 23:49:22,120:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-29 23:49:22,120:INFO:Starting cross validation
2023-04-29 23:49:22,122:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:22,836:INFO:Calculating mean and std
2023-04-29 23:49:22,837:INFO:Creating metrics dataframe
2023-04-29 23:49:22,841:INFO:Finalizing model
2023-04-29 23:49:23,505:INFO:Uploading results into container
2023-04-29 23:49:23,506:INFO:Uploading model into container now
2023-04-29 23:49:23,507:INFO:_master_model_container: 16
2023-04-29 23:49:23,508:INFO:_display_container: 4
2023-04-29 23:49:23,509:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 23:49:23,509:INFO:create_model() successfully completed......................................
2023-04-29 23:49:23,702:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:23,703:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9408
2023-04-29 23:49:23,705:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.4, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=20, n_jobs=-1, num_leaves=150, objective=None,
               random_state=123, reg_alpha=0.005, reg_lambda=0.0005,
               silent='warn', subsample=1.0, subsample_for_bin=200000,
               subsample_freq=0) result for Accuracy is 0.9271
2023-04-29 23:49:23,705:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2023-04-29 23:49:23,706:INFO:choose_better completed
2023-04-29 23:49:23,706:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 23:49:23,725:INFO:_master_model_container: 16
2023-04-29 23:49:23,725:INFO:_display_container: 3
2023-04-29 23:49:23,726:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-04-29 23:49:23,726:INFO:tune_model() successfully completed......................................
2023-04-29 23:49:23,912:INFO:Initializing evaluate_model()
2023-04-29 23:49:23,913:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 23:49:23,949:INFO:Initializing plot_model()
2023-04-29 23:49:23,949:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, system=True)
2023-04-29 23:49:23,949:INFO:Checking exceptions
2023-04-29 23:49:23,953:INFO:Preloading libraries
2023-04-29 23:49:24,043:INFO:Copying training dataset
2023-04-29 23:49:24,043:INFO:Plot type: pipeline
2023-04-29 23:49:24,323:INFO:Visual Rendered Successfully
2023-04-29 23:49:24,533:INFO:plot_model() successfully completed......................................
2023-04-29 23:49:24,549:INFO:Initializing create_model()
2023-04-29 23:49:24,550:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=qda, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:24,550:INFO:Checking exceptions
2023-04-29 23:49:24,605:INFO:Importing libraries
2023-04-29 23:49:24,605:INFO:Copying training dataset
2023-04-29 23:49:24,610:INFO:Defining folds
2023-04-29 23:49:24,611:INFO:Declaring metric variables
2023-04-29 23:49:24,618:INFO:Importing untrained model
2023-04-29 23:49:24,627:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 23:49:24,642:INFO:Starting cross validation
2023-04-29 23:49:24,644:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:24,775:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:24,793:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:24,825:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:24,853:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:24,879:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:24,957:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:24,978:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:25,013:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:25,078:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:25,138:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:25,211:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:25,353:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:25,388:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:25,394:INFO:Calculating mean and std
2023-04-29 23:49:25,394:INFO:Creating metrics dataframe
2023-04-29 23:49:25,404:INFO:Finalizing model
2023-04-29 23:49:25,474:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:25,486:INFO:Uploading results into container
2023-04-29 23:49:25,488:INFO:Uploading model into container now
2023-04-29 23:49:25,514:INFO:_master_model_container: 17
2023-04-29 23:49:25,514:INFO:_display_container: 4
2023-04-29 23:49:25,515:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 23:49:25,515:INFO:create_model() successfully completed......................................
2023-04-29 23:49:25,749:INFO:Initializing tune_model()
2023-04-29 23:49:25,749:INFO:tune_model(estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>)
2023-04-29 23:49:25,750:INFO:Checking exceptions
2023-04-29 23:49:25,811:INFO:Copying training dataset
2023-04-29 23:49:25,822:INFO:Checking base model
2023-04-29 23:49:25,823:INFO:Base model : Quadratic Discriminant Analysis
2023-04-29 23:49:25,832:INFO:Declaring metric variables
2023-04-29 23:49:25,840:INFO:Defining Hyperparameters
2023-04-29 23:49:26,044:INFO:Tuning with n_jobs=-1
2023-04-29 23:49:26,045:INFO:Initializing RandomizedSearchCV
2023-04-29 23:49:26,230:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,291:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,313:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,356:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,431:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,494:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,550:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,585:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,631:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,651:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,800:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,809:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,880:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,884:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,888:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,943:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,962:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,964:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:26,976:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,003:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,118:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,143:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,225:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,255:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,272:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,340:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,377:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,463:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,474:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,504:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,616:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,637:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,693:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,713:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,756:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,790:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,814:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,825:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:27,958:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,014:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,024:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,100:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,129:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,222:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,253:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,282:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,308:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,323:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,330:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,451:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,574:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,596:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,684:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,713:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,755:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,790:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,848:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,868:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,883:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,939:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:28,980:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,042:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,076:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,134:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,152:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,204:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,270:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,387:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,399:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,406:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,433:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,472:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,526:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,664:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,665:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,766:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,798:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,826:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,923:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:29,999:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,036:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,093:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,094:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,218:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,233:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,286:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,326:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,433:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,499:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,513:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,699:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,708:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,770:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,787:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,790:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,824:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,843:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,855:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,866:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,876:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:30,970:INFO:best_params: {'actual_estimator__reg_param': 0.17}
2023-04-29 23:49:30,972:INFO:Hyperparameter search completed
2023-04-29 23:49:30,973:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:30,974:INFO:Initializing create_model()
2023-04-29 23:49:30,974:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017ACF628340>, model_only=True, return_train_score=False, kwargs={'reg_param': 0.17})
2023-04-29 23:49:30,974:INFO:Checking exceptions
2023-04-29 23:49:30,974:INFO:Importing libraries
2023-04-29 23:49:30,975:INFO:Copying training dataset
2023-04-29 23:49:30,979:INFO:Defining folds
2023-04-29 23:49:30,979:INFO:Declaring metric variables
2023-04-29 23:49:30,985:INFO:Importing untrained model
2023-04-29 23:49:30,985:INFO:Declaring custom model
2023-04-29 23:49:30,990:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 23:49:31,000:INFO:Starting cross validation
2023-04-29 23:49:31,003:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:31,144:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,172:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,214:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,222:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,267:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,283:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,288:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,360:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,374:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,416:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,502:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:31,515:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:31,540:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:31,558:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:31,608:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:31,655:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:31,670:INFO:Calculating mean and std
2023-04-29 23:49:31,673:INFO:Creating metrics dataframe
2023-04-29 23:49:31,683:INFO:Finalizing model
2023-04-29 23:49:31,766:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:31,781:INFO:Uploading results into container
2023-04-29 23:49:31,784:INFO:Uploading model into container now
2023-04-29 23:49:31,784:INFO:_master_model_container: 18
2023-04-29 23:49:31,785:INFO:_display_container: 5
2023-04-29 23:49:31,785:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.17,
                              store_covariance=False, tol=0.0001)
2023-04-29 23:49:31,786:INFO:create_model() successfully completed......................................
2023-04-29 23:49:32,000:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:32,000:INFO:choose_better activated
2023-04-29 23:49:32,007:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:32,008:INFO:Initializing create_model()
2023-04-29 23:49:32,008:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:32,008:INFO:Checking exceptions
2023-04-29 23:49:32,012:INFO:Importing libraries
2023-04-29 23:49:32,012:INFO:Copying training dataset
2023-04-29 23:49:32,016:INFO:Defining folds
2023-04-29 23:49:32,016:INFO:Declaring metric variables
2023-04-29 23:49:32,017:INFO:Importing untrained model
2023-04-29 23:49:32,017:INFO:Declaring custom model
2023-04-29 23:49:32,017:INFO:Quadratic Discriminant Analysis Imported successfully
2023-04-29 23:49:32,018:INFO:Starting cross validation
2023-04-29 23:49:32,020:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:32,173:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,191:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,235:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,249:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,289:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,324:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,348:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,360:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,373:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,384:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,474:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:32,533:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:32,556:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:32,580:INFO:Calculating mean and std
2023-04-29 23:49:32,582:INFO:Creating metrics dataframe
2023-04-29 23:49:32,586:INFO:Finalizing model
2023-04-29 23:49:32,644:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:878: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-04-29 23:49:32,647:INFO:Uploading results into container
2023-04-29 23:49:32,648:INFO:Uploading model into container now
2023-04-29 23:49:32,649:INFO:_master_model_container: 19
2023-04-29 23:49:32,649:INFO:_display_container: 6
2023-04-29 23:49:32,649:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 23:49:32,649:INFO:create_model() successfully completed......................................
2023-04-29 23:49:32,828:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:32,829:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001) result for Accuracy is 0.8154
2023-04-29 23:49:32,830:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.17,
                              store_covariance=False, tol=0.0001) result for Accuracy is 0.7096
2023-04-29 23:49:32,831:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001) is best model
2023-04-29 23:49:32,831:INFO:choose_better completed
2023-04-29 23:49:32,831:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 23:49:32,846:INFO:_master_model_container: 19
2023-04-29 23:49:32,846:INFO:_display_container: 5
2023-04-29 23:49:32,846:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-04-29 23:49:32,847:INFO:tune_model() successfully completed......................................
2023-04-29 23:49:33,075:INFO:Initializing evaluate_model()
2023-04-29 23:49:33,076:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 23:49:33,122:INFO:Initializing plot_model()
2023-04-29 23:49:33,123:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, system=True)
2023-04-29 23:49:33,123:INFO:Checking exceptions
2023-04-29 23:49:33,126:INFO:Preloading libraries
2023-04-29 23:49:33,127:INFO:Copying training dataset
2023-04-29 23:49:33,127:INFO:Plot type: pipeline
2023-04-29 23:49:33,385:INFO:Visual Rendered Successfully
2023-04-29 23:49:33,594:INFO:plot_model() successfully completed......................................
2023-04-29 23:49:33,621:INFO:Initializing create_model()
2023-04-29 23:49:33,621:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=nb, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:33,622:INFO:Checking exceptions
2023-04-29 23:49:33,678:INFO:Importing libraries
2023-04-29 23:49:33,678:INFO:Copying training dataset
2023-04-29 23:49:33,687:INFO:Defining folds
2023-04-29 23:49:33,689:INFO:Declaring metric variables
2023-04-29 23:49:33,698:INFO:Importing untrained model
2023-04-29 23:49:33,706:INFO:Naive Bayes Imported successfully
2023-04-29 23:49:33,724:INFO:Starting cross validation
2023-04-29 23:49:33,726:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:34,442:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:34,457:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:34,458:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:34,504:INFO:Calculating mean and std
2023-04-29 23:49:34,505:INFO:Creating metrics dataframe
2023-04-29 23:49:34,514:INFO:Finalizing model
2023-04-29 23:49:34,588:INFO:Uploading results into container
2023-04-29 23:49:34,590:INFO:Uploading model into container now
2023-04-29 23:49:34,619:INFO:_master_model_container: 20
2023-04-29 23:49:34,620:INFO:_display_container: 6
2023-04-29 23:49:34,621:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 23:49:34,621:INFO:create_model() successfully completed......................................
2023-04-29 23:49:34,836:INFO:Initializing tune_model()
2023-04-29 23:49:34,837:INFO:tune_model(estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>)
2023-04-29 23:49:34,837:INFO:Checking exceptions
2023-04-29 23:49:34,890:INFO:Copying training dataset
2023-04-29 23:49:34,898:INFO:Checking base model
2023-04-29 23:49:34,899:INFO:Base model : Naive Bayes
2023-04-29 23:49:34,908:INFO:Declaring metric variables
2023-04-29 23:49:34,915:INFO:Defining Hyperparameters
2023-04-29 23:49:35,114:INFO:Tuning with n_jobs=-1
2023-04-29 23:49:35,114:INFO:Initializing RandomizedSearchCV
2023-04-29 23:49:40,152:INFO:best_params: {'actual_estimator__var_smoothing': 2e-09}
2023-04-29 23:49:40,154:INFO:Hyperparameter search completed
2023-04-29 23:49:40,155:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:40,156:INFO:Initializing create_model()
2023-04-29 23:49:40,156:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD412D490>, model_only=True, return_train_score=False, kwargs={'var_smoothing': 2e-09})
2023-04-29 23:49:40,157:INFO:Checking exceptions
2023-04-29 23:49:40,157:INFO:Importing libraries
2023-04-29 23:49:40,157:INFO:Copying training dataset
2023-04-29 23:49:40,162:INFO:Defining folds
2023-04-29 23:49:40,163:INFO:Declaring metric variables
2023-04-29 23:49:40,170:INFO:Importing untrained model
2023-04-29 23:49:40,171:INFO:Declaring custom model
2023-04-29 23:49:40,178:INFO:Naive Bayes Imported successfully
2023-04-29 23:49:40,197:INFO:Starting cross validation
2023-04-29 23:49:40,202:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:41,214:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:41,227:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:41,246:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:41,280:INFO:Calculating mean and std
2023-04-29 23:49:41,283:INFO:Creating metrics dataframe
2023-04-29 23:49:41,297:INFO:Finalizing model
2023-04-29 23:49:41,387:INFO:Uploading results into container
2023-04-29 23:49:41,388:INFO:Uploading model into container now
2023-04-29 23:49:41,390:INFO:_master_model_container: 21
2023-04-29 23:49:41,390:INFO:_display_container: 7
2023-04-29 23:49:41,391:INFO:GaussianNB(priors=None, var_smoothing=2e-09)
2023-04-29 23:49:41,391:INFO:create_model() successfully completed......................................
2023-04-29 23:49:41,603:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:41,603:INFO:choose_better activated
2023-04-29 23:49:41,610:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:41,611:INFO:Initializing create_model()
2023-04-29 23:49:41,612:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:41,612:INFO:Checking exceptions
2023-04-29 23:49:41,615:INFO:Importing libraries
2023-04-29 23:49:41,615:INFO:Copying training dataset
2023-04-29 23:49:41,621:INFO:Defining folds
2023-04-29 23:49:41,621:INFO:Declaring metric variables
2023-04-29 23:49:41,622:INFO:Importing untrained model
2023-04-29 23:49:41,622:INFO:Declaring custom model
2023-04-29 23:49:41,623:INFO:Naive Bayes Imported successfully
2023-04-29 23:49:41,623:INFO:Starting cross validation
2023-04-29 23:49:41,626:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:42,296:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:42,364:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:42,408:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:42,457:INFO:Calculating mean and std
2023-04-29 23:49:42,458:INFO:Creating metrics dataframe
2023-04-29 23:49:42,463:INFO:Finalizing model
2023-04-29 23:49:42,566:INFO:Uploading results into container
2023-04-29 23:49:42,567:INFO:Uploading model into container now
2023-04-29 23:49:42,568:INFO:_master_model_container: 22
2023-04-29 23:49:42,568:INFO:_display_container: 8
2023-04-29 23:49:42,568:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 23:49:42,568:INFO:create_model() successfully completed......................................
2023-04-29 23:49:42,782:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:42,782:INFO:GaussianNB(priors=None, var_smoothing=1e-09) result for Accuracy is 0.7683
2023-04-29 23:49:42,783:INFO:GaussianNB(priors=None, var_smoothing=2e-09) result for Accuracy is 0.7683
2023-04-29 23:49:42,783:INFO:GaussianNB(priors=None, var_smoothing=1e-09) is best model
2023-04-29 23:49:42,783:INFO:choose_better completed
2023-04-29 23:49:42,784:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 23:49:42,806:INFO:_master_model_container: 22
2023-04-29 23:49:42,806:INFO:_display_container: 7
2023-04-29 23:49:42,806:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-04-29 23:49:42,807:INFO:tune_model() successfully completed......................................
2023-04-29 23:49:43,082:INFO:Initializing evaluate_model()
2023-04-29 23:49:43,082:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 23:49:43,140:INFO:Initializing plot_model()
2023-04-29 23:49:43,141:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GaussianNB(priors=None, var_smoothing=1e-09), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, system=True)
2023-04-29 23:49:43,141:INFO:Checking exceptions
2023-04-29 23:49:43,146:INFO:Preloading libraries
2023-04-29 23:49:43,148:INFO:Copying training dataset
2023-04-29 23:49:43,148:INFO:Plot type: pipeline
2023-04-29 23:49:43,688:INFO:Visual Rendered Successfully
2023-04-29 23:49:44,139:INFO:plot_model() successfully completed......................................
2023-04-29 23:49:44,205:INFO:Initializing create_model()
2023-04-29 23:49:44,205:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:44,206:INFO:Checking exceptions
2023-04-29 23:49:44,549:INFO:Importing libraries
2023-04-29 23:49:44,549:INFO:Copying training dataset
2023-04-29 23:49:44,577:INFO:Defining folds
2023-04-29 23:49:44,579:INFO:Declaring metric variables
2023-04-29 23:49:44,614:INFO:Importing untrained model
2023-04-29 23:49:44,648:INFO:Logistic Regression Imported successfully
2023-04-29 23:49:44,713:INFO:Starting cross validation
2023-04-29 23:49:44,724:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:46,402:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:46,430:INFO:Calculating mean and std
2023-04-29 23:49:46,439:INFO:Creating metrics dataframe
2023-04-29 23:49:46,463:INFO:Finalizing model
2023-04-29 23:49:46,664:INFO:Uploading results into container
2023-04-29 23:49:46,665:INFO:Uploading model into container now
2023-04-29 23:49:46,688:INFO:_master_model_container: 23
2023-04-29 23:49:46,688:INFO:_display_container: 8
2023-04-29 23:49:46,690:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 23:49:46,690:INFO:create_model() successfully completed......................................
2023-04-29 23:49:46,952:INFO:Initializing tune_model()
2023-04-29 23:49:46,953:INFO:tune_model(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>)
2023-04-29 23:49:46,953:INFO:Checking exceptions
2023-04-29 23:49:47,037:INFO:Copying training dataset
2023-04-29 23:49:47,046:INFO:Checking base model
2023-04-29 23:49:47,046:INFO:Base model : Logistic Regression
2023-04-29 23:49:47,057:INFO:Declaring metric variables
2023-04-29 23:49:47,068:INFO:Defining Hyperparameters
2023-04-29 23:49:47,343:INFO:Tuning with n_jobs=-1
2023-04-29 23:49:47,343:INFO:Initializing RandomizedSearchCV
2023-04-29 23:49:56,218:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 3.882}
2023-04-29 23:49:56,219:INFO:Hyperparameter search completed
2023-04-29 23:49:56,220:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:56,221:INFO:Initializing create_model()
2023-04-29 23:49:56,221:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD4122CD0>, model_only=True, return_train_score=False, kwargs={'class_weight': {}, 'C': 3.882})
2023-04-29 23:49:56,222:INFO:Checking exceptions
2023-04-29 23:49:56,223:INFO:Importing libraries
2023-04-29 23:49:56,224:INFO:Copying training dataset
2023-04-29 23:49:56,229:INFO:Defining folds
2023-04-29 23:49:56,229:INFO:Declaring metric variables
2023-04-29 23:49:56,236:INFO:Importing untrained model
2023-04-29 23:49:56,237:INFO:Declaring custom model
2023-04-29 23:49:56,243:INFO:Logistic Regression Imported successfully
2023-04-29 23:49:56,261:INFO:Starting cross validation
2023-04-29 23:49:56,265:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:57,471:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:57,542:INFO:Calculating mean and std
2023-04-29 23:49:57,552:INFO:Creating metrics dataframe
2023-04-29 23:49:57,565:INFO:Finalizing model
2023-04-29 23:49:57,762:INFO:Uploading results into container
2023-04-29 23:49:57,764:INFO:Uploading model into container now
2023-04-29 23:49:57,766:INFO:_master_model_container: 24
2023-04-29 23:49:57,767:INFO:_display_container: 9
2023-04-29 23:49:57,768:INFO:LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 23:49:57,770:INFO:create_model() successfully completed......................................
2023-04-29 23:49:58,017:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:58,017:INFO:choose_better activated
2023-04-29 23:49:58,027:INFO:SubProcess create_model() called ==================================
2023-04-29 23:49:58,028:INFO:Initializing create_model()
2023-04-29 23:49:58,028:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:49:58,028:INFO:Checking exceptions
2023-04-29 23:49:58,031:INFO:Importing libraries
2023-04-29 23:49:58,031:INFO:Copying training dataset
2023-04-29 23:49:58,038:INFO:Defining folds
2023-04-29 23:49:58,038:INFO:Declaring metric variables
2023-04-29 23:49:58,038:INFO:Importing untrained model
2023-04-29 23:49:58,038:INFO:Declaring custom model
2023-04-29 23:49:58,040:INFO:Logistic Regression Imported successfully
2023-04-29 23:49:58,041:INFO:Starting cross validation
2023-04-29 23:49:58,044:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:49:59,097:WARNING:C:\Users\eeman\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-04-29 23:49:59,118:INFO:Calculating mean and std
2023-04-29 23:49:59,119:INFO:Creating metrics dataframe
2023-04-29 23:49:59,122:INFO:Finalizing model
2023-04-29 23:49:59,241:INFO:Uploading results into container
2023-04-29 23:49:59,242:INFO:Uploading model into container now
2023-04-29 23:49:59,242:INFO:_master_model_container: 25
2023-04-29 23:49:59,242:INFO:_display_container: 10
2023-04-29 23:49:59,243:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 23:49:59,243:INFO:create_model() successfully completed......................................
2023-04-29 23:49:59,463:INFO:SubProcess create_model() end ==================================
2023-04-29 23:49:59,465:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.7288
2023-04-29 23:49:59,466:INFO:LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.7888
2023-04-29 23:49:59,467:INFO:LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2023-04-29 23:49:59,467:INFO:choose_better completed
2023-04-29 23:49:59,497:INFO:_master_model_container: 25
2023-04-29 23:49:59,499:INFO:_display_container: 9
2023-04-29 23:49:59,501:INFO:LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-04-29 23:49:59,501:INFO:tune_model() successfully completed......................................
2023-04-29 23:49:59,768:INFO:Initializing evaluate_model()
2023-04-29 23:49:59,769:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 23:49:59,807:INFO:Initializing plot_model()
2023-04-29 23:49:59,807:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=LogisticRegression(C=3.882, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, system=True)
2023-04-29 23:49:59,807:INFO:Checking exceptions
2023-04-29 23:49:59,811:INFO:Preloading libraries
2023-04-29 23:49:59,812:INFO:Copying training dataset
2023-04-29 23:49:59,812:INFO:Plot type: pipeline
2023-04-29 23:50:00,300:INFO:Visual Rendered Successfully
2023-04-29 23:50:00,555:INFO:plot_model() successfully completed......................................
2023-04-29 23:50:00,585:INFO:Initializing create_model()
2023-04-29 23:50:00,586:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=et, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:50:00,586:INFO:Checking exceptions
2023-04-29 23:50:00,653:INFO:Importing libraries
2023-04-29 23:50:00,653:INFO:Copying training dataset
2023-04-29 23:50:00,661:INFO:Defining folds
2023-04-29 23:50:00,662:INFO:Declaring metric variables
2023-04-29 23:50:00,670:INFO:Importing untrained model
2023-04-29 23:50:00,679:INFO:Extra Trees Classifier Imported successfully
2023-04-29 23:50:00,699:INFO:Starting cross validation
2023-04-29 23:50:00,703:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:50:03,141:INFO:Calculating mean and std
2023-04-29 23:50:03,145:INFO:Creating metrics dataframe
2023-04-29 23:50:03,156:INFO:Finalizing model
2023-04-29 23:50:03,545:INFO:Uploading results into container
2023-04-29 23:50:03,547:INFO:Uploading model into container now
2023-04-29 23:50:03,570:INFO:_master_model_container: 26
2023-04-29 23:50:03,571:INFO:_display_container: 10
2023-04-29 23:50:03,572:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 23:50:03,573:INFO:create_model() successfully completed......................................
2023-04-29 23:50:03,848:INFO:Initializing tune_model()
2023-04-29 23:50:03,850:INFO:tune_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>)
2023-04-29 23:50:03,850:INFO:Checking exceptions
2023-04-29 23:50:03,915:INFO:Copying training dataset
2023-04-29 23:50:03,921:INFO:Checking base model
2023-04-29 23:50:03,922:INFO:Base model : Extra Trees Classifier
2023-04-29 23:50:03,930:INFO:Declaring metric variables
2023-04-29 23:50:03,939:INFO:Defining Hyperparameters
2023-04-29 23:50:04,157:INFO:Tuning with n_jobs=-1
2023-04-29 23:50:04,158:INFO:Initializing RandomizedSearchCV
2023-04-29 23:50:32,540:INFO:best_params: {'actual_estimator__n_estimators': 200, 'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 6, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': 'balanced_subsample', 'actual_estimator__bootstrap': False}
2023-04-29 23:50:32,543:INFO:Hyperparameter search completed
2023-04-29 23:50:32,544:INFO:SubProcess create_model() called ==================================
2023-04-29 23:50:32,546:INFO:Initializing create_model()
2023-04-29 23:50:32,547:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017AD423C310>, model_only=True, return_train_score=False, kwargs={'n_estimators': 200, 'min_samples_split': 7, 'min_samples_leaf': 4, 'min_impurity_decrease': 0, 'max_features': 1.0, 'max_depth': 6, 'criterion': 'gini', 'class_weight': 'balanced_subsample', 'bootstrap': False})
2023-04-29 23:50:32,547:INFO:Checking exceptions
2023-04-29 23:50:32,548:INFO:Importing libraries
2023-04-29 23:50:32,549:INFO:Copying training dataset
2023-04-29 23:50:32,556:INFO:Defining folds
2023-04-29 23:50:32,556:INFO:Declaring metric variables
2023-04-29 23:50:32,566:INFO:Importing untrained model
2023-04-29 23:50:32,567:INFO:Declaring custom model
2023-04-29 23:50:32,580:INFO:Extra Trees Classifier Imported successfully
2023-04-29 23:50:32,616:INFO:Starting cross validation
2023-04-29 23:50:32,620:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:50:37,913:INFO:Calculating mean and std
2023-04-29 23:50:37,920:INFO:Creating metrics dataframe
2023-04-29 23:50:37,948:INFO:Finalizing model
2023-04-29 23:50:38,820:INFO:Uploading results into container
2023-04-29 23:50:38,822:INFO:Uploading model into container now
2023-04-29 23:50:38,823:INFO:_master_model_container: 27
2023-04-29 23:50:38,823:INFO:_display_container: 11
2023-04-29 23:50:38,825:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                     class_weight='balanced_subsample', criterion='gini',
                     max_depth=6, max_features=1.0, max_leaf_nodes=None,
                     max_samples=None, min_impurity_decrease=0,
                     min_samples_leaf=4, min_samples_split=7,
                     min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2023-04-29 23:50:38,825:INFO:create_model() successfully completed......................................
2023-04-29 23:50:39,067:INFO:SubProcess create_model() end ==================================
2023-04-29 23:50:39,068:INFO:choose_better activated
2023-04-29 23:50:39,094:INFO:SubProcess create_model() called ==================================
2023-04-29 23:50:39,099:INFO:Initializing create_model()
2023-04-29 23:50:39,100:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-29 23:50:39,101:INFO:Checking exceptions
2023-04-29 23:50:39,110:INFO:Importing libraries
2023-04-29 23:50:39,110:INFO:Copying training dataset
2023-04-29 23:50:39,131:INFO:Defining folds
2023-04-29 23:50:39,132:INFO:Declaring metric variables
2023-04-29 23:50:39,140:INFO:Importing untrained model
2023-04-29 23:50:39,141:INFO:Declaring custom model
2023-04-29 23:50:39,147:INFO:Extra Trees Classifier Imported successfully
2023-04-29 23:50:39,148:INFO:Starting cross validation
2023-04-29 23:50:39,153:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-29 23:50:41,306:INFO:Calculating mean and std
2023-04-29 23:50:41,308:INFO:Creating metrics dataframe
2023-04-29 23:50:41,316:INFO:Finalizing model
2023-04-29 23:50:41,854:INFO:Uploading results into container
2023-04-29 23:50:41,856:INFO:Uploading model into container now
2023-04-29 23:50:41,857:INFO:_master_model_container: 28
2023-04-29 23:50:41,857:INFO:_display_container: 12
2023-04-29 23:50:41,858:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 23:50:41,858:INFO:create_model() successfully completed......................................
2023-04-29 23:50:42,257:INFO:SubProcess create_model() end ==================================
2023-04-29 23:50:42,267:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False) result for Accuracy is 0.9338
2023-04-29 23:50:42,274:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                     class_weight='balanced_subsample', criterion='gini',
                     max_depth=6, max_features=1.0, max_leaf_nodes=None,
                     max_samples=None, min_impurity_decrease=0,
                     min_samples_leaf=4, min_samples_split=7,
                     min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False) result for Accuracy is 0.8679
2023-04-29 23:50:42,276:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False) is best model
2023-04-29 23:50:42,276:INFO:choose_better completed
2023-04-29 23:50:42,276:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-04-29 23:50:42,314:INFO:_master_model_container: 28
2023-04-29 23:50:42,315:INFO:_display_container: 11
2023-04-29 23:50:42,316:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2023-04-29 23:50:42,316:INFO:tune_model() successfully completed......................................
2023-04-29 23:50:42,634:INFO:Initializing evaluate_model()
2023-04-29 23:50:42,634:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-04-29 23:50:42,711:INFO:Initializing plot_model()
2023-04-29 23:50:42,712:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>, system=True)
2023-04-29 23:50:42,713:INFO:Checking exceptions
2023-04-29 23:50:42,786:INFO:Preloading libraries
2023-04-29 23:50:42,820:INFO:Copying training dataset
2023-04-29 23:50:42,820:INFO:Plot type: pipeline
2023-04-29 23:50:43,308:INFO:Visual Rendered Successfully
2023-04-29 23:50:43,624:INFO:plot_model() successfully completed......................................
2023-04-29 23:50:49,743:INFO:Initializing interpret_model()
2023-04-29 23:50:49,745:INFO:interpret_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>)
2023-04-29 23:50:49,745:INFO:Checking exceptions
2023-04-29 23:50:49,746:INFO:Soft dependency imported: shap: 0.41.0
2023-04-29 23:50:49,843:INFO:plot type: summary
2023-04-29 23:50:49,843:INFO:Creating TreeExplainer
2023-04-29 23:50:49,860:INFO:Compiling shap values
2023-04-29 23:50:50,558:INFO:Visual Rendered Successfully
2023-04-29 23:50:50,558:INFO:interpret_model() successfully completed......................................
2023-04-29 23:50:50,871:INFO:Initializing interpret_model()
2023-04-29 23:50:50,872:INFO:interpret_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=32, plot=reason, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>)
2023-04-29 23:50:50,873:INFO:Checking exceptions
2023-04-29 23:50:50,874:INFO:Soft dependency imported: shap: 0.41.0
2023-04-29 23:50:50,974:INFO:plot type: reason
2023-04-29 23:50:50,974:INFO:model type detected: type 1
2023-04-29 23:50:50,974:INFO:Creating TreeExplainer
2023-04-29 23:50:50,989:INFO:Compiling shap values
2023-04-29 23:50:50,990:INFO:model type detected: Unknown
2023-04-29 23:50:51,015:INFO:Visual Rendered Successfully
2023-04-29 23:50:51,015:INFO:interpret_model() successfully completed......................................
2023-04-29 23:50:51,308:INFO:Initializing interpret_model()
2023-04-29 23:50:51,309:INFO:interpret_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=reason, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017AD42759D0>)
2023-04-29 23:50:51,309:INFO:Checking exceptions
2023-04-29 23:50:51,309:INFO:Soft dependency imported: shap: 0.41.0
2023-04-29 23:50:51,402:INFO:plot type: reason
2023-04-29 23:50:51,403:INFO:model type detected: type 1
2023-04-29 23:50:51,403:INFO:Creating TreeExplainer
2023-04-29 23:50:51,416:INFO:Compiling shap values
2023-04-29 23:50:51,417:WARNING:Observation set to None. Model agnostic plot will be rendered.
2023-04-29 23:50:51,578:INFO:Visual Rendered Successfully
2023-04-29 23:50:51,579:INFO:interpret_model() successfully completed......................................
